{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Iterable, Dict, Any, List, Optional\n",
    "import requests\n",
    "from functools import lru_cache\n",
    "from collections import Counter\n",
    "import re \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique NDC11 candidates: 29607 | Cached: 0 | To fetch: 29607\n",
      "  fetched 200/29607\n",
      "  fetched 400/29607\n",
      "  fetched 600/29607\n",
      "  fetched 800/29607\n",
      "  fetched 1000/29607\n",
      "  fetched 1200/29607\n",
      "  fetched 1400/29607\n",
      "  fetched 1600/29607\n",
      "  fetched 1800/29607\n",
      "  fetched 2000/29607\n"
     ]
    }
   ],
   "source": [
    "import re, sqlite3, threading, time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from contextlib import closing\n",
    "from functools import lru_cache\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "BASE = \"https://rxnav.nlm.nih.gov/REST\"\n",
    "DB_PATH = \"rx_cache.sqlite\"\n",
    "\n",
    "# ---------- Cleaning ----------\n",
    "def clean_csv(path, keep=(\"State\",\"NDC\",\"Units Reimbursed\",\"Number of Prescriptions\"),\n",
    "              drop_xx=True, add_year=True):\n",
    "    # Removed chunksize parameter so read_csv returns a DataFrame instead of an iterator\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        usecols=lambda c: c in set(keep),\n",
    "        dtype={\"State\": \"string\", \"NDC\": \"string\"},\n",
    "        low_memory=False\n",
    "    )\n",
    "    df[\"State\"] = df[\"State\"].str.strip().str.upper()\n",
    "    for c in (\"Units Reimbursed\", \"Number of Prescriptions\"):\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=list(keep))\n",
    "    if drop_xx:\n",
    "        df = df[df[\"State\"] != \"XX\"]\n",
    "    if add_year:\n",
    "        df[\"Year\"] = str(path)[4:8]\n",
    "    # Keep tidy cols\n",
    "    cols = [c for c in (*keep, \"Year\") if c in df.columns]\n",
    "    return df.loc[:, cols].copy()\n",
    "\n",
    "# ---------- NDC normalize ----------\n",
    "def ndc11_candidates(ndc):\n",
    "    s = re.sub(r\"\\D\", \"\", str(ndc or \"\"))\n",
    "    if len(s) == 11:\n",
    "        return [s]\n",
    "    if len(s) == 10:\n",
    "        return [\n",
    "            s[:4].rjust(5,\"0\")+s[4:8]+s[8:],   # 4-4-2\n",
    "            s[:5]+s[5:8].rjust(4,\"0\")+s[8:],   # 5-3-2\n",
    "            s[:9]+s[9:].rjust(2,\"0\"),          # 5-4-1\n",
    "        ]\n",
    "    return []\n",
    "\n",
    "# ---------- HTTP session w/ retries ----------\n",
    "def make_sess():\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\"User-Agent\": \"ndc-enrichment/parallel/1.0\"})\n",
    "    retry = Retry(total=5, backoff_factor=0.4, status_forcelist=[429,500,502,503,504])\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
    "    return s\n",
    "\n",
    "@lru_cache(maxsize=100_000)\n",
    "def ndcproperties(ndc11):\n",
    "    with make_sess() as s:\n",
    "        r = s.get(f\"{BASE}/ndcproperties.json\", params={\"id\": ndc11, \"ndcstatus\": \"ALL\"}, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        return (r.json().get(\"ndcPropertyList\") or {}).get(\"ndcProperty\") or []\n",
    "\n",
    "@lru_cache(maxsize=100_000)\n",
    "def rx_allprops(rxcui):\n",
    "    with make_sess() as s:\n",
    "        r = s.get(f\"{BASE}/rxcui/{rxcui}/allProperties.json\", params={\"prop\":\"names\"}, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        return (r.json().get(\"propConceptGroup\") or {}).get(\"propConcept\") or []\n",
    "\n",
    "def props_dict(item):\n",
    "    plist = (item.get(\"propertyConceptList\") or {}).get(\"propertyConcept\") or []\n",
    "    return { (p.get(\"propName\") or \"\").upper(): (p.get(\"propValue\") or \"\").strip()\n",
    "             for p in plist }\n",
    "\n",
    "# ---------- SQLite cache ----------\n",
    "DDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ndc_map (\n",
    "  ndc11 TEXT PRIMARY KEY,\n",
    "  rxcui TEXT,\n",
    "  brand_name TEXT,\n",
    "  generic_name TEXT,\n",
    "  family_in TEXT,\n",
    "  family_scd TEXT,\n",
    "  family_bn TEXT,\n",
    "  labeler TEXT,\n",
    "  fetched_at REAL\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS idx_rxcui ON ndc_map(rxcui);\n",
    "\"\"\"\n",
    "\n",
    "def init_db(path=DB_PATH):\n",
    "    with closing(sqlite3.connect(path, check_same_thread=False)) as con:\n",
    "        con.executescript(DDL)\n",
    "        con.commit()\n",
    "\n",
    "def get_cached(ndc11_list, path=DB_PATH):\n",
    "    if not ndc11_list: return pd.DataFrame(columns=[\"ndc11\"])\n",
    "    qmarks = \",\".join(\"?\"*len(ndc11_list))\n",
    "    with closing(sqlite3.connect(path, check_same_thread=False)) as con:\n",
    "        return pd.read_sql_query(\n",
    "            f\"SELECT * FROM ndc_map WHERE ndc11 IN ({qmarks})\", con, params=ndc11_list\n",
    "        )\n",
    "\n",
    "_lock = threading.Lock()\n",
    "\n",
    "def upsert_row(row, path=DB_PATH):\n",
    "    with _lock, closing(sqlite3.connect(path, check_same_thread=False)) as con:\n",
    "        con.execute(\"\"\"\n",
    "            INSERT INTO ndc_map (ndc11,rxcui,brand_name,generic_name,family_in,family_scd,family_bn,labeler,fetched_at)\n",
    "            VALUES (?,?,?,?,?,?,?,?,?)\n",
    "            ON CONFLICT(ndc11) DO UPDATE SET\n",
    "              rxcui=excluded.rxcui,\n",
    "              brand_name=excluded.brand_name,\n",
    "              generic_name=excluded.generic_name,\n",
    "              family_in=excluded.family_in,\n",
    "              family_scd=excluded.family_scd,\n",
    "              family_bn=excluded.family_bn,\n",
    "              labeler=excluded.labeler,\n",
    "              fetched_at=excluded.fetched_at\n",
    "        \"\"\", (row[\"ndc11\"], row[\"rxcui\"], row[\"brand_name\"], row[\"generic_name\"],\n",
    "              row[\"family_in\"], row[\"family_scd\"], row[\"family_bn\"], row[\"labeler\"], time.time()))\n",
    "        con.commit()\n",
    "\n",
    "# ---------- Worker (fetch one NDC11) ----------\n",
    "def fetch_one(ndc11):\n",
    "    items = ndcproperties(ndc11)\n",
    "    if not items:\n",
    "        return None\n",
    "    it = items[0]\n",
    "    pr = props_dict(it)\n",
    "    rxcui = it.get(\"rxcui\")\n",
    "    brand = pr.get(\"PROPRIETARYNAME\")\n",
    "    generic = pr.get(\"NONPROPRIETARYNAME\")\n",
    "    labeler = pr.get(\"LABELER\") or pr.get(\"LABELERNAME\")\n",
    "    family_in = family_scd = family_bn = None\n",
    "    if rxcui:\n",
    "        allp = rx_allprops(rxcui)\n",
    "        def first(kind):\n",
    "            for p in allp:\n",
    "                if (p.get(\"propName\") or \"\").upper() == kind:\n",
    "                    return (p.get(\"propValue\") or \"\").strip()\n",
    "        family_in  = first(\"IN\")\n",
    "        family_scd = first(\"SCD\")\n",
    "        family_bn  = first(\"BN\")\n",
    "        if not generic:\n",
    "            generic = family_scd or family_in\n",
    "    return {\n",
    "        \"ndc11\": it.get(\"ndcItem\") or ndc11,\n",
    "        \"rxcui\": rxcui,\n",
    "        \"brand_name\": brand,\n",
    "        \"generic_name\": generic,\n",
    "        \"family_in\": family_in,\n",
    "        \"family_scd\": family_scd,\n",
    "        \"family_bn\": family_bn,\n",
    "        \"labeler\": labeler\n",
    "    }\n",
    "\n",
    "# ---------- Warm cache in parallel ----------\n",
    "def warm_cache_from_ndcs(ndc_series, max_workers=12, pause=0.0):\n",
    "    \"\"\"Accepts raw NDCs, normalizes to NDC11, filters ones missing in cache, fetches in parallel.\"\"\"\n",
    "    init_db(DB_PATH)\n",
    "\n",
    "    # 1) Unique raw NDCs -> candidate NDC11s -> pick all candidates (we'll test each)\n",
    "    raw = (ndc_series.dropna().astype(str).str.strip().unique().tolist())\n",
    "    cands = set()\n",
    "    for ndc in raw:\n",
    "        for c in ndc11_candidates(ndc):\n",
    "            cands.add(c)\n",
    "    cands = sorted(cands)\n",
    "\n",
    "    # 2) Remove already-cached\n",
    "    cached = set(get_cached(cands)[\"ndc11\"].tolist())\n",
    "    todo = [c for c in cands if c not in cached]\n",
    "    print(f\"Unique NDC11 candidates: {len(cands)} | Cached: {len(cached)} | To fetch: {len(todo)}\")\n",
    "\n",
    "    if not todo:\n",
    "        return\n",
    "\n",
    "    # 3) Parallel fetch with polite pause between tasks (optional)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = {ex.submit(fetch_one, ndc11): ndc11 for ndc11 in todo}\n",
    "        for i, fut in enumerate(as_completed(futures), 1):\n",
    "            ndc11 = futures[fut]\n",
    "            try:\n",
    "                row = fut.result()\n",
    "                if row:\n",
    "                    upsert_row(row)\n",
    "            except Exception:\n",
    "                pass\n",
    "            if pause:\n",
    "                time.sleep(pause)\n",
    "            if i % 200 == 0:\n",
    "                print(f\"  fetched {i}/{len(todo)}\")\n",
    "\n",
    "# ---------- Join fast from cache ----------\n",
    "def enrich_from_cache(df):\n",
    "    # Normalize to a *single* chosen NDC11 per row (first valid candidate)\n",
    "    df = df.copy()\n",
    "    df[\"ndc11\"] = df[\"NDC\"].apply(lambda x: next(iter(ndc11_candidates(x)), None))\n",
    "    cached = get_cached(df[\"ndc11\"].dropna().unique().tolist())\n",
    "    return df.merge(cached, on=\"ndc11\", how=\"left\")\n",
    "\n",
    "# 1) Clean fast\n",
    "cleaned = clean_csv(\"SDUD2017.csv\")\n",
    "\n",
    "# 2) Warm the cache *once per set of files* (parallel + persistent)\n",
    "#    Tweak max_workers to your network/CPU; start with 8–12.\n",
    "warm_cache_from_ndcs(cleaned[\"NDC\"], max_workers=12, pause=0.0)\n",
    "\n",
    "# 3) Enrich instantly from local SQLite (no HTTP for already-cached NDCs)\n",
    "enriched = enrich_from_cache(cleaned)\n",
    "\n",
    "# 4) Repeat steps 1 & 3 for other files — step 2 will be tiny because cache is warm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
