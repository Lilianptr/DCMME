{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import shelve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import statsmodels \n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.sandbox.stats.runs import runstest_1samp\n",
    "import glob\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "user=\"Lilian\"\n",
    "years_list = [2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
    "#user in personal pc1 <- \"asus\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f557914",
   "metadata": {},
   "source": [
    "#### Generating some stats and Runs Test for the panels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f104f",
   "metadata": {},
   "source": [
    "Doing the statistical analysis. Be careful with the units scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71234f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDCATC_stats:  #Creating a new class to analyze correlations\n",
    "    \n",
    "    @staticmethod \n",
    "    def correlation_look(years_list, base_path=None, min_records=8, state_filter=None):\n",
    "        \n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(\"CORRELATION ANALYSIS: Units Reimbursed vs Number of Prescriptions\")\n",
    "        print(\"THREE-PANEL ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        results = {\n",
    "            'panel1_state_atc2': [],\n",
    "            'panel2_national_atc2': [], \n",
    "            'panel3_atc2_quarter': []\n",
    "        }\n",
    "        \n",
    "        # Load all data first\n",
    "        all_data = {}\n",
    "        for year in years_list:\n",
    "            print(f\"Loading {year} data...\", end=\" \")\n",
    "            try:\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                records['ATC2 Class'] = records['ATC4 Class'].str[:3]\n",
    "                all_data[year] = records\n",
    "                print(f\"✓ ({len(records):,} records)\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "        \n",
    "        # ==================== PANEL 1: STATE x ATC2 ANALYSIS ====================\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        if state_filter:\n",
    "            print(f\"PANEL 1: {state_filter} STATE x ATC2 CORRELATION ANALYSIS\")\n",
    "        else:\n",
    "            print(\"PANEL 1: STATE x ATC2 CORRELATION ANALYSIS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Combine all years first\n",
    "        all_combined = []\n",
    "        for year, records in all_data.items():\n",
    "            records_copy = records.copy()\n",
    "            records_copy['Year'] = year\n",
    "            all_combined.append(records_copy)\n",
    "        \n",
    "        combined_df = pd.concat(all_combined, ignore_index=True)\n",
    "        \n",
    "        # Apply state filter for Panel 1\n",
    "        if state_filter:\n",
    "            panel1_states = [state_filter]\n",
    "        else:\n",
    "            panel1_states = combined_df['State'].unique()\n",
    "        \n",
    "        # Now analyze across all years\n",
    "        for state in panel1_states:\n",
    "            state_data = combined_df[combined_df['State'] == state]\n",
    "            \n",
    "            for atc2 in state_data['ATC2 Class'].unique():\n",
    "                subset = state_data[state_data['ATC2 Class'] == atc2]\n",
    "                \n",
    "                # AGGREGATE TO QUARTERLY TIME SERIES\n",
    "                subset_ts = subset.groupby(['Year', 'Quarter']).agg({\n",
    "                    'Units Reimbursed': 'sum', \n",
    "                    'Number of Prescriptions': 'sum'\n",
    "                }).reset_index().sort_values(['Year','Quarter'])\n",
    "                \n",
    "                # DEBUG LINE\n",
    "                #if state == 'IN' and len(subset_ts) > 0:\n",
    "                #    print(f\"DEBUG: {state}-{atc2}: {len(subset_ts)} quarterly records\")\n",
    "                \n",
    "                if len(subset_ts) >= min_records:\n",
    "                    try:\n",
    "                        atc2_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns else ''\n",
    "                        \n",
    "                        # Calculate correlations\n",
    "                        pearson_r, pearson_p = pearsonr(subset_ts['Units Reimbursed'], subset_ts['Number of Prescriptions'])\n",
    "                        spearman_r, spearman_p = spearmanr(subset_ts['Units Reimbursed'], subset_ts['Number of Prescriptions'])\n",
    "                        \n",
    "                        results['panel1_state_atc2'].append({\n",
    "                            'State': state,\n",
    "                            'ATC2_Class': atc2,\n",
    "                            'ATC2_Name': atc2_name,\n",
    "                            'N_Records': len(subset_ts),\n",
    "                            'Pearson_r': pearson_r,\n",
    "                            'Pearson_p': pearson_p,\n",
    "                            'Spearman_r': spearman_r,\n",
    "                            'Spearman_p': spearman_p,\n",
    "                            'Mean_Units': subset_ts['Units Reimbursed'].mean(),\n",
    "                            'Mean_Prescriptions': subset_ts['Number of Prescriptions'].mean()\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {state}-{atc2}: {e}\")\n",
    "        \n",
    "        # Display Panel 1 results\n",
    "        panel1_df = pd.DataFrame(results['panel1_state_atc2'])\n",
    "        if not panel1_df.empty:\n",
    "            print(f\"\\nPanel 1 Results: {len(panel1_df)} state x ATC2 combinations analyzed\")\n",
    "            \n",
    "            # Summary statistics\n",
    "            print(f\"Average Pearson correlation: {panel1_df['Pearson_r'].mean():.4f}\")\n",
    "            print(f\"Range: {panel1_df['Pearson_r'].min():.4f} to {panel1_df['Pearson_r'].max():.4f}\")\n",
    "            print(f\"Significant correlations (p<0.05): {(panel1_df['Pearson_p'] < 0.05).sum()}/{len(panel1_df)} ({100*(panel1_df['Pearson_p'] < 0.05).sum()/len(panel1_df):.1f}%)\")\n",
    "            \n",
    "            # Top correlations\n",
    "            print(f\"\\nTop 10 Highest Pearson Correlations (State x ATC2):\")\n",
    "            top_corr = panel1_df.nlargest(10, 'Pearson_r')[['State', 'ATC2_Class', 'ATC2_Name', 'Pearson_r', 'Pearson_p', 'N_Records']].copy()\n",
    "            top_corr['Pearson_p'] = top_corr['Pearson_p'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(top_corr.to_string(index=False))\n",
    "            \n",
    "            # Lowest correlations\n",
    "            print(f\"\\nTop 10 Lowest Pearson Correlations (State x ATC2):\")\n",
    "            low_corr = panel1_df.nsmallest(10, 'Pearson_r')[['State', 'ATC2_Class', 'ATC2_Name', 'Pearson_r', 'Pearson_p', 'N_Records']].copy()\n",
    "            low_corr['Pearson_p'] = low_corr['Pearson_p'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(low_corr.to_string(index=False))\n",
    "        \n",
    "        # ==================== PANEL 2: NATIONAL x ATC2 ANALYSIS ====================\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"PANEL 2: NATIONAL x ATC2 CORRELATION ANALYSIS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Use the combined_df from Panel 1\n",
    "        for atc2 in combined_df['ATC2 Class'].unique():\n",
    "            subset = combined_df[combined_df['ATC2 Class'] == atc2]\n",
    "            \n",
    "            # AGGREGATE TO QUARTERLY TIME SERIES\n",
    "            subset_ts = subset.groupby(['Year', 'Quarter']).agg({\n",
    "                'Units Reimbursed': 'sum', \n",
    "                'Number of Prescriptions': 'sum'\n",
    "            }).reset_index().sort_values(['Year','Quarter'])\n",
    "            \n",
    "            # DEBUG LINE\n",
    "            if len(subset_ts) > 0:\n",
    "                print(f\"DEBUG: National-{atc2}: {len(subset_ts)} quarterly records\")\n",
    "            \n",
    "            if len(subset_ts) >= min_records:\n",
    "                try:\n",
    "                    atc2_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns else ''\n",
    "                    \n",
    "                    # Calculate correlations\n",
    "                    pearson_r, pearson_p = pearsonr(subset_ts['Units Reimbursed'], subset_ts['Number of Prescriptions'])\n",
    "                    spearman_r, spearman_p = spearmanr(subset_ts['Units Reimbursed'], subset_ts['Number of Prescriptions'])\n",
    "                    \n",
    "                    results['panel2_national_atc2'].append({\n",
    "                        'ATC2_Class': atc2,\n",
    "                        'ATC2_Name': atc2_name,\n",
    "                        'N_Records': len(subset_ts),\n",
    "                        'N_States': subset['State'].nunique(),\n",
    "                        'Pearson_r': pearson_r,\n",
    "                        'Pearson_p': pearson_p,\n",
    "                        'Spearman_r': spearman_r,\n",
    "                        'Spearman_p': spearman_p,\n",
    "                        'Mean_Units': subset_ts['Units Reimbursed'].mean(),\n",
    "                        'Mean_Prescriptions': subset_ts['Number of Prescriptions'].mean()\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing National-{atc2}: {e}\")\n",
    "        \n",
    "        # Display Panel 2 results\n",
    "        panel2_df = pd.DataFrame(results['panel2_national_atc2'])\n",
    "        if not panel2_df.empty:\n",
    "            print(f\"\\nPanel 2 Results: {len(panel2_df)} national x ATC2 combinations analyzed\")\n",
    "            \n",
    "            print(f\"Average Pearson correlation: {panel2_df['Pearson_r'].mean():.4f}\")\n",
    "            print(f\"Range: {panel2_df['Pearson_r'].min():.4f} to {panel2_df['Pearson_r'].max():.4f}\")\n",
    "            print(f\"Significant correlations (p<0.05): {(panel2_df['Pearson_p'] < 0.05).sum()}/{len(panel2_df)} ({100*(panel2_df['Pearson_p'] < 0.05).sum()/len(panel2_df):.1f}%)\")\n",
    "            \n",
    "            print(f\"\\nTop 10 Highest Pearson Correlations (National x ATC2):\")\n",
    "            top_corr_nat = panel2_df.nlargest(10, 'Pearson_r')[['ATC2_Class', 'ATC2_Name', 'Pearson_r', 'Pearson_p', 'N_Records']].copy()\n",
    "            top_corr_nat['Pearson_p'] = top_corr_nat['Pearson_p'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(top_corr_nat.to_string(index=False))\n",
    "            \n",
    "            print(f\"\\nTop 10 Lowest Pearson Correlations (National x ATC2):\")\n",
    "            low_corr_nat = panel2_df.nsmallest(10, 'Pearson_r')[['ATC2_Class', 'ATC2_Name', 'Pearson_r', 'Pearson_p', 'N_Records']].copy()\n",
    "            low_corr_nat['Pearson_p'] = low_corr_nat['Pearson_p'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(low_corr_nat.to_string(index=False))\n",
    "        \n",
    "        # ==================== PANEL 3: ATC2 x QUARTER ANALYSIS ====================\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"PANEL 3: ATC2 x QUARTER CORRELATION ANALYSIS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # combined_df already exists from Panel 1\n",
    "        for atc2 in combined_df['ATC2 Class'].unique():\n",
    "            atc2_data = combined_df[combined_df['ATC2 Class'] == atc2]\n",
    "            \n",
    "            for quarter in sorted(atc2_data['Quarter'].unique()):\n",
    "                subset = atc2_data[atc2_data['Quarter'] == quarter]\n",
    "                \n",
    "                # AGGREGATE TO YEARLY TIME SERIES (for this specific quarter across years)\n",
    "                subset_ts = subset.groupby(['Year', 'Quarter']).agg({\n",
    "                    'Units Reimbursed': 'sum', \n",
    "                    'Number of Prescriptions': 'sum'\n",
    "                }).reset_index().sort_values(['Year','Quarter'])\n",
    "                \n",
    "                # DEBUG LINE\n",
    "                #if len(subset_ts) > 0:\n",
    "                    #print(f\"DEBUG: {atc2}-Q{quarter}: {len(subset_ts)} quarterly records\")\n",
    "                \n",
    "                if len(subset_ts) >= min_records:\n",
    "                    try:\n",
    "                        atc2_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns else ''\n",
    "                        \n",
    "                        # Calculate correlations\n",
    "                        pearson_r, pearson_p = pearsonr(subset_ts['Units Reimbursed'], subset_ts['Number of Prescriptions'])\n",
    "                        spearman_r, spearman_p = spearmanr(subset_ts['Units Reimbursed'], subset_ts['Number of Prescriptions'])\n",
    "                        \n",
    "                        results['panel3_atc2_quarter'].append({\n",
    "                            'ATC2_Class': atc2,\n",
    "                            'ATC2_Name': atc2_name,\n",
    "                            'Quarter': quarter,\n",
    "                            'N_Records': len(subset_ts),\n",
    "                            'N_Years': subset_ts['Year'].nunique(),\n",
    "                            'N_States': subset['State'].nunique(),\n",
    "                            'Pearson_r': pearson_r,\n",
    "                            'Pearson_p': pearson_p,\n",
    "                            'Spearman_r': spearman_r,\n",
    "                            'Spearman_p': spearman_p,\n",
    "                            'Mean_Units': subset_ts['Units Reimbursed'].mean(),\n",
    "                            'Mean_Prescriptions': subset_ts['Number of Prescriptions'].mean()\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {atc2}-Q{quarter}: {e}\")\n",
    "        \n",
    "        # Display Panel 3 results\n",
    "        panel3_df = pd.DataFrame(results['panel3_atc2_quarter'])\n",
    "        if not panel3_df.empty:\n",
    "            print(f\"\\nPanel 3 Results: {len(panel3_df)} ATC2 x quarter combinations analyzed\")\n",
    "            \n",
    "            print(f\"Average Pearson correlation: {panel3_df['Pearson_r'].mean():.4f}\")\n",
    "            print(f\"Range: {panel3_df['Pearson_r'].min():.4f} to {panel3_df['Pearson_r'].max():.4f}\")\n",
    "            print(f\"Significant correlations (p<0.05): {(panel3_df['Pearson_p'] < 0.05).sum()}/{len(panel3_df)} ({100*(panel3_df['Pearson_p'] < 0.05).sum()/len(panel3_df):.1f}%)\")\n",
    "            \n",
    "            # Summary by Quarter\n",
    "            quarter_summary = panel3_df.groupby('Quarter').agg({\n",
    "                'Pearson_r': ['mean', 'std', 'min', 'max'], 'N_Records': 'sum'\n",
    "            }).round(4)\n",
    "            quarter_summary.columns = ['Avg_Pearson', 'Std_Pearson', 'Min_Pearson', 'Max_Pearson', 'Total_Records']\n",
    "            \n",
    "            print(f\"\\nCorrelation Summary by Quarter:\")\n",
    "            print(f\"{'Quarter':<8} {'Avg':<8} {'Std':<8} {'Min':<8} {'Max':<8} {'Records':<10}\")\n",
    "            print(\"-\" * 60)\n",
    "            for quarter, row in quarter_summary.iterrows():\n",
    "                print(f\"Q{quarter:<7} {row['Avg_Pearson']:<8.4f} {row['Std_Pearson']:<8.4f} {row['Min_Pearson']:<8.4f} {row['Max_Pearson']:<8.4f} {row['Total_Records']:<10.0f}\")\n",
    "            \n",
    "            print(f\"\\nTop 10 Highest Pearson Correlations (ATC2 x Quarter):\")\n",
    "            top_corr_quarter = panel3_df.nlargest(10, 'Pearson_r')[['ATC2_Class', 'ATC2_Name', 'Quarter', 'Pearson_r', 'Pearson_p', 'N_Records']].copy()\n",
    "            top_corr_quarter['Pearson_p'] = top_corr_quarter['Pearson_p'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(top_corr_quarter.to_string(index=False))\n",
    "            \n",
    "            print(f\"\\nTop 10 Lowest Pearson Correlations (ATC2 x Quarter):\")\n",
    "            low_corr_quarter = panel3_df.nsmallest(10, 'Pearson_r')[['ATC2_Class', 'ATC2_Name', 'Quarter', 'Pearson_r', 'Pearson_p', 'N_Records']].copy()\n",
    "            low_corr_quarter['Pearson_p'] = low_corr_quarter['Pearson_p'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(low_corr_quarter.to_string(index=False))\n",
    "        \n",
    "        # ==================== COMPARATIVE SUMMARY ACROSS PANELS ====================\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"COMPARATIVE SUMMARY ACROSS ALL THREE PANELS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        summary_data = []\n",
    "        \n",
    "        if not panel1_df.empty:\n",
    "            panel1_avg = panel1_df['Pearson_r'].mean()\n",
    "            panel1_std = panel1_df['Pearson_r'].std()\n",
    "            panel1_sig = (panel1_df['Pearson_p'] < 0.05).sum()\n",
    "            panel1_total = len(panel1_df)\n",
    "            summary_data.append(['State x ATC2', panel1_total, panel1_avg, panel1_std, panel1_sig])\n",
    "        \n",
    "        if not panel2_df.empty:\n",
    "            panel2_avg = panel2_df['Pearson_r'].mean()\n",
    "            panel2_std = panel2_df['Pearson_r'].std()\n",
    "            panel2_sig = (panel2_df['Pearson_p'] < 0.05).sum()\n",
    "            panel2_total = len(panel2_df)\n",
    "            summary_data.append(['National x ATC2', panel2_total, panel2_avg, panel2_std, panel2_sig])\n",
    "        \n",
    "        if not panel3_df.empty:\n",
    "            panel3_avg = panel3_df['Pearson_r'].mean()\n",
    "            panel3_std = panel3_df['Pearson_r'].std()\n",
    "            panel3_sig = (panel3_df['Pearson_p'] < 0.05).sum()\n",
    "            panel3_total = len(panel3_df)\n",
    "            summary_data.append(['ATC2 x Quarter', panel3_total, panel3_avg, panel3_std, panel3_sig])\n",
    "        \n",
    "        if summary_data:\n",
    "            summary_df = pd.DataFrame(summary_data, columns=['Panel', 'Total_Tests', 'Avg_Pearson', 'Std_Pearson', 'Sig_Correlations'])\n",
    "            summary_df['Sig_Percentage'] = (summary_df['Sig_Correlations'] / summary_df['Total_Tests'] * 100).round(1)\n",
    "            summary_df['Avg_Pearson'] = summary_df['Avg_Pearson'].round(4)\n",
    "            summary_df['Std_Pearson'] = summary_df['Std_Pearson'].round(4)\n",
    "            \n",
    "            print(summary_df.to_string(index=False))\n",
    "            \n",
    "            # Additional insights\n",
    "            print(f\"\\nKey Insights:\")\n",
    "            if len(summary_data) > 1:\n",
    "                highest_avg_panel = summary_df.loc[summary_df['Avg_Pearson'].idxmax(), 'Panel']\n",
    "                lowest_avg_panel = summary_df.loc[summary_df['Avg_Pearson'].idxmin(), 'Panel']\n",
    "                print(f\"  - Highest average correlation: {highest_avg_panel}\")\n",
    "                print(f\"  - Lowest average correlation: {lowest_avg_panel}\")\n",
    "                \n",
    "                highest_sig_panel = summary_df.loc[summary_df['Sig_Percentage'].idxmax(), 'Panel']\n",
    "                print(f\"  - Most significant correlations: {highest_sig_panel}\")\n",
    "        \n",
    "        print(f\"\\nNote: Correlations measure linear (Pearson) and monotonic (Spearman) relationships.\")\n",
    "        print(f\"Significant p-values (< 0.05) indicate correlations likely not due to chance.\")\n",
    "        \n",
    "        # Convert results to DataFrames for return\n",
    "        results_dfs = {\n",
    "            'panel1_state_atc2': panel1_df,\n",
    "            'panel2_national_atc2': panel2_df,\n",
    "            'panel3_atc2_quarter': panel3_df\n",
    "        }\n",
    "        \n",
    "        return results_dfs\n",
    "\n",
    "    @staticmethod #This is for Indiana data\n",
    "    def plot_units_vs_prescriptions_by_atc(years_list, base_path=None, min_records=8, include_negative=True):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(\"Creating plots for Indiana ATC classes...\")\n",
    "        \n",
    "        # Combine all years of data\n",
    "        all_data = []\n",
    "        for year in years_list:\n",
    "            try:\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                records = df_merged[(df_merged['ATC4 Class'].notna()) & (df_merged['State'] == 'IN')].copy()\n",
    "                records['Year'] = year\n",
    "                all_data.append(records)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {year}: {e}\")\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"No data loaded!\")\n",
    "            return\n",
    "        \n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # Get ATC classes with sufficient data and calculate correlations\n",
    "        atc_counts = combined_df['ATC2 Class'].value_counts()\n",
    "        sufficient_data_classes = atc_counts[atc_counts >= min_records].index\n",
    "        \n",
    "        # Calculate correlations for all classes with sufficient data\n",
    "        class_correlations = {}\n",
    "        for atc_class in sufficient_data_classes:\n",
    "            subset = combined_df[combined_df['ATC2 Class'] == atc_class]\n",
    "            if len(subset) > 1:\n",
    "                corr = subset['Number of Prescriptions'].corr(subset['Units Reimbursed'])\n",
    "                class_correlations[atc_class] = corr\n",
    "        \n",
    "        # Select classes to plot\n",
    "        if include_negative:\n",
    "            # Get top positive correlations and all negative correlations\n",
    "            positive_corrs = {k: v for k, v in class_correlations.items() if v >= 0}\n",
    "            negative_corrs = {k: v for k, v in class_correlations.items() if v < 0}\n",
    "            \n",
    "            # Sort positive by correlation (descending) and negative by correlation (ascending, most negative first)\n",
    "            positive_sorted = sorted(positive_corrs.items(), key=lambda x: x[1], reverse=True)\n",
    "            negative_sorted = sorted(negative_corrs.items(), key=lambda x: x[1])\n",
    "            \n",
    "            # Take top 8 positive and all negative (up to 4 more)\n",
    "            selected_positive = [x[0] for x in positive_sorted[:8]]\n",
    "            selected_negative = [x[0] for x in negative_sorted[:4]]\n",
    "            \n",
    "            valid_atc_classes = selected_positive + selected_negative\n",
    "            \n",
    "            print(f\"\\nSelected classes: {len(selected_positive)} positive correlations + {len(selected_negative)} negative correlations\")\n",
    "            if selected_negative:\n",
    "                print(f\"Negative correlation classes: {selected_negative}\")\n",
    "        else:\n",
    "            # Original behavior - top classes by count\n",
    "            valid_atc_classes = sufficient_data_classes[:12]\n",
    "        \n",
    "        # Determine grid size based on number of classes\n",
    "        n_classes = len(valid_atc_classes)\n",
    "        if n_classes <= 6:\n",
    "            rows, cols = 2, 3\n",
    "        elif n_classes <= 9:\n",
    "            rows, cols = 3, 3\n",
    "        elif n_classes <= 12:\n",
    "            rows, cols = 3, 4\n",
    "        else:\n",
    "            rows, cols = 4, 4\n",
    "            valid_atc_classes = valid_atc_classes[:16]  # Limit to 16 for display\n",
    "        \n",
    "        # Set up the plot grid\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols*5, rows*4))\n",
    "        if rows == 1 or cols == 1:\n",
    "            axes = axes.flatten() if hasattr(axes, 'flatten') else [axes]\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(valid_atc_classes)))\n",
    "        \n",
    "        for i, atc_class in enumerate(valid_atc_classes):\n",
    "            subset = combined_df[combined_df['ATC2 Class'] == atc_class]\n",
    "            atc_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns and not subset['ATC2_Name'].isna().all() else atc_class\n",
    "            \n",
    "            # Create scatter plot\n",
    "            axes[i].scatter(subset['Number of Prescriptions'], \n",
    "                           subset['Units Reimbursed'], \n",
    "                           alpha=0.6, color=colors[i], s=20)\n",
    "            \n",
    "            # Add trend line\n",
    "            if len(subset) > 1:\n",
    "                z = np.polyfit(subset['Number of Prescriptions'], subset['Units Reimbursed'], 1)\n",
    "                p = np.poly1d(z)\n",
    "                axes[i].plot(subset['Number of Prescriptions'], p(subset['Number of Prescriptions']), \n",
    "                            \"r--\", alpha=0.8, linewidth=1)\n",
    "            \n",
    "            # Format axes\n",
    "            axes[i].set_xlabel('Number of Prescriptions')\n",
    "            axes[i].set_ylabel('Units Reimbursed')\n",
    "            axes[i].set_title(f'{atc_class}\\n{atc_name[:30]}', fontsize=10)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add correlation coefficient with color coding\n",
    "            if len(subset) > 1:\n",
    "                corr = subset['Number of Prescriptions'].corr(subset['Units Reimbursed'])\n",
    "                color = 'red' if corr < 0 else 'blue'\n",
    "                axes[i].text(0.05, 0.95, f'r = {corr:.3f}', \n",
    "                            transform=axes[i].transAxes, \n",
    "                            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8, edgecolor=color),\n",
    "                            fontsize=9, color=color)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for j in range(len(valid_atc_classes), len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        title_suffix = \" (Including Negative Correlations)\" if include_negative else \"\"\n",
    "        plt.suptitle(f'Indiana: Units Reimbursed vs Number of Prescriptions by ATC2 Class{title_suffix}\\n(All Years Combined)', \n",
    "                     fontsize=16, y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary table\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PLOT SUMMARY - INDIANA ATC CLASSES\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"{'ATC2':<5} {'Name':<30} {'Records':<8} {'Correlation':<12} {'Type':<8}\")\n",
    "        print(\"-\" * 90)\n",
    "        \n",
    "        for atc_class in valid_atc_classes:\n",
    "            subset = combined_df[combined_df['ATC2 Class'] == atc_class]\n",
    "            atc_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns and not subset['ATC2_Name'].isna().all() else atc_class\n",
    "            corr = subset['Number of Prescriptions'].corr(subset['Units Reimbursed'])\n",
    "            corr_type = \"Negative\" if corr < 0 else \"Positive\"\n",
    "            \n",
    "            print(f\"{atc_class:<5} {atc_name[:28]:<30} {len(subset):<8} {corr:<12.4f} {corr_type:<8}\")\n",
    "        \n",
    "        return combined_df[combined_df['ATC2 Class'].isin(valid_atc_classes)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def runs_test_analysis(years_list, base_path=None, min_records=8, generate_heatmaps=False, export_results=False):\n",
    "        \n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(\"RUNS TEST ANALYSIS: Testing for Randomness in Units Reimbursed and Number of Prescriptions\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        all_results = {\n",
    "            'state_atc2': [],\n",
    "            'national_atc2': [], \n",
    "            'atc2_quarter': []\n",
    "        }\n",
    "        \n",
    "        # Load all data first\n",
    "        all_data = {}\n",
    "        for year in years_list:\n",
    "            print(f\"Loading {year} data...\", end=\" \")\n",
    "            try:\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                records['ATC2 Class'] = records['ATC4 Class'].str[:3]\n",
    "                all_data[year] = records\n",
    "                print(f\"✓ ({len(records):,} records)\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "        \n",
    "        # ==================== PANEL 1: STATE x ATC2 CLASSES RUNS TEST ====================\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PANEL 1: STATE x ATC2 CLASSES RUNS TEST\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Combine all years first\n",
    "        all_combined = []\n",
    "        for year, records in all_data.items():\n",
    "            records_copy = records.copy()\n",
    "            records_copy['Year'] = year\n",
    "            all_combined.append(records_copy)\n",
    "        \n",
    "        combined_df = pd.concat(all_combined, ignore_index=True)\n",
    "        \n",
    "        # Now analyze across all years\n",
    "        for state in combined_df['State'].unique():\n",
    "            state_data = combined_df[combined_df['State'] == state]\n",
    "            \n",
    "            for atc2 in state_data['ATC2 Class'].unique():\n",
    "                subset = state_data[state_data['ATC2 Class'] == atc2]\n",
    "                \n",
    "                # AGGREGATE TO QUARTERLY TIME SERIES\n",
    "                subset_ts = subset.groupby(['Year', 'Quarter']).agg({\n",
    "                    'Units Reimbursed': 'sum', \n",
    "                    'Number of Prescriptions': 'sum'\n",
    "                }).reset_index().sort_values(['Year','Quarter'])\n",
    "                \n",
    "                # DEBUG LINE\n",
    "                #if state == 'IN' and len(subset_ts) > 0:\n",
    "                #    print(f\"DEBUG: {state}-{atc2}: {len(subset_ts)} quarterly records\")\n",
    "                \n",
    "                if len(subset_ts) >= min_records:\n",
    "                    try:\n",
    "                        atc2_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns else ''\n",
    "                        \n",
    "                        # Test Units Reimbursed\n",
    "                        units_values = subset_ts['Units Reimbursed'].values\n",
    "                        units_median = np.median(units_values)\n",
    "                        units_binary = (units_values > units_median).astype(int)\n",
    "                        \n",
    "                        if len(np.unique(units_binary)) > 1:\n",
    "                            runs_stat_units, p_val_units = runstest_1samp(units_binary)\n",
    "                        else:\n",
    "                            runs_stat_units, p_val_units = np.nan, np.nan\n",
    "                        \n",
    "                        # Test Number of Prescriptions\n",
    "                        presc_values = subset_ts['Number of Prescriptions'].values\n",
    "                        presc_median = np.median(presc_values)\n",
    "                        presc_binary = (presc_values > presc_median).astype(int)\n",
    "                        \n",
    "                        if len(np.unique(presc_binary)) > 1:\n",
    "                            runs_stat_presc, p_val_presc = runstest_1samp(presc_binary)\n",
    "                        else:\n",
    "                            runs_stat_presc, p_val_presc = np.nan, np.nan\n",
    "                        \n",
    "                        all_results['state_atc2'].append({\n",
    "                            'State': state,\n",
    "                            'ATC2_Class': atc2,\n",
    "                            'ATC2_Name': atc2_name,\n",
    "                            'N_Records': len(subset_ts),\n",
    "                            'Units_Runs_Stat': runs_stat_units,\n",
    "                            'Units_P_Value': p_val_units,\n",
    "                            'Presc_Runs_Stat': runs_stat_presc,\n",
    "                            'Presc_P_Value': p_val_presc\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {state}-{atc2}: {e}\")\n",
    "        \n",
    "        # Convert to DataFrame and display results\n",
    "        state_atc2_df = pd.DataFrame(all_results['state_atc2'])\n",
    "        if not state_atc2_df.empty:\n",
    "            print(f\"\\nState x ATC2 Results: {len(state_atc2_df)} combinations analyzed\")\n",
    "            \n",
    "            # Summary statistics\n",
    "            units_significant = (state_atc2_df['Units_P_Value'] < 0.05).sum()\n",
    "            presc_significant = (state_atc2_df['Presc_P_Value'] < 0.05).sum() \n",
    "            total_valid = state_atc2_df['Units_P_Value'].notna().sum()\n",
    "            \n",
    "            print(f\"Units Reimbursed - Significant non-randomness: {units_significant}/{total_valid} ({100*units_significant/total_valid if total_valid > 0 else 0:.1f}%)\")\n",
    "            print(f\"Number of Prescriptions - Significant non-randomness: {presc_significant}/{total_valid} ({100*presc_significant/total_valid if total_valid > 0 else 0:.1f}%)\")\n",
    "            \n",
    "            # Top significant results\n",
    "            print(f\"\\nTop 10 Most Significant Units Reimbursed Results (State x ATC2):\")\n",
    "            top_units = state_atc2_df.nsmallest(10, 'Units_P_Value')[['State', 'ATC2_Class', 'ATC2_Name', 'Units_Runs_Stat', 'Units_P_Value', 'N_Records']].copy()\n",
    "            top_units['Units_P_Value'] = top_units['Units_P_Value'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(top_units.to_string(index=False))\n",
    "            \n",
    "            print(f\"\\nTop 10 Most Significant Number of Prescriptions Results (State x ATC2):\")\n",
    "            top_presc = state_atc2_df.nsmallest(10, 'Presc_P_Value')[['State', 'ATC2_Class', 'ATC2_Name', 'Presc_Runs_Stat', 'Presc_P_Value', 'N_Records']].copy()\n",
    "            top_presc['Presc_P_Value'] = top_presc['Presc_P_Value'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(top_presc.to_string(index=False))\n",
    "        \n",
    "        # ==================== PANEL 2: NATIONAL x ATC2 CLASSES RUNS TEST ====================\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PANEL 2: NATIONAL x ATC2 CLASSES RUNS TEST\") \n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Use the combined_df from Panel 1\n",
    "        for atc2 in combined_df['ATC2 Class'].unique():\n",
    "            subset = combined_df[combined_df['ATC2 Class'] == atc2]\n",
    "            \n",
    "            # AGGREGATE TO QUARTERLY TIME SERIES\n",
    "            subset_ts = subset.groupby(['Year', 'Quarter']).agg({\n",
    "                'Units Reimbursed': 'sum', \n",
    "                'Number of Prescriptions': 'sum'\n",
    "            }).reset_index().sort_values(['Year','Quarter'])\n",
    "            \n",
    "            # DEBUG LINE\n",
    "            #if len(subset_ts) > 0:\n",
    "            #    print(f\"DEBUG: National-{atc2}: {len(subset_ts)} quarterly records\")\n",
    "            \n",
    "            if len(subset_ts) >= min_records:\n",
    "                try:\n",
    "                    atc2_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns else ''\n",
    "                    \n",
    "                    # Test Units Reimbursed\n",
    "                    units_values = subset_ts['Units Reimbursed'].values\n",
    "                    units_median = np.median(units_values)\n",
    "                    units_binary = (units_values > units_median).astype(int)\n",
    "                    \n",
    "                    if len(np.unique(units_binary)) > 1:\n",
    "                        runs_stat_units, p_val_units = runstest_1samp(units_binary)\n",
    "                    else:\n",
    "                        runs_stat_units, p_val_units = np.nan, np.nan\n",
    "                    \n",
    "                    # Test Number of Prescriptions\n",
    "                    presc_values = subset_ts['Number of Prescriptions'].values\n",
    "                    presc_median = np.median(presc_values)\n",
    "                    presc_binary = (presc_values > presc_median).astype(int)\n",
    "                    \n",
    "                    if len(np.unique(presc_binary)) > 1:\n",
    "                        runs_stat_presc, p_val_presc = runstest_1samp(presc_binary)\n",
    "                    else:\n",
    "                        runs_stat_presc, p_val_presc = np.nan, np.nan\n",
    "                    \n",
    "                    all_results['national_atc2'].append({\n",
    "                        'ATC2_Class': atc2,\n",
    "                        'ATC2_Name': atc2_name,\n",
    "                        'N_Records': len(subset_ts),\n",
    "                        'N_States': subset['State'].nunique(),\n",
    "                        'Units_Runs_Stat': runs_stat_units,\n",
    "                        'Units_P_Value': p_val_units,\n",
    "                        'Presc_Runs_Stat': runs_stat_presc,\n",
    "                        'Presc_P_Value': p_val_presc\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing National-{atc2}: {e}\")\n",
    "        \n",
    "        # Display National x ATC2 results\n",
    "        national_atc2_df = pd.DataFrame(all_results['national_atc2'])\n",
    "        if not national_atc2_df.empty:\n",
    "            print(f\"\\nPanel 2 Results: {len(national_atc2_df)} national x ATC2 combinations analyzed\")\n",
    "            \n",
    "            units_significant = (national_atc2_df['Units_P_Value'] < 0.05).sum()\n",
    "            presc_significant = (national_atc2_df['Presc_P_Value'] < 0.05).sum()\n",
    "            total_valid = national_atc2_df['Units_P_Value'].notna().sum()\n",
    "            \n",
    "            print(f\"Units Reimbursed - Significant non-randomness: {units_significant}/{total_valid} ({100*units_significant/total_valid if total_valid > 0 else 0:.1f}%)\")\n",
    "            print(f\"Number of Prescriptions - Significant non-randomness: {presc_significant}/{total_valid} ({100*presc_significant/total_valid if total_valid > 0 else 0:.1f}%)\")\n",
    "            \n",
    "            print(f\"\\nTop 10 Most Significant Units Reimbursed Results (National x ATC2):\")\n",
    "            top_units_nat = national_atc2_df.nsmallest(10, 'Units_P_Value')[['ATC2_Class', 'ATC2_Name', 'Units_Runs_Stat', 'Units_P_Value', 'N_Records']].copy()\n",
    "            top_units_nat['Units_P_Value'] = top_units_nat['Units_P_Value'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(top_units_nat.to_string(index=False))\n",
    "            \n",
    "            print(f\"\\nTop 10 Most Significant Number of Prescriptions Results (National x ATC2):\")\n",
    "            top_presc_nat = national_atc2_df.nsmallest(10, 'Presc_P_Value')[['ATC2_Class', 'ATC2_Name', 'Presc_Runs_Stat', 'Presc_P_Value', 'N_Records']].copy()\n",
    "            top_presc_nat['Presc_P_Value'] = top_presc_nat['Presc_P_Value'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(top_presc_nat.to_string(index=False))\n",
    "        \n",
    "        # ==================== PANEL 3: ATC2 CLASSES x QUARTER RUNS TEST ====================\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PANEL 3: ATC2 CLASSES x QUARTER RUNS TEST\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # combined_df already exists from Panel 1\n",
    "        for atc2 in combined_df['ATC2 Class'].unique():\n",
    "            atc2_data = combined_df[combined_df['ATC2 Class'] == atc2]\n",
    "            \n",
    "            for quarter in sorted(atc2_data['Quarter'].unique()):\n",
    "                subset = atc2_data[atc2_data['Quarter'] == quarter]\n",
    "                \n",
    "                # AGGREGATE TO YEARLY TIME SERIES (for this specific quarter across years)\n",
    "                subset_ts = subset.groupby(['Year', 'Quarter']).agg({\n",
    "                    'Units Reimbursed': 'sum', \n",
    "                    'Number of Prescriptions': 'sum'\n",
    "                }).reset_index().sort_values(['Year','Quarter'])\n",
    "                \n",
    "                # DEBUG LINE\n",
    "                #if len(subset_ts) > 0:\n",
    "                #    print(f\"DEBUG: {atc2}-Q{quarter}: {len(subset_ts)} quarterly records\")\n",
    "                \n",
    "                if len(subset_ts) >= min_records:\n",
    "                    try:\n",
    "                        atc2_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns else ''\n",
    "                        \n",
    "                        # Test Units Reimbursed\n",
    "                        units_values = subset_ts['Units Reimbursed'].values\n",
    "                        units_median = np.median(units_values)\n",
    "                        units_binary = (units_values > units_median).astype(int)\n",
    "                        \n",
    "                        if len(np.unique(units_binary)) > 1:\n",
    "                            runs_stat_units, p_val_units = runstest_1samp(units_binary)\n",
    "                        else:\n",
    "                            runs_stat_units, p_val_units = np.nan, np.nan\n",
    "                        \n",
    "                        # Test Number of Prescriptions\n",
    "                        presc_values = subset_ts['Number of Prescriptions'].values\n",
    "                        presc_median = np.median(presc_values)\n",
    "                        presc_binary = (presc_values > presc_median).astype(int)\n",
    "                        \n",
    "                        if len(np.unique(presc_binary)) > 1:\n",
    "                            runs_stat_presc, p_val_presc = runstest_1samp(presc_binary)\n",
    "                        else:\n",
    "                            runs_stat_presc, p_val_presc = np.nan, np.nan\n",
    "                        \n",
    "                        all_results['atc2_quarter'].append({\n",
    "                            'ATC2_Class': atc2,\n",
    "                            'ATC2_Name': atc2_name,\n",
    "                            'Quarter': quarter,\n",
    "                            'N_Records': len(subset_ts),\n",
    "                            'N_Years': subset_ts['Year'].nunique(),\n",
    "                            'N_States': subset['State'].nunique(),\n",
    "                            'Units_Runs_Stat': runs_stat_units,\n",
    "                            'Units_P_Value': p_val_units,\n",
    "                            'Presc_Runs_Stat': runs_stat_presc,\n",
    "                            'Presc_P_Value': p_val_presc\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {atc2}-Q{quarter}: {e}\")\n",
    "        \n",
    "        # Display ATC2 x Quarter results\n",
    "        atc2_quarter_df = pd.DataFrame(all_results['atc2_quarter'])\n",
    "        if not atc2_quarter_df.empty:\n",
    "            print(f\"\\nATC2 x Quarter Results: {len(atc2_quarter_df)} combinations analyzed\")\n",
    "            \n",
    "            units_significant = (atc2_quarter_df['Units_P_Value'] < 0.05).sum()\n",
    "            presc_significant = (atc2_quarter_df['Presc_P_Value'] < 0.05).sum()\n",
    "            total_valid = atc2_quarter_df['Units_P_Value'].notna().sum()\n",
    "            \n",
    "            print(f\"Units Reimbursed - Significant non-randomness: {units_significant}/{total_valid} ({100*units_significant/total_valid if total_valid > 0 else 0:.1f}%)\")\n",
    "            print(f\"Number of Prescriptions - Significant non-randomness: {presc_significant}/{total_valid} ({100*presc_significant/total_valid if total_valid > 0 else 0:.1f}%)\")\n",
    "            \n",
    "            print(f\"\\nTop 10 Most Significant Units Reimbursed Results (ATC2 x Quarter):\")\n",
    "            top_units_quarter = atc2_quarter_df.nsmallest(10, 'Units_P_Value')[['ATC2_Class', 'ATC2_Name', 'Quarter', 'Units_Runs_Stat', 'Units_P_Value', 'N_Records']].copy()\n",
    "            top_units_quarter['Units_P_Value'] = top_units_quarter['Units_P_Value'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(top_units_quarter.to_string(index=False))\n",
    "            \n",
    "            print(f\"\\nTop 10 Most Significant Number of Prescriptions Results (ATC2 x Quarter):\")\n",
    "            top_presc_quarter = atc2_quarter_df.nsmallest(10, 'Presc_P_Value')[['ATC2_Class', 'ATC2_Name', 'Quarter', 'Presc_Runs_Stat', 'Presc_P_Value', 'N_Records']].copy()\n",
    "            top_presc_quarter['Presc_P_Value'] = top_presc_quarter['Presc_P_Value'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(top_presc_quarter.to_string(index=False))\n",
    "        \n",
    "        # ==================== SUMMARY SECTION ====================\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"RUNS TEST SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        summary_data = []\n",
    "        \n",
    "        if not state_atc2_df.empty:\n",
    "            state_units_sig = (state_atc2_df['Units_P_Value'] < 0.05).sum()\n",
    "            state_presc_sig = (state_atc2_df['Presc_P_Value'] < 0.05).sum()\n",
    "            state_total = state_atc2_df['Units_P_Value'].notna().sum()\n",
    "            summary_data.append(['State x ATC2', state_total, state_units_sig, state_presc_sig])\n",
    "        \n",
    "        if not national_atc2_df.empty:\n",
    "            nat_units_sig = (national_atc2_df['Units_P_Value'] < 0.05).sum()\n",
    "            nat_presc_sig = (national_atc2_df['Presc_P_Value'] < 0.05).sum()\n",
    "            nat_total = national_atc2_df['Units_P_Value'].notna().sum()\n",
    "            summary_data.append(['National x ATC2', nat_total, nat_units_sig, nat_presc_sig])\n",
    "        \n",
    "        if not atc2_quarter_df.empty:\n",
    "            quarter_units_sig = (atc2_quarter_df['Units_P_Value'] < 0.05).sum()\n",
    "            quarter_presc_sig = (atc2_quarter_df['Presc_P_Value'] < 0.05).sum()\n",
    "            quarter_total = atc2_quarter_df['Units_P_Value'].notna().sum()\n",
    "            summary_data.append(['ATC2 x Quarter', quarter_total, quarter_units_sig, quarter_presc_sig])\n",
    "        \n",
    "        if summary_data:\n",
    "            summary_df = pd.DataFrame(summary_data, columns=['Panel', 'Total Tests', 'Units Sig (p<0.05)', 'Prescriptions Sig (p<0.05)'])\n",
    "            summary_df['Units % Sig'] = (summary_df['Units Sig (p<0.05)'] / summary_df['Total Tests'] * 100).round(1)\n",
    "            summary_df['Prescriptions % Sig'] = (summary_df['Prescriptions Sig (p<0.05)'] / summary_df['Total Tests'] * 100).round(1)\n",
    "            print(summary_df.to_string(index=False))\n",
    "        \n",
    "        print(f\"\\nNote: Runs test examines whether data points occur in a random sequence.\")\n",
    "        print(f\"Significant p-values (< 0.05) indicate non-random patterns in the data.\")\n",
    "        print(f\"Lower runs statistics suggest more clustering; higher values suggest more alternation.\")\n",
    "        \n",
    "        # ==================== EXPORT PANEL DATAFRAMES ====================\n",
    "        if export_results:\n",
    "            export_dir = os.path.join(base_path, \"ATC\\\\exported_analysis\")\n",
    "            os.makedirs(export_dir, exist_ok=True)\n",
    "            if not state_atc2_df.empty:\n",
    "                state_atc2_df.to_csv(os.path.join(export_dir, \"runs_test_panel1_state_atc2.csv\"), index=False)\n",
    "            if not national_atc2_df.empty:\n",
    "                national_atc2_df.to_csv(os.path.join(export_dir, \"runs_test_panel2_national_atc2.csv\"), index=False)\n",
    "            if not atc2_quarter_df.empty:\n",
    "                atc2_quarter_df.to_csv(os.path.join(export_dir, \"runs_test_panel3_atc2_quarter.csv\"), index=False)\n",
    "            print(f\"Exported panel DataFrames to {export_dir}\")\n",
    "        \n",
    "        # ==================== HEATMAPS ====================\n",
    "        if generate_heatmaps:\n",
    "            \n",
    "            # Top classes by Units and Prescriptions (union of both top-30 lists)\n",
    "            if not combined_df.empty:\n",
    "                top_units = (combined_df.groupby('ATC2 Class')['Units Reimbursed'].sum()\n",
    "                            .sort_values(ascending=False).head(30).index.tolist())\n",
    "                top_presc = (combined_df.groupby('ATC2 Class')['Number of Prescriptions'].sum()\n",
    "                            .sort_values(ascending=False).head(30).index.tolist())\n",
    "                top_classes = list(dict.fromkeys(top_units + top_presc))\n",
    "            else:\n",
    "                top_classes = []\n",
    "            \n",
    "            def _prepare_pivot(df, value_col):\n",
    "                if 'ATC2_Class' not in df.columns:\n",
    "                    return pd.DataFrame()\n",
    "                filtered = df[df['ATC2_Class'].isin(top_classes)]\n",
    "                if filtered.empty:\n",
    "                    return pd.DataFrame()\n",
    "                agg = (filtered.groupby(['ATC2_Class', 'State'])[value_col]\n",
    "                            .mean().reset_index())\n",
    "                pivot = agg.pivot(index='ATC2_Class', columns='State', values=value_col)\n",
    "                return pivot.reindex(top_classes)\n",
    "            \n",
    "            def _prepare_pivot_state(df, value_col, state_code='IN'):\n",
    "                if 'ATC2_Class' not in df.columns:\n",
    "                    return pd.DataFrame()\n",
    "                filtered = df[(df['ATC2_Class'].isin(top_classes)) & (df['State'] == state_code)]\n",
    "                if filtered.empty:\n",
    "                    return pd.DataFrame()\n",
    "                agg = (filtered.groupby(['ATC2_Class', 'State'])[value_col]\n",
    "                            .mean().reset_index())\n",
    "                pivot = agg.pivot(index='ATC2_Class', columns='State', values=value_col)\n",
    "                return pivot.reindex(top_classes)\n",
    "            \n",
    "            def _prepare_pivot_quarter(df, value_col):\n",
    "                if 'ATC2_Class' not in df.columns:\n",
    "                    return pd.DataFrame()\n",
    "                filtered = df[df['ATC2_Class'].isin(top_classes)]\n",
    "                if filtered.empty or 'Quarter' not in filtered.columns:\n",
    "                    return pd.DataFrame()\n",
    "                agg = (filtered.groupby(['ATC2_Class', 'Quarter'])[value_col]\n",
    "                            .mean().reset_index())\n",
    "                pivot = agg.pivot(index='ATC2_Class', columns='Quarter', values=value_col)\n",
    "                return pivot.reindex(top_classes)\n",
    "            \n",
    "            def _plot_heatmap(pivot_df, title):\n",
    "                if pivot_df.empty:\n",
    "                    print(f\"No data for {title}\")\n",
    "                    return\n",
    "                cmap = LinearSegmentedColormap.from_list('pval_map', ['green', 'yellow', 'red'])\n",
    "                plt.figure(figsize=(14, max(6, len(pivot_df)*0.35)))\n",
    "                sns.heatmap(pivot_df, cmap=cmap, vmin=0.01, vmax=0.1, annot=False, cbar_kws={'label': 'Avg p-value'}, linewidths=0.5)\n",
    "                plt.title(title, fontsize=12, fontweight='bold')\n",
    "                plt.xlabel(pivot_df.columns.name if pivot_df.columns.name else '')\n",
    "                plt.ylabel('ATC2 Class')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Panel 1 (State x ATC2)\n",
    "            units_pivot = _prepare_pivot(state_atc2_df, 'Units_P_Value')\n",
    "            presc_pivot = _prepare_pivot(state_atc2_df, 'Presc_P_Value')\n",
    "            _plot_heatmap(units_pivot, 'Runs Test p-values by State (Units Reimbursed)')\n",
    "            _plot_heatmap(presc_pivot, 'Runs Test p-values by State (Number of Prescriptions)')\n",
    "            \n",
    "            # Indiana-only heatmap\n",
    "            in_units_pivot = _prepare_pivot_state(state_atc2_df, 'Units_P_Value', state_code='IN')\n",
    "            in_presc_pivot = _prepare_pivot_state(state_atc2_df, 'Presc_P_Value', state_code='IN')\n",
    "            _plot_heatmap(in_units_pivot, 'Runs Test p-values for Indiana (Units Reimbursed)')\n",
    "            _plot_heatmap(in_presc_pivot, 'Runs Test p-values for Indiana (Number of Prescriptions)')\n",
    "            \n",
    "            # Panel 3 (ATC2 x Quarter)\n",
    "            units_quarter_pivot = _prepare_pivot_quarter(atc2_quarter_df, 'Units_P_Value')\n",
    "            presc_quarter_pivot = _prepare_pivot_quarter(atc2_quarter_df, 'Presc_P_Value')\n",
    "            _plot_heatmap(units_quarter_pivot, 'Runs Test p-values by Quarter (Units)')\n",
    "            _plot_heatmap(presc_quarter_pivot, 'Runs Test p-values by Quarter (Prescriptions)')\n",
    "        \n",
    "        return {\n",
    "            'state_atc2': state_atc2_df,\n",
    "            'national_atc2': national_atc2_df, \n",
    "            'atc2_quarter': atc2_quarter_df\n",
    "        }\n",
    "\n",
    "    #new function\n",
    "    @staticmethod\n",
    "    def build_state_atc2_quarterly_panel(years_list, base_path=None, export_results=True):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        \n",
    "        # Load and validate data\n",
    "        all_records = []\n",
    "        for year in years_list:\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(base_path, f\"ATC\\\\merged_data\\\\unscaled_data\\\\MUD_{year}.csv\"))\n",
    "                # Filter valid records (ATC2 Class column already exists in dataset)\n",
    "                df = df[df[['State', 'ATC2 Class', 'Year', 'Quarter', 'Number of Prescriptions', 'Units Reimbursed']].notna().all(axis=1)]\n",
    "                df = df[(df['Number of Prescriptions'] >= 0) & (df['Units Reimbursed'] >= 0)]\n",
    "                df = df[['State', 'ATC2 Class', 'Year', 'Quarter', 'Number of Prescriptions', 'Units Reimbursed']].copy()\n",
    "                df[['Year', 'Quarter']] = df[['Year', 'Quarter']].astype(int)\n",
    "                all_records.append(df)\n",
    "                print(f\"  {year}: {len(df):,} valid records\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {year}: Error - {e}\")\n",
    "        \n",
    "        if not all_records:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        combined_df = pd.concat(all_records, ignore_index=True)\n",
    "        \n",
    "        # Get all unique values for balanced panel\n",
    "        all_states = combined_df['State'].unique()\n",
    "        all_atc2 = combined_df['ATC2 Class'].unique()\n",
    "        all_quarters = pd.period_range(\n",
    "            start=pd.Period(year=combined_df['Year'].min(), quarter=1, freq='Q'),\n",
    "            end=pd.Period(year=combined_df['Year'].max(), quarter=4, freq='Q'),\n",
    "            freq='Q'\n",
    "        )\n",
    "        print(f\"\\nAll ATC2 classes: {len(all_atc2)} | States: {len(all_states)} | Quarters: {len(all_quarters)}\")\n",
    "        \n",
    "        # Aggregate by State, ATC2, Year, Quarter (sum is additive across NDCs)\n",
    "        panel_df = combined_df.groupby(['State', 'ATC2 Class', 'Year', 'Quarter'], as_index=False).agg({\n",
    "            'Units Reimbursed': 'sum', 'Number of Prescriptions': 'sum'\n",
    "        }).rename(columns={'ATC2 Class': 'ATC2_Class', 'Units Reimbursed': 'Units_Reimbursed', \n",
    "                          'Number of Prescriptions': 'Number_of_Prescriptions'})\n",
    "        \n",
    "        # Create quarterly PeriodIndex\n",
    "        panel_df['Quarter_Period'] = pd.PeriodIndex.from_fields(year=panel_df['Year'], quarter=panel_df['Quarter'], freq='Q')\n",
    "        panel_df = panel_df[['State', 'ATC2_Class', 'Quarter_Period', 'Units_Reimbursed', 'Number_of_Prescriptions']]\n",
    "        \n",
    "        # Create complete balanced panel with all State x ATC2 x Quarter combinations\n",
    "        from itertools import product\n",
    "        full_index = pd.DataFrame(list(product(all_states, all_atc2, all_quarters)), \n",
    "                                  columns=['State', 'ATC2_Class', 'Quarter_Period'])\n",
    "        \n",
    "        # Merge to get balanced panel (missing combinations will have NaN)\n",
    "        panel_df = full_index.merge(panel_df, on=['State', 'ATC2_Class', 'Quarter_Period'], how='left')\n",
    "        panel_df = panel_df.sort_values(['State', 'ATC2_Class', 'Quarter_Period']).reset_index(drop=True)\n",
    "        \n",
    "        # Summary\n",
    "        n_missing = panel_df['Units_Reimbursed'].isna().sum()\n",
    "        print(f\"\\nBalanced Panel: {len(panel_df):,} rows | {panel_df['State'].nunique()} states | {panel_df['ATC2_Class'].nunique()} ATC2 classes\")\n",
    "        print(f\"Period: {panel_df['Quarter_Period'].min()} to {panel_df['Quarter_Period'].max()}\")\n",
    "        print(f\"Missing observations (NaN): {n_missing:,} ({100*n_missing/len(panel_df):.1f}%)\")\n",
    "        \n",
    "        # Export\n",
    "        if export_results:\n",
    "            export_path = os.path.join(base_path, \"ATC\\\\merged_data\\\\Prebuilt_panels\\\\state_atc2_quarterly_panel_arima.csv\")\n",
    "            os.makedirs(os.path.dirname(export_path), exist_ok=True)\n",
    "            export_df = panel_df.copy()\n",
    "            export_df['Quarter_Period'] = export_df['Quarter_Period'].astype(str)\n",
    "            export_df.to_csv(export_path, index=False)\n",
    "            print(f\"Exported to: {export_path}\")\n",
    "        \n",
    "        return panel_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04705709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#covarience_results = NDCATC_ind.covariance_look(years_list)\n",
    "\n",
    "# Example 1: Indiana only\n",
    "correlation_results_IN = NDCATC_stats.correlation_look(years_list, state_filter='IN')\n",
    "\n",
    "# Example 2: All states (original behavior)\n",
    "# correlation_results_all = NDCATC_stats.correlation_look(years_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b269fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2017: 1,931,088 valid records\n",
      "  2018: 2,022,672 valid records\n",
      "  2019: 2,109,684 valid records\n",
      "  2020: 2,173,775 valid records\n",
      "  2021: 2,287,508 valid records\n",
      "  2022: 2,353,181 valid records\n",
      "  2023: 2,385,896 valid records\n",
      "  2024: 2,338,667 valid records\n",
      "\n",
      "All ATC2 classes: 90 | States: 52 | Quarters: 32\n",
      "\n",
      "Balanced Panel: 149,760 rows | 52 states | 90 ATC2 classes\n",
      "Period: 2017Q1 to 2024Q4\n",
      "Missing observations (NaN): 15,501 (10.4%)\n",
      "Exported to: c:\\Users\\Lilian\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\merged_data\\Prebuilt_panels\\state_atc2_quarterly_panel_arima.csv\n"
     ]
    }
   ],
   "source": [
    "# Calling the method build_state_atc2_quarterly_panel()\n",
    "panel_results = NDCATC_stats.build_state_atc2_quarterly_panel(years_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71bc133",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the three-panel correlation analysis\n",
    "correlation_results = NDCATC_stats.correlation_look(years_list, min_records=8)\n",
    "\n",
    "# Uncomment the line below to run the analysis\n",
    "# runs_results = NDCATC_ind.runs_test_analysis(years_list, min_records=25, generate_heatmaps=True, export_results=True)\n",
    "runs_results = NDCATC_stats.runs_test_analysis(years_list, min_records=8, generate_heatmaps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c4e7f4",
   "metadata": {},
   "source": [
    "#### Working with Medicaid Enrollment Data collected through MBES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abb0f2c",
   "metadata": {},
   "source": [
    "##### Both scaled and unscaled data are merged with the population (Medicaid Enrollment), the unscaled datasets (raw and original datasets) are used initially for the ML models. The unscaled data should be considered first for the ML models, the scaling could be included inside the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b39af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking the datatypes in the column\n",
    "enroll_csv_path1=rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\merged_data\\merged_pop_unscaled\\MUD_2016_with_pop.csv\"\n",
    "enroll_csv_path2=rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\merged_data\\merged_pop_scaled\\merged_NEWdata_2016_with_pop.csv\"\n",
    "df_enroll1 = pd.read_csv(enroll_csv_path1)\n",
    "df_enroll2= pd.read_csv(enroll_csv_path2)\n",
    "\n",
    "#comparing number of rows in both dataframes\n",
    "print(f\"Number of rows in unscaled enrollment data: {len(df_enroll1)}\")\n",
    "print(f\"Number of rows in scaled enrollment data: {len(df_enroll2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b89dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pop_SDUD:\n",
    "   \n",
    "    def __init__(self):\n",
    "        \n",
    "        #Path to enrollement data\n",
    "        self.data_directory = rf'c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\Medicaid_ENR'\n",
    "        self.output_filename = 'all_enrolled.csv'\n",
    "\n",
    "    def build_quarterly_panel(self):\n",
    "    \n",
    "        file_pattern = os.path.join(self.data_directory, \"*.csv\")\n",
    "        csv_files = glob.glob(file_pattern)\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(f\"No CSV files found in directory: {self.data_directory}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Found {len(csv_files)} files. Loading and concatenating...\")\n",
    "        \n",
    "        df_list = []\n",
    "        for file in csv_files:\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                df_list.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        if not df_list:\n",
    "            return None\n",
    "    \n",
    "        full_df = pd.concat(df_list, ignore_index=True)\n",
    "        \n",
    "        # Clean types\n",
    "        full_df['Enrollment Month'] = pd.to_numeric(full_df.get('Enrollment Month'), errors='coerce')\n",
    "        full_df['Total Medicaid Enrollees'] = (\n",
    "            full_df['Total Medicaid Enrollees'].astype(str).str.replace(',', '', regex=False)\n",
    "        )\n",
    "        full_df['Total Medicaid Enrollees'] = pd.to_numeric(full_df['Total Medicaid Enrollees'], errors='coerce')\n",
    "    \n",
    "        # Preserve state name and create 2-letter code\n",
    "        if 'State' in full_df.columns:\n",
    "            # Keep original name trimmed (do not force title-case to avoid mismatches)\n",
    "            full_df['State Name'] = full_df['State'].astype(str).str.strip()\n",
    "    \n",
    "            state_map_code_to_name = {\n",
    "                'AL':'Alabama','AK':'Alaska','AZ':'Arizona','AR':'Arkansas','CA':'California','CO':'Colorado','CT':'Connecticut',\n",
    "                'DE':'Delaware','DC':'District of Columbia','FL':'Florida','GA':'Georgia','HI':'Hawaii','ID':'Idaho','IL':'Illinois',\n",
    "                'IN':'Indiana','IA':'Iowa','KS':'Kansas','KY':'Kentucky','LA':'Louisiana','ME':'Maine','MD':'Maryland','MA':'Massachusetts',\n",
    "                'MI':'Michigan','MN':'Minnesota','MS':'Mississippi','MO':'Missouri','MT':'Montana','NE':'Nebraska','NV':'Nevada','NH':'New Hampshire',\n",
    "                'NJ':'New Jersey','NM':'New Mexico','NY':'New York','NC':'North Carolina','ND':'North Dakota','OH':'Ohio','OK':'Oklahoma','OR':'Oregon',\n",
    "                'PA':'Pennsylvania','RI':'Rhode Island','SC':'South Carolina','SD':'South Dakota','TN':'Tennessee','TX':'Texas','UT':'Utah','VT':'Vermont',\n",
    "                'VA':'Virginia','WA':'Washington','WV':'West Virginia','WI':'Wisconsin','WY':'Wyoming', 'Totals':'Totals',\n",
    "                # Territories and Totals\n",
    "                'AS':'Amer. Samoa','GU':'Guam','MP':'N. Mariana Islands','PR':'Puerto Rico','VI':'Virgin Islands','TT':'Trust Territories'\n",
    "            }\n",
    "    \n",
    "            state_map_name_to_code = {v: k for k, v in state_map_code_to_name.items()}\n",
    "\n",
    "            # Exact match on trimmed names\n",
    "            full_df['State Name'] = full_df['State'].astype(str).str.strip()\n",
    "            full_df['State'] = full_df['State Name'].map(state_map_name_to_code)\n",
    "\n",
    "        # Quarter from month\n",
    "        full_df['Quarter'] = (full_df['Enrollment Month'] - 1) // 3 + 1\n",
    "    \n",
    "        # Group by 2-letter code\n",
    "        group_cols = ['State', 'Enrollment Year', 'Quarter']\n",
    "        quarterly_panel = full_df.groupby(group_cols, as_index=False)['Total Medicaid Enrollees'].mean()\n",
    "    \n",
    "        # Rename metric\n",
    "        quarterly_panel.rename(columns={'Total Medicaid Enrollees': 'Average_pop'}, inplace=True)\n",
    "        # Rename year column to only \"Year\"\n",
    "        quarterly_panel.rename(columns={'Enrollment Year': 'Year'}, inplace=True)\n",
    "\n",
    "        # Save\n",
    "        output_path = os.path.join(self.data_directory, self.output_filename)\n",
    "        quarterly_panel.to_csv(output_path, index=False)\n",
    "        print(f\"Successfully saved quarterly panel to: {output_path}\")\n",
    "        \n",
    "        return quarterly_panel\n",
    "    \n",
    "    def merge_enroll(self):\n",
    "\n",
    "        # Paths\n",
    "        enroll_path = os.path.join(self.data_directory, self.output_filename)  # all_enrolled.csv\n",
    "        src_dir = rf\"C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\merged_data\\unscaled_data\"\n",
    "        dest_dir = rf\"C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\merged_data\\merged_pop_unscaled\"\n",
    "\n",
    "        # Ensure destination folder exists\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "        # Load enrollment panel\n",
    "        if not os.path.exists(enroll_path):\n",
    "            print(f\"Enrollment panel not found: {enroll_path}\")\n",
    "            return None\n",
    "\n",
    "        enr = pd.read_csv(enroll_path)\n",
    "\n",
    "        # Normalize column names\n",
    "        if \"Enrollment Year\" in enr.columns and \"Year\" not in enr.columns:\n",
    "            enr = enr.rename(columns={\"Enrollment Year\": \"Year\"})\n",
    "        if \"Average pop\" in enr.columns and \"Average_pop\" not in enr.columns:\n",
    "            enr = enr.rename(columns={\"Average pop\": \"Average_pop\"})\n",
    "\n",
    "        # Clean types\n",
    "        for df in (enr,):\n",
    "            if \"State\" in df.columns:\n",
    "                df[\"State\"] = df[\"State\"].astype(str).str.strip()\n",
    "            if \"Year\" in df.columns:\n",
    "                df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\")\n",
    "            if \"Quarter\" in df.columns:\n",
    "                df[\"Quarter\"] = pd.to_numeric(df[\"Quarter\"], errors=\"coerce\")\n",
    "\n",
    "        # Keep only required columns\n",
    "        required = {\"State\", \"Year\", \"Quarter\", \"Average_pop\"}\n",
    "        if not required.issubset(enr.columns):\n",
    "            print(\"Enrollment file missing required columns: State, Year, Quarter, Average_pop\")\n",
    "            return None\n",
    "        enr_key = enr[list(required)].drop_duplicates(subset=[\"State\", \"Year\", \"Quarter\"])\n",
    "\n",
    "        # Process each CSV in source folder - FIXED to exclude already processed files\n",
    "        all_files = glob.glob(os.path.join(src_dir, \"*.csv\"))\n",
    "        merged_files = [f for f in all_files \n",
    "                        if '_with_pop.csv' not in os.path.basename(f) \n",
    "                        and os.path.dirname(f) == src_dir]  # Only files directly in src_dir\n",
    "        \n",
    "        if not merged_files:\n",
    "            print(f\"No CSV files found in: {src_dir}\")\n",
    "            return None\n",
    "\n",
    "        outputs = []\n",
    "        for fpath in sorted(merged_files):\n",
    "            try:\n",
    "                base = os.path.basename(fpath)\n",
    "                stem, _ = os.path.splitext(base)\n",
    "\n",
    "                df = pd.read_csv(fpath)\n",
    "                if \"State\" in df.columns:\n",
    "                    df[\"State\"] = df[\"State\"].astype(str).str.strip()\n",
    "                if \"Year\" in df.columns:\n",
    "                    df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\")\n",
    "                if \"Quarter\" in df.columns:\n",
    "                    df[\"Quarter\"] = pd.to_numeric(df[\"Quarter\"], errors=\"coerce\")\n",
    "\n",
    "                out_df = pd.merge(\n",
    "                    df, enr_key,\n",
    "                    on=[\"State\", \"Year\", \"Quarter\"],\n",
    "                    how=\"left\",\n",
    "                    validate=\"m:1\"\n",
    "                )\n",
    "\n",
    "                out_name = f\"{stem}_with_pop.csv\"\n",
    "                out_path = os.path.join(dest_dir, out_name)\n",
    "                out_df.to_csv(out_path, index=False)\n",
    "                print(f\"Saved: {out_path}\")\n",
    "                outputs.append(out_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {fpath}: {e}\")\n",
    "        \n",
    "        print(\"Merging complete.\")\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def build_atc2_subpanel(self):\n",
    " \n",
    "        # Path to merged data with population\n",
    "        merged_pop_dir = rf\"C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\merged_data\\merged_pop_unscaled\"\n",
    "        \n",
    "        # Find all _with_pop.csv files\n",
    "        file_pattern = os.path.join(merged_pop_dir, \"*_with_pop.csv\")\n",
    "        csv_files = glob.glob(file_pattern)\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(f\"No _with_pop.csv files found in directory: {merged_pop_dir}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Found {len(csv_files)} _with_pop.csv files. Processing...\")\n",
    "        \n",
    "        # Required columns for validation\n",
    "        required_cols = [\"ATC2 Class\", \"State\", \"Year\", \"Quarter\", \"Units Reimbursed\", \"Number of Prescriptions\", \"Average_pop\"]\n",
    "        \n",
    "        valid_dfs = []\n",
    "        for file_path in csv_files:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Verify required columns exist\n",
    "                missing_cols = set(required_cols) - set(df.columns)\n",
    "                if missing_cols:\n",
    "                    print(f\"Skipping {os.path.basename(file_path)}: missing columns {missing_cols}\")\n",
    "                    continue\n",
    "                \n",
    "                # Keep only required columns\n",
    "                df = df[required_cols].copy()\n",
    "                \n",
    "                # Clean data types\n",
    "                df[\"State\"] = df[\"State\"].astype(str).str.strip()\n",
    "                df[\"ATC2 Class\"] = df[\"ATC2 Class\"].astype(str).str.strip()\n",
    "                df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\")\n",
    "                df[\"Quarter\"] = pd.to_numeric(df[\"Quarter\"], errors=\"coerce\")\n",
    "                df[\"Units Reimbursed\"] = pd.to_numeric(df[\"Units Reimbursed\"], errors=\"coerce\")\n",
    "                df[\"Number of Prescriptions\"] = pd.to_numeric(df[\"Number of Prescriptions\"], errors=\"coerce\")\n",
    "                df[\"Average_pop\"] = pd.to_numeric(df[\"Average_pop\"], errors=\"coerce\")\n",
    "                \n",
    "                # Drop rows with missing or zero population\n",
    "                initial_rows = len(df)\n",
    "                df = df.dropna(subset=[\"Average_pop\"])\n",
    "                df = df[df[\"Average_pop\"] > 0]\n",
    "                dropped_rows = initial_rows - len(df)\n",
    "                if dropped_rows > 0:\n",
    "                    print(f\"Dropped {dropped_rows} rows with missing/zero population from {os.path.basename(file_path)}\")\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    valid_dfs.append(df)\n",
    "                    print(f\"Successfully loaded {os.path.basename(file_path)}: {len(df)} valid rows\")\n",
    "                else:\n",
    "                    print(f\"Skipping {os.path.basename(file_path)}: no valid rows after population filter\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {os.path.basename(file_path)}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not valid_dfs:\n",
    "            print(\"No valid dataframes to process\")\n",
    "            return None\n",
    "        \n",
    "        # Concatenate all valid dataframes\n",
    "        print(\"Concatenating valid dataframes...\")\n",
    "        full_df = pd.concat(valid_dfs, ignore_index=True)\n",
    "        print(f\"Combined dataset: {len(full_df)} total rows\")\n",
    "        \n",
    "        # Aggregate by ATC2 Class, State, Year, Quarter\n",
    "        print(\"Aggregating data by ATC2 Class, State, Year, Quarter...\")\n",
    "        agg_funcs = {\n",
    "            \"Units Reimbursed\": \"sum\",\n",
    "            \"Number of Prescriptions\": \"sum\",\n",
    "            \"Average_pop\": \"first\"  # Population should be constant within state-year-quarter\n",
    "        }\n",
    "        \n",
    "        group_cols = [\"ATC2 Class\", \"State\", \"Year\", \"Quarter\"]\n",
    "        aggregated_df = full_df.groupby(group_cols, as_index=False).agg(agg_funcs)\n",
    "        \n",
    "        print(f\"Aggregated to {len(aggregated_df)} unique ATC2-State-Year-Quarter combinations\")\n",
    "        \n",
    "        # Compute population-normalized rates\n",
    "        print(\"Computing population-normalized rates...\")\n",
    "        aggregated_df[\"units_per_1000\"] = (aggregated_df[\"Units Reimbursed\"] / aggregated_df[\"Average_pop\"]) * 1000\n",
    "        aggregated_df[\"NoP_per_1000\"] = (aggregated_df[\"Number of Prescriptions\"] / aggregated_df[\"Average_pop\"]) * 1000\n",
    "        \n",
    "        # Keep required output columns including raw values and population\n",
    "        output_cols = [\"ATC2 Class\", \"State\", \"Year\", \"Quarter\", \"Units Reimbursed\", \"Number of Prescriptions\", \"Average_pop\", \"units_per_1000\", \"NoP_per_1000\"]\n",
    "        final_df = aggregated_df[output_cols].copy()\n",
    "        \n",
    "        # Sort by State, ATC2 Class, Year, Quarter\n",
    "        print(\"Sorting output...\")\n",
    "        final_df = final_df.sort_values([\"State\", \"ATC2 Class\", \"Year\", \"Quarter\"]).reset_index(drop=True)\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_filename = \"pop_unscaled_subpanel.csv\"\n",
    "        output_path = os.path.join(merged_pop_dir, output_filename)\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        print(f\"Successfully saved ATC2 sub-panel to: {output_path}\")\n",
    "        print(f\"Final dataset: {len(final_df)} rows across {final_df['ATC2 Class'].nunique()} ATC2 classes\")\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    def analyze_top_states_by_pop_normalized_metrics(self, top_n_states=20, top_n_classes=15):\n",
    "  \n",
    "        # Path to the ATC2 subpanel data\n",
    "        merged_pop_dir = rf\"C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\merged_data\\merged_pop_unscaled\"\n",
    "        subpanel_path = os.path.join(merged_pop_dir, \"pop_unscaled_subpanel.csv\")\n",
    "        \n",
    "        if not os.path.exists(subpanel_path):\n",
    "            print(f\"ATC2 subpanel file not found: {subpanel_path}\"); print(\"Please run build_atc2_subpanel() first to create the subpanel data.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"ANALYZING TOP {top_n_states} STATES BY POPULATION-NORMALIZED ATC2 METRICS\"); print(\"=\"*80)\n",
    "        \n",
    "        # Load the subpanel data\n",
    "        print(f\"Loading ATC2 subpanel data from: {subpanel_path}\")\n",
    "        try:\n",
    "            combined_df = pd.read_csv(subpanel_path)\n",
    "            print(f\"✓ Loaded {len(combined_df):,} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading data: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Validate required columns\n",
    "        required_cols = ['ATC2 Class', 'State', 'Year', 'Quarter', 'units_per_1000', 'NoP_per_1000', 'Average_pop']\n",
    "        missing_cols = set(required_cols) - set(combined_df.columns)\n",
    "        if missing_cols:\n",
    "            print(f\"Missing required columns: {missing_cols}\")\n",
    "            return None\n",
    "        \n",
    "        # Use ATC2 Class directly\n",
    "        combined_df['ATC_Level'] = combined_df['ATC2 Class']\n",
    "        \n",
    "        # Get top ATC2 classes nationally (by total units_per_1000)\n",
    "        print(f\"\\nFinding top {top_n_classes} ATC2 classes nationally by population-normalized units...\")\n",
    "        national_summary = combined_df.groupby('ATC_Level').agg(\n",
    "            Total_Units_per_1000=('units_per_1000', 'sum'),\n",
    "            Total_Rx_per_1000=('NoP_per_1000', 'sum'),\n",
    "            Total_Records=('ATC2 Class', 'count'),\n",
    "            Unique_States=('State', 'nunique'),\n",
    "            Avg_Population=('Average_pop', 'mean')\n",
    "        ).sort_values('Total_Units_per_1000', ascending=False)\n",
    "        \n",
    "        top_classes = national_summary.head(top_n_classes).index.tolist()\n",
    "        \n",
    "        print(f\"Top {len(top_classes)} ATC2 classes by total population-normalized units:\")\n",
    "        for i, cls in enumerate(top_classes[:10], 1):\n",
    "            units_per_1k = national_summary.loc[cls, 'Total_Units_per_1000']\n",
    "            rx_per_1k = national_summary.loc[cls, 'Total_Rx_per_1000']\n",
    "            states = national_summary.loc[cls, 'Unique_States']\n",
    "            print(f\"  {i:2d}. {cls}: {units_per_1k:.1f} units/1K, {rx_per_1k:.1f} rx/1K ({states} states)\")\n",
    "        \n",
    "        if len(top_classes) > 10:\n",
    "            print(f\"  ... and {len(top_classes) - 10} more classes\")\n",
    "        \n",
    "        # Analyze each top class by state\n",
    "        class_results = {}\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ANALYZING TOP STATES FOR EACH ATC2 CLASS (POPULATION-NORMALIZED)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for i, atc_class in enumerate(top_classes, 1):\n",
    "            print(f\"\\n{i:2d}. Analyzing {atc_class}...\")\n",
    "            \n",
    "            class_data = combined_df[combined_df['ATC_Level'] == atc_class].copy()\n",
    "            \n",
    "            if class_data.empty:\n",
    "                continue\n",
    "            \n",
    "            # Aggregate by state (sum across all years/quarters)\n",
    "            state_summary = class_data.groupby('State').agg(\n",
    "                Total_Units_per_1000=('units_per_1000', 'sum'),\n",
    "                Total_Rx_per_1000=('NoP_per_1000', 'sum'),\n",
    "                Total_Records=('ATC2 Class', 'count'),\n",
    "                Avg_Population=('Average_pop', 'mean'),\n",
    "                Years_Covered=('Year', 'nunique'),\n",
    "                Quarters_Covered=('Quarter', 'nunique')\n",
    "            )\n",
    "            \n",
    "            # Calculate percentages\n",
    "            total_class_units_per_1k = state_summary['Total_Units_per_1000'].sum()\n",
    "            total_class_rx_per_1k = state_summary['Total_Rx_per_1000'].sum()\n",
    "            \n",
    "            state_summary['Units_per_1000_Percentage'] = (\n",
    "                state_summary['Total_Units_per_1000'] / total_class_units_per_1k * 100\n",
    "            ).round(2)\n",
    "            \n",
    "            state_summary['Rx_per_1000_Percentage'] = (\n",
    "                state_summary['Total_Rx_per_1000'] / total_class_rx_per_1k * 100\n",
    "            ).round(2)\n",
    "            \n",
    "            # Sort by population-normalized metrics and get top states\n",
    "            top_states_units = state_summary.sort_values('Total_Units_per_1000', ascending=False).head(top_n_states)\n",
    "            top_states_rx = state_summary.sort_values('Total_Rx_per_1000', ascending=False).head(top_n_states)\n",
    "            \n",
    "            # Store results\n",
    "            class_results[atc_class] = {\n",
    "                'total_units_per_1000': total_class_units_per_1k,\n",
    "                'total_rx_per_1000': total_class_rx_per_1k,\n",
    "                'top_states_units': top_states_units,\n",
    "                'top_states_rx': top_states_rx,\n",
    "                'all_states': state_summary\n",
    "            }\n",
    "            \n",
    "            # Print summary for this class\n",
    "            print(f\"   Total: {total_class_units_per_1k:.1f} units/1K population, {total_class_rx_per_1k:.1f} rx/1K population\")\n",
    "            print(f\"   States with data: {len(state_summary)}\")\n",
    "            \n",
    "            # Print top 10 states by units per 1000\n",
    "            print(f\"   Top 10 by Units per 1000 Population:\")\n",
    "            for j, (state, row) in enumerate(top_states_units.head(10).iterrows(), 1):\n",
    "                print(f\"     {j:2d}. {state}: {row['Total_Units_per_1000']:.1f} ({row['Units_per_1000_Percentage']:.1f}%)\")\n",
    "            \n",
    "            # Print top 10 states by prescriptions per 1000\n",
    "            print(f\"   Top 10 by Prescriptions per 1000 Population:\")\n",
    "            for j, (state, row) in enumerate(top_states_rx.head(10).iterrows(), 1):\n",
    "                print(f\"     {j:2d}. {state}: {row['Total_Rx_per_1000']:.1f} ({row['Rx_per_1000_Percentage']:.1f}%)\")\n",
    "        \n",
    "        # Create summary heatmap visualizations\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"CREATING HEATMAP VISUALIZATIONS (POPULATION-NORMALIZED)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Get top 20 states overall by units per 1000\n",
    "        state_totals_units = combined_df.groupby('State')['units_per_1000'].sum().sort_values(ascending=False)\n",
    "        top_states_overall_units = state_totals_units.head(20).index.tolist()\n",
    "        \n",
    "        # Get top 20 states overall by rx per 1000\n",
    "        state_totals_rx = combined_df.groupby('State')['NoP_per_1000'].sum().sort_values(ascending=False)\n",
    "        top_states_overall_rx = state_totals_rx.head(20).index.tolist()\n",
    "        \n",
    "        # Use top 10 classes for heatmaps\n",
    "        heatmap_columns = top_classes[:10]\n",
    "        \n",
    "        # 1. Create heatmap data for Units per 1000 (RESCALED BY MILLION for heatmap only)\n",
    "        print(\"Creating heatmap of Units per 1000 Population by State and ATC2 Class...\")\n",
    "        \n",
    "        heatmap_data_units = []\n",
    "        heatmap_index_units = []\n",
    "        \n",
    "        for state in top_states_overall_units:\n",
    "            row_data = []\n",
    "            for atc_class in heatmap_columns:\n",
    "                if atc_class in class_results and state in class_results[atc_class]['all_states'].index:\n",
    "                    value = class_results[atc_class]['all_states'].loc[state, 'Total_Units_per_1000']\n",
    "                    # RESCALE UNITS BY MILLION FOR HEATMAP ONLY\n",
    "                    value = value / 1_000_000\n",
    "                else:\n",
    "                    value = 0\n",
    "                row_data.append(value)\n",
    "            heatmap_data_units.append(row_data)\n",
    "            heatmap_index_units.append(state)\n",
    "        \n",
    "        heatmap_df_units = pd.DataFrame(heatmap_data_units, index=heatmap_index_units, columns=heatmap_columns)\n",
    "        \n",
    "        # 2. Create heatmap data for Prescriptions per 1000 (NO RESCALING)\n",
    "        print(\"Creating heatmap of Prescriptions per 1000 Population by State and ATC2 Class...\")\n",
    "        \n",
    "        heatmap_data_rx = []\n",
    "        heatmap_index_rx = []\n",
    "        \n",
    "        for state in top_states_overall_rx:\n",
    "            row_data = []\n",
    "            for atc_class in heatmap_columns:\n",
    "                if atc_class in class_results and state in class_results[atc_class]['all_states'].index:\n",
    "                    value = class_results[atc_class]['all_states'].loc[state, 'Total_Rx_per_1000']\n",
    "                    # NO RESCALING FOR PRESCRIPTIONS\n",
    "                else:\n",
    "                    value = 0\n",
    "                row_data.append(value)\n",
    "            heatmap_data_rx.append(row_data)\n",
    "            heatmap_index_rx.append(state)\n",
    "        \n",
    "        heatmap_df_rx = pd.DataFrame(heatmap_data_rx, index=heatmap_index_rx, columns=heatmap_columns)\n",
    "        \n",
    "        # Create side-by-side heatmaps\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(32, 12))\n",
    "        \n",
    "        # Units per 1000 heatmap (RESCALED)\n",
    "        sns.heatmap(heatmap_df_units, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "                    cbar_kws={'label': 'Units per 1000 Population (Millions)'},  # Updated label\n",
    "                    annot_kws={'size': 8}, ax=ax1)\n",
    "        ax1.set_title(f'Units per 1000 Population by State and ATC2 Class\\n(Top 20 States vs Top 10 Classes - Units in Millions)', \n",
    "                      fontweight='bold', pad=20)  # Updated title\n",
    "        ax1.set_xlabel('ATC2 Classes', fontweight='bold')\n",
    "        ax1.set_ylabel('States', fontweight='bold')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        ax1.tick_params(axis='y', rotation=0)\n",
    "        \n",
    "        # Prescriptions per 1000 heatmap (NOT RESCALED)\n",
    "        sns.heatmap(heatmap_df_rx, annot=True, fmt='.1f', cmap='Blues', \n",
    "                    cbar_kws={'label': 'Prescriptions per 1000 Population'},\n",
    "                    annot_kws={'size': 8}, ax=ax2)\n",
    "        ax2.set_title(f'Prescriptions per 1000 Population by State and ATC2 Class\\n(Top 20 States vs Top 10 Classes)', \n",
    "                      fontweight='bold', pad=20)\n",
    "        ax2.set_xlabel('ATC2 Classes', fontweight='bold')\n",
    "        ax2.set_ylabel('States', fontweight='bold')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        ax2.tick_params(axis='y', rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Also create separate individual heatmaps for better detail viewing\n",
    "        print(\"\\nCreating detailed individual heatmaps...\")\n",
    "        \n",
    "        # Individual Units per 1000 heatmap (RESCALED)\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        sns.heatmap(heatmap_df_units, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "                    cbar_kws={'label': 'Units per 1000 Population (Millions)'},  # Updated label\n",
    "                    annot_kws={'size': 9})\n",
    "        plt.title(f'Units per 1000 Population by State and ATC2 Class\\n(Top 20 States vs Top 10 Classes - Units in Millions)', \n",
    "                  fontweight='bold', pad=20)  # Updated title\n",
    "        plt.xlabel('ATC2 Classes', fontweight='bold')\n",
    "        plt.ylabel('States', fontweight='bold')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Individual Prescriptions per 1000 heatmap (NOT RESCALED)\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        sns.heatmap(heatmap_df_rx, annot=True, fmt='.1f', cmap='Blues', \n",
    "                    cbar_kws={'label': 'Prescriptions per 1000 Population'},\n",
    "                    annot_kws={'size': 9})\n",
    "        plt.title(f'Prescriptions per 1000 Population by State and ATC2 Class\\n(Top 20 States vs Top 10 Classes)', \n",
    "                  fontweight='bold', pad=20)\n",
    "        plt.xlabel('ATC2 Classes', fontweight='bold')\n",
    "        plt.ylabel('States', fontweight='bold')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed summary\n",
    "        print(f\"\\n{'='*80}\"); print(\"DETAILED ATC2 CLASS SUMMARY (POPULATION-NORMALIZED)\"); print(f\"{'='*80}\")\n",
    "        \n",
    "        for i, atc_class in enumerate(top_classes, 1):\n",
    "            if atc_class in class_results:\n",
    "                total_units_per_1k = class_results[atc_class]['total_units_per_1000']\n",
    "                total_rx_per_1k = class_results[atc_class]['total_rx_per_1000']\n",
    "                print(f\"{i:2d}. {atc_class}\")\n",
    "                print(f\"    Total: {total_units_per_1k:.1f} units/1K population, {total_rx_per_1k:.1f} rx/1K population\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\n{'='*80}\"); print(\"SUMMARY STATISTICS (POPULATION-NORMALIZED)\"); print(f\"{'='*80}\")\n",
    "        \n",
    "        print(f\"  Total ATC2 classes analyzed: {len(class_results)}\")\n",
    "        print(f\"  Total states in data: {combined_df['State'].nunique()}\")\n",
    "        print(f\"  Total records: {len(combined_df):,}\")\n",
    "        print(f\"  Total units per 1K population: {combined_df['units_per_1000'].sum():.1f}\")\n",
    "        print(f\"  Total prescriptions per 1K population: {combined_df['NoP_per_1000'].sum():.1f}\")\n",
    "        \n",
    "        # State concentration analysis\n",
    "        print(f\"\\nState Concentration Analysis (Population-Normalized):\")\n",
    "        \n",
    "        # Calculate concentration for different numbers of top states\n",
    "        for top_n in [5, 10, 20]:\n",
    "            top_n_states_pct_units = (state_totals_units.head(top_n).sum() / state_totals_units.sum() * 100)\n",
    "            top_n_states_pct_rx = (state_totals_rx.head(top_n).sum() / state_totals_rx.sum() * 100)\n",
    "            \n",
    "            print(f\"  Top {top_n} states account for {top_n_states_pct_units:.1f}% of total units/1K, {top_n_states_pct_rx:.1f}% of rx/1K\")\n",
    "        \n",
    "        top_20_states_units = state_totals_units.head(20)\n",
    "        top_20_states_rx = state_totals_rx.head(20)\n",
    "        \n",
    "        print(f\"\\n  Top 20 states by units/1K: {', '.join(top_20_states_units.index)}\")\n",
    "        print(f\"  Top 20 states by rx/1K: {', '.join(top_20_states_rx.index)}\")\n",
    "        \n",
    "        return {\n",
    "            'class_results': class_results,\n",
    "            'national_summary': national_summary,\n",
    "            'top_classes': top_classes,\n",
    "            'state_totals_units_per_1000': state_totals_units,\n",
    "            'state_totals_rx_per_1000': state_totals_rx,\n",
    "            'heatmap_data_units_per_1000': heatmap_df_units,  # This contains rescaled data\n",
    "            'heatmap_data_rx_per_1000': heatmap_df_rx,\n",
    "            'combined_data': combined_df\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b89dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_sdud = Pop_SDUD()\n",
    "\n",
    "#merged_files = pop_sdud.merge_enroll()\n",
    "#subpanel_df = pop_sdud.build_atc2_subpanel()\n",
    "\n",
    "#This is only fo\n",
    "pop_normalized_results = pop_sdud.analyze_top_states_by_pop_normalized_metrics(top_n_states=20, top_n_classes=15)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
