{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2b3e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import shelve\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f7f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "user=\"lholguin\"\n",
    "#user in personal pc1 <- \"asus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aebaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDCATCProcessor:\n",
    "\n",
    "    def __init__(self, year, base_path=None):\n",
    "        self.year = year\n",
    "        self.base_path = base_path or rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        self.df_cleaned = None\n",
    "        self.df_merged = None\n",
    "        self.atc_mapping = None\n",
    "        \n",
    "    def clean_sdud_data(self):\n",
    "\n",
    "        csv_file = os.path.join(self.base_path, f\"SDUD\\\\SDUD{self.year}.csv\")\n",
    "        print(f\"Reading CSV: {csv_file}\")\n",
    "        \n",
    "        df = pd.read_csv(csv_file, dtype={'NDC': 'object'})\n",
    "        print(f\"Initial rows: {len(df):,}\")\n",
    "        \n",
    "        # Filter data\n",
    "        df_filtered = df.dropna(subset=['Units Reimbursed', 'Number of Prescriptions'])\n",
    "        df_filtered = df_filtered[df_filtered['State'] != 'XX']\n",
    "        \n",
    "        print(f\"After cleaning: {len(df_filtered):,} rows, {df_filtered['NDC'].nunique():,} unique NDCs\")\n",
    "        \n",
    "        self.df_cleaned = df_filtered\n",
    "        return self.df_cleaned\n",
    "    \n",
    "    def adding_key(self):\n",
    "        if self.df_cleaned is None:\n",
    "            raise ValueError(\"Run clean_sdud_data() first\")\n",
    "        \n",
    "        self.df_cleaned['record_id'] = (\n",
    "            self.df_cleaned['State'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['Year'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['Quarter'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['Utilization Type'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['NDC'].astype(str)\n",
    "        )\n",
    "        \n",
    "        print(f\"Created {len(self.df_cleaned):,} record IDs\")\n",
    "        return self.df_cleaned\n",
    "    \n",
    "    def generate_ndc_txt(self, output_filename=None):\n",
    "        if 'record_id' not in self.df_cleaned.columns:\n",
    "            raise ValueError(\"Run adding_key() first\")\n",
    "            \n",
    "        output_filename = output_filename or f\"NDCNEW_{self.year}.txt\"\n",
    "        output_path = os.path.join(self.base_path, f\"ATC\\\\text_files\\\\{output_filename}\")\n",
    "        \n",
    "        unique_pairs = self.df_cleaned[['NDC', 'record_id']].drop_duplicates()\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(\"NDC\\trecord_id\\n\")\n",
    "            for _, row in unique_pairs.iterrows():\n",
    "                f.write(f\"{row['NDC']}\\t{row['record_id']}\\n\")\n",
    "        \n",
    "        print(f\"Exported {unique_pairs['record_id'].nunique():,} unique records to {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def analyze_atc4_mapping(self):\n",
    "\n",
    "        if 'record_id' not in self.df_cleaned.columns:\n",
    "            raise ValueError(\"Run adding_key() first\")\n",
    "            \n",
    "        atc4_path = os.path.join(self.base_path, f\"ATC\\\\ATC4_classes\\\\NDCNEW_{self.year}_ATC4_classes.csv\")\n",
    "        \n",
    "        # Load ATC4 mapping\n",
    "        df_atc4 = pd.read_csv(atc4_path, dtype={'NDC': 'object', 'record_id': 'string'})\n",
    "        df_atc4['NDC'] = df_atc4['NDC'].str.zfill(11)\n",
    "        \n",
    "        print(f\"ATC4 file: {len(df_atc4):,} rows, {df_atc4['NDC'].nunique():,} unique NDCs\")\n",
    "        \n",
    "        # Ensure consistent types\n",
    "        self.df_cleaned['record_id'] = self.df_cleaned['record_id'].astype('string')\n",
    "        self.df_cleaned['NDC'] = self.df_cleaned['NDC'].astype('object')\n",
    "        \n",
    "        # Merge on both record_id and NDC\n",
    "        self.atc_mapping = pd.merge(\n",
    "            self.df_cleaned,\n",
    "            df_atc4[['record_id', 'NDC', 'ATC4 Class']],\n",
    "            on=['record_id', 'NDC'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        total = len(self.atc_mapping)\n",
    "        mapped = self.atc_mapping['ATC4 Class'].notna().sum()\n",
    "        print(f\"Merged: {total:,} records, {mapped:,} with ATC4 ({mapped/total*100:.1f}%)\")\n",
    "        \n",
    "        missing = total - mapped\n",
    "        if missing > 0:\n",
    "            print(f\"Missing: {missing:,} records, {self.atc_mapping[self.atc_mapping['ATC4 Class'].isna()]['NDC'].nunique():,} unique NDCs\")\n",
    "        \n",
    "        return self.atc_mapping\n",
    "    \n",
    "    def analyze_atc_distribution(self, level='ATC3'):\n",
    "\n",
    "        if self.atc_mapping is None:\n",
    "            raise ValueError(\"Run analyze_atc4_mapping() first\")\n",
    "        \n",
    "        records = self.atc_mapping[self.atc_mapping['ATC4 Class'].notna()].copy()\n",
    "        \n",
    "        if len(records) == 0:\n",
    "            print(\"No records with valid ATC4 mappings.\")\n",
    "            return None\n",
    "        \n",
    "        # Create ATC level column if needed\n",
    "        if level == 'ATC3':\n",
    "            records['ATC3 Class'] = records['ATC4 Class'].str[:4]\n",
    "            class_col = 'ATC3 Class'\n",
    "        elif level == 'ATC2':\n",
    "            records['ATC2 Class'] = records['ATC4 Class'].str[:3]\n",
    "            class_col = 'ATC2 Class'\n",
    "        else:\n",
    "            class_col = 'ATC4 Class'\n",
    "        \n",
    "        # Count classes per record_id\n",
    "        per_record = records.groupby('record_id')[class_col].nunique().reset_index()\n",
    "        per_record.columns = ['record_id', 'num_classes']\n",
    "        \n",
    "        distribution = per_record['num_classes'].value_counts().sort_index()\n",
    "        \n",
    "        print(f\"\\n{level} CLASSES PER RECORD_ID:\")\n",
    "        for n_classes, count in distribution.items():\n",
    "            pct = (count / len(per_record)) * 100\n",
    "            print(f\"  {n_classes} class(es): {count:,} records ({pct:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"  Avg {level} per record: {per_record['num_classes'].mean():.2f}\")\n",
    "        print(f\"  Max {level} per record: {per_record['num_classes'].max()}\")\n",
    "        \n",
    "        return per_record\n",
    "\n",
    "    def fetch_atc_names(self, cache_path=None):\n",
    "        \"\"\"Fetch ATC class names (ATC4, ATC3, ATC2) from RxNav API.\"\"\"\n",
    "        if self.atc_mapping is None:\n",
    "            raise ValueError(\"Must run analyze_atc4_mapping() first\")\n",
    "        \n",
    "        if cache_path is None:\n",
    "            cache_path = os.path.join(self.base_path, \"ATC\\\\cache_files\\\\atc_names_cache\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"FETCHING ATC CLASS NAMES\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Using cache: {cache_path}\")\n",
    "        \n",
    "        # Get only records with valid ATC4 mappings\n",
    "        df_with_atc = self.atc_mapping[self.atc_mapping['ATC4 Class'].notna()].copy()\n",
    "        \n",
    "        # Create ATC3 and ATC2 columns from ATC4\n",
    "        print(\"\\nCreating ATC3 and ATC2 columns from ATC4...\")\n",
    "        df_with_atc['ATC3 Class'] = df_with_atc['ATC4 Class'].str[:4]\n",
    "        df_with_atc['ATC2 Class'] = df_with_atc['ATC4 Class'].str[:3]\n",
    "        \n",
    "        # Get unique codes for each level\n",
    "        unique_atc4 = df_with_atc['ATC4 Class'].dropna().unique()\n",
    "        unique_atc3 = df_with_atc['ATC3 Class'].dropna().unique()\n",
    "        unique_atc2 = df_with_atc['ATC2 Class'].dropna().unique()\n",
    "        \n",
    "        # Filter out invalid codes\n",
    "        unique_atc4 = [c for c in unique_atc4 if c not in ['No ATC Mapping Found', 'No RxCUI Found', '']]\n",
    "        unique_atc3 = [c for c in unique_atc3 if c not in ['No ATC Mapping Found', 'No RxCUI Found', '', 'No ', 'No']]\n",
    "        unique_atc2 = [c for c in unique_atc2 if c not in ['No ATC Mapping Found', 'No RxCUI Found', '', 'No ', 'No']]\n",
    "        \n",
    "        print(f\"\\nUnique codes to fetch:\")\n",
    "        print(f\"  ATC4: {len(unique_atc4)}\")\n",
    "        print(f\"  ATC3: {len(unique_atc3)}\")\n",
    "        print(f\"  ATC2: {len(unique_atc2)}\")\n",
    "        \n",
    "        # Build mappings\n",
    "        atc4_names = {}\n",
    "        atc3_names = {}\n",
    "        atc2_names = {}\n",
    "        \n",
    "        with shelve.open(cache_path) as cache:\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            print(\"\\nFetching ATC4 names...\")\n",
    "            for code in unique_atc4:\n",
    "                atc4_names[code] = self._get_atc_name(code, cache)\n",
    "            \n",
    "            print(\"Fetching ATC3 names...\")\n",
    "            for code in unique_atc3:\n",
    "                atc3_names[code] = self._get_atc_name(code, cache)\n",
    "            \n",
    "            print(\"Fetching ATC2 names...\")\n",
    "            for code in unique_atc2:\n",
    "                atc2_names[code] = self._get_atc_name(code, cache)\n",
    "            \n",
    "            print(f\"\\nTotal processing time: {(datetime.now() - start_time).total_seconds()/60:.1f} minutes\")\n",
    "        \n",
    "        # Apply names to all records in atc_mapping\n",
    "        print(\"\\nApplying names to dataframe...\")\n",
    "        self.atc_mapping['ATC3 Class'] = self.atc_mapping['ATC4 Class'].str[:4]\n",
    "        self.atc_mapping['ATC2 Class'] = self.atc_mapping['ATC4 Class'].str[:3]\n",
    "        \n",
    "        self.atc_mapping['ATC4_Name'] = self.atc_mapping['ATC4 Class'].map(atc4_names).fillna('')\n",
    "        self.atc_mapping['ATC3_Name'] = self.atc_mapping['ATC3 Class'].map(atc3_names).fillna('')\n",
    "        self.atc_mapping['ATC2_Name'] = self.atc_mapping['ATC2 Class'].map(atc2_names).fillna('')\n",
    "        \n",
    "        print(f\"\\nATC names added successfully!\")\n",
    "        print(\"\\nSample output:\")\n",
    "        sample = self.atc_mapping[self.atc_mapping['ATC4 Class'].notna()][['NDC', 'record_id', 'ATC4 Class', 'ATC4_Name', 'ATC3 Class', 'ATC3_Name', 'ATC2 Class', 'ATC2_Name']].head(5)\n",
    "        print(sample.to_string())\n",
    "        \n",
    "        return self.atc_mapping\n",
    "    \n",
    "    def prepare_final_dataframe(self):\n",
    "\n",
    "        if self.atc_mapping is None:\n",
    "            raise ValueError(\"Run fetch_atc_names() first\")\n",
    "        \n",
    "        self.df_merged = self.atc_mapping.copy()\n",
    "        \n",
    "        # Scale units\n",
    "        self.df_merged['Units Reimbursed'] = self.df_merged['Units Reimbursed'] / 1e9\n",
    "        self.df_merged['Number of Prescriptions'] = self.df_merged['Number of Prescriptions'] / 1e6\n",
    "        \n",
    "        total = len(self.df_merged)\n",
    "        mapped = self.df_merged['ATC4 Class'].notna().sum()\n",
    "        \n",
    "        print(f\"\\nFinal Statistics:\")\n",
    "        print(f\"  Records: {total:,} ({mapped:,} with ATC4, {mapped/total*100:.1f}%)\")\n",
    "        print(f\"  Units Reimbursed: {self.df_merged['Units Reimbursed'].sum():.2f} Billion\")\n",
    "        print(f\"  Prescriptions: {self.df_merged['Number of Prescriptions'].sum():.2f} Million\")\n",
    "        \n",
    "        return self.df_merged\n",
    "    \n",
    "    def _get_atc_name(self, atc_code, cache):\n",
    "        \"\"\"Helper: Fetch ATC name from RxNav API with caching.\"\"\"\n",
    "        cache_key = f\"atc_name:{atc_code}\"\n",
    "        if cache_key in cache:\n",
    "            return cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            url = f\"https://rxnav.nlm.nih.gov/REST/rxclass/class/byId.json?classId={atc_code}\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'rxclassMinConceptList' in data and 'rxclassMinConcept' in data['rxclassMinConceptList']:\n",
    "                concepts = data['rxclassMinConceptList']['rxclassMinConcept']\n",
    "                if concepts:\n",
    "                    name = concepts[0].get('className', '')\n",
    "                    cache[cache_key] = name\n",
    "                    return name\n",
    "            \n",
    "            cache[cache_key] = ''\n",
    "            return ''\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving {atc_code}: {e}\")\n",
    "            cache[cache_key] = ''\n",
    "            return ''\n",
    "\n",
    "    def export_merged_data(self, output_filename=None, show_details=True):\n",
    "\n",
    "        if self.df_merged is None:\n",
    "            raise ValueError(\"Run prepare_final_dataframe() first\")\n",
    "            \n",
    "        output_filename = output_filename or f\"merged_NEWdata_{self.year}.csv\"\n",
    "        output_path = os.path.join(self.base_path, f\"ATC\\\\merged_data\\\\{output_filename}\")\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Check duplicates\n",
    "        initial_count = len(self.df_merged)\n",
    "        duplicate_count = self.df_merged['record_id'].duplicated().sum()\n",
    "        \n",
    "        print(f\"\\nDeduplication Check:\")\n",
    "        print(f\"  Before: {initial_count:,} rows\")\n",
    "        print(f\"  Duplicates: {duplicate_count:,}\")\n",
    "        \n",
    "        # Show sample duplicates if requested\n",
    "        if show_details and duplicate_count > 0:\n",
    "            dup_records = self.df_merged[self.df_merged['record_id'].duplicated(keep=False)].sort_values('record_id')\n",
    "            sample_ids = dup_records['record_id'].unique()[:2]\n",
    "            \n",
    "            print(f\"\\nSample duplicate record_ids:\")\n",
    "            for rid in sample_ids:\n",
    "                sample = self.df_merged[self.df_merged['record_id'] == rid][\n",
    "                    ['record_id', 'NDC', 'State', 'ATC4 Class', 'ATC2 Class']\n",
    "                ]\n",
    "                print(f\"\\n{rid}:\")\n",
    "                print(sample.to_string(index=False))\n",
    "        \n",
    "        # Deduplicate and export\n",
    "        df_final = self.df_merged.drop_duplicates(subset='record_id', keep='first')\n",
    "        df_final.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\n  After: {len(df_final):,} rows\")\n",
    "        print(f\"  Removed: {initial_count - len(df_final):,}\")\n",
    "        print(f\"\\nExported to: {output_path}\")\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        agg = df_final.groupby('record_id').agg({\n",
    "            'Units Reimbursed': 'sum',\n",
    "            'Number of Prescriptions': 'sum'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nAggregated Totals:\")\n",
    "        print(f\"  Units Reimbursed: {agg['Units Reimbursed'].sum():.3f} Billion\")\n",
    "        print(f\"  Number of Prescriptions: {agg['Number of Prescriptions'].sum():3f} Million\")\n",
    "        \n",
    "        return output_path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c736ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDCATC_overview:\n",
    "\n",
    "    @staticmethod\n",
    "    def create_multi_year_distribution_analysis(years_list, base_path=None):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "            \n",
    "        print(\"Creating Multi-Year ATC Distribution Analysis...\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        results = {\n",
    "            'ATC4_1_class': {}, 'ATC4_2_classes': {}, 'ATC4_3+_classes': {},\n",
    "            'ATC3_1_class': {}, 'ATC3_2_classes': {}, 'ATC3_3+_classes': {},\n",
    "            'ATC2_1_class': {}, 'ATC2_2_classes': {}, 'ATC2_3+_classes': {}\n",
    "        }\n",
    "        \n",
    "        for year in years_list:\n",
    "            print(f\"Processing {year}...\", end=\" \")\n",
    "            try:\n",
    "                # Load the pre-processed CSV file\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                \n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                if records.empty:\n",
    "                    print(\"No ATC records\")\n",
    "                    for key in results.keys():\n",
    "                        results[key][year] = \"N/A\"\n",
    "                    continue\n",
    "                    \n",
    "                records['ATC2 Class'] = records['ATC4 Class'].str[:3]\n",
    "                records['ATC3 Class'] = records['ATC4 Class'].str[:4]\n",
    "                \n",
    "                # Calculate distributions for each level\n",
    "                for level, col in [('ATC4', 'ATC4 Class'), ('ATC3', 'ATC3 Class'), ('ATC2', 'ATC2 Class')]:\n",
    "                    per_record = records.groupby('record_id')[col].nunique()\n",
    "                    dist = per_record.value_counts().sort_index()\n",
    "                    total = len(per_record)\n",
    "                    \n",
    "                    results[f'{level}_1_class'][year] = f\"{(dist.get(1, 0) / total * 100):.1f}%\"\n",
    "                    results[f'{level}_2_classes'][year] = f\"{(dist.get(2, 0) / total * 100):.1f}%\"\n",
    "                    results[f'{level}_3+_classes'][year] = f\"{(dist[dist.index >= 3].sum() / total * 100):.1f}%\"\n",
    "                \n",
    "                print(\"✓\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"✗ File not found: {csv_path}\")\n",
    "                for key in results.keys():\n",
    "                    results[key][year] = \"N/A\"\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "                for key in results.keys():\n",
    "                    results[key][year] = \"N/A\"\n",
    "        \n",
    "        df_percentages = pd.DataFrame(results).T\n",
    "        print(f\"\\nATC DISTRIBUTION PERCENTAGES ACROSS YEARS\")\n",
    "        print(\"=\"*60)\n",
    "        print(df_percentages)\n",
    "        return df_percentages\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_general_atc_overview(years_list, base_path=None):\n",
    "  \n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "\n",
    "        print(\"Creating ATC2 & ATC3 Overview: Unique NDCs per Class Across Years...\")\n",
    "        print(\"=\"*78)\n",
    "        \n",
    "        atc2_year_results = {}\n",
    "        atc3_year_results = {}\n",
    "        \n",
    "        for year in years_list:\n",
    "            print(f\"Processing {year}...\", end=\" \")\n",
    "            try:\n",
    "                # Load the pre-processed CSV file\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                \n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                if records.empty:\n",
    "                    print(\"No records with ATC mapping\")\n",
    "                    atc2_year_results[year] = pd.DataFrame()\n",
    "                    atc3_year_results[year] = pd.DataFrame()\n",
    "                    continue\n",
    "        \n",
    "                # ATC2 summary\n",
    "                pairs2 = records[['record_id', 'NDC', 'ATC2 Class']].drop_duplicates()\n",
    "                atc2_summary = pairs2.groupby('ATC2 Class').agg(\n",
    "                    Unique_NDCs=('NDC', 'nunique'),\n",
    "                    Total_Records=('record_id', 'nunique')\n",
    "                ).sort_values('Unique_NDCs', ascending=False)\n",
    "                atc2_summary['Percentage_of_NDCs'] = (\n",
    "                    atc2_summary['Unique_NDCs'] / pairs2['NDC'].nunique() * 100\n",
    "                ).round(1)\n",
    "                \n",
    "                # ATC3 summary\n",
    "                pairs3 = records[['record_id', 'NDC', 'ATC3 Class']].drop_duplicates()\n",
    "                atc3_summary = pairs3.groupby('ATC3 Class').agg(\n",
    "                    Unique_NDCs=('NDC', 'nunique'),\n",
    "                    Total_Records=('record_id', 'nunique')\n",
    "                ).sort_values('Unique_NDCs', ascending=False)\n",
    "                atc3_summary['Percentage_of_NDCs'] = (\n",
    "                    atc3_summary['Unique_NDCs'] / pairs3['NDC'].nunique() * 100\n",
    "                ).round(1)\n",
    "                \n",
    "                atc2_year_results[year] = atc2_summary\n",
    "                atc3_year_results[year] = atc3_summary\n",
    "                \n",
    "                print(f\"✓ (ATC2: {len(atc2_summary)} classes, {pairs2['NDC'].nunique():,} NDCs; \"\n",
    "                      f\"ATC3: {len(atc3_summary)} classes, {pairs3['NDC'].nunique():,} NDCs)\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"✗ File not found: {csv_path}\")\n",
    "                atc2_year_results[year] = pd.DataFrame()\n",
    "                atc3_year_results[year] = pd.DataFrame()\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "                atc2_year_results[year] = pd.DataFrame()\n",
    "                atc3_year_results[year] = pd.DataFrame()\n",
    "        \n",
    "        # Print summaries\n",
    "        print(\"\\nUNIQUE NDCs PER ATC2 CLASS BY YEAR\")\n",
    "        print(\"=\"*60)\n",
    "        for year in years_list:\n",
    "            if not atc2_year_results[year].empty:\n",
    "                print(f\"\\n{year}: {len(atc2_year_results[year])} classes, \"\n",
    "                      f\"{atc2_year_results[year]['Unique_NDCs'].sum():,} total NDCs\")\n",
    "                print(\"Top 10:\")\n",
    "                print(atc2_year_results[year].head(10))\n",
    "        \n",
    "        print(\"\\nUNIQUE NDCs PER ATC3 CLASS BY YEAR\")\n",
    "        print(\"=\"*60)\n",
    "        for year in years_list:\n",
    "            if not atc3_year_results[year].empty:\n",
    "                print(f\"\\n{year}: {len(atc3_year_results[year])} classes, \"\n",
    "                      f\"{atc3_year_results[year]['Unique_NDCs'].sum():,} total NDCs\")\n",
    "                print(\"Top 10:\")\n",
    "                print(atc3_year_results[year].head(10))\n",
    "        \n",
    "        # Build comparison tables\n",
    "        def build_comparison(year_tables):\n",
    "            all_classes = set()\n",
    "            for tbl in year_tables.values():\n",
    "                if not tbl.empty:\n",
    "                    all_classes.update(tbl.index.tolist())\n",
    "            comp = {cls: {y: int(year_tables[y].loc[cls, 'Unique_NDCs']) \n",
    "                          if not year_tables[y].empty and cls in year_tables[y].index else 0\n",
    "                          for y in years_list}\n",
    "                    for cls in sorted(all_classes)}\n",
    "            df = pd.DataFrame(comp).T\n",
    "            return df.loc[df.sum(axis=1).sort_values(ascending=False).index]\n",
    "        \n",
    "        atc2_comparison = build_comparison(atc2_year_results)\n",
    "        atc3_comparison = build_comparison(atc3_year_results)\n",
    "        \n",
    "        # Create cumulative frequency tables\n",
    "        def create_cumulative_frequency_table(comparison_df, level_name):\n",
    "            \"\"\"Create cumulative frequency table showing classes with most unique NDCs.\"\"\"\n",
    "            # Calculate total NDCs across all years for each class\n",
    "            total_ndcs = comparison_df.sum(axis=1).sort_values(ascending=False)\n",
    "            \n",
    "            # Create frequency table\n",
    "            freq_table = pd.DataFrame({\n",
    "                'ATC_Class': total_ndcs.index,\n",
    "                'Total_Unique_NDCs': total_ndcs.values,\n",
    "                'Percentage': (total_ndcs.values / total_ndcs.sum() * 100).round(2)\n",
    "            })\n",
    "            \n",
    "            # Add cumulative frequency and percentage\n",
    "            freq_table['Cumulative_NDCs'] = freq_table['Total_Unique_NDCs'].cumsum()\n",
    "            freq_table['Cumulative_Percentage'] = freq_table['Percentage'].cumsum().round(2)\n",
    "            \n",
    "            # Reset index\n",
    "            freq_table.reset_index(drop=True, inplace=True)\n",
    "            freq_table.index = freq_table.index + 1  # Start ranking from 1\n",
    "            \n",
    "            return freq_table\n",
    "        \n",
    "        # Generate cumulative frequency tables\n",
    "        atc2_freq_table = create_cumulative_frequency_table(atc2_comparison, 'ATC2')\n",
    "        atc3_freq_table = create_cumulative_frequency_table(atc3_comparison, 'ATC3')\n",
    "\n",
    "        \n",
    "        return atc2_year_results, atc3_year_results, atc2_comparison, atc3_comparison, atc2_freq_table, atc3_freq_table\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_atc_ndc_details(year, top_n=10, base_path=None):\n",
    "    \n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "            \n",
    "        print(f\"Analyzing ATC-NDC details for {year}...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            # Load the pre-processed CSV file\n",
    "            csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "            df_merged = pd.read_csv(csv_path)\n",
    "            \n",
    "            records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "            if records.empty:\n",
    "                print(\"No records with ATC mapping\")\n",
    "                return pd.DataFrame(), pd.DataFrame()\n",
    "                \n",
    "            records['ATC2 Class'] = records['ATC4 Class'].str[:3]\n",
    "            records['ATC3 Class'] = records['ATC4 Class'].str[:4]\n",
    "            \n",
    "            # ATC2 details\n",
    "            atc2_details = records.groupby('ATC2 Class').agg(\n",
    "                Unique_NDCs=('NDC', 'nunique'),\n",
    "                Total_Records=('record_id', 'nunique')\n",
    "            ).sort_values('Unique_NDCs', ascending=False).head(top_n)\n",
    "            \n",
    "            # ATC3 details\n",
    "            atc3_details = records.groupby('ATC3 Class').agg(\n",
    "                Unique_NDCs=('NDC', 'nunique'),\n",
    "                Total_Records=('record_id', 'nunique')\n",
    "            ).sort_values('Unique_NDCs', ascending=False).head(top_n)\n",
    "            \n",
    "            print(f\"\\nTop {top_n} ATC2 Classes:\")\n",
    "            print(atc2_details)\n",
    "            print(f\"\\nTop {top_n} ATC3 Classes:\")\n",
    "            print(atc3_details)\n",
    "            \n",
    "            return atc2_details, atc3_details\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"✗ File not found: {csv_path}\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error: {e}\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    @staticmethod\n",
    "    def export_cumulative_frequency_excel(years_list, level='ATC2', base_path=None, output_filename=None, include_ndc_counts=True, by_state=False, state_filter=None):\n",
    "        \n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        # Handle state filtering messaging\n",
    "        if by_state:\n",
    "            if state_filter:\n",
    "                if isinstance(state_filter, str):\n",
    "                    state_filter = [state_filter]\n",
    "                print(f\"Creating {level} Cumulative Frequency Analysis Excel for States: {', '.join(state_filter)}\")\n",
    "            else:\n",
    "                print(f\"Creating {level} Cumulative Frequency Analysis Excel by State (All States)\")\n",
    "        else:\n",
    "            print(f\"Creating {level} Cumulative Frequency Analysis Excel...\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if by_state:\n",
    "            # State-based analysis\n",
    "            state_year_results = {}\n",
    "            state_ndc_counts = {}\n",
    "            all_states = set()\n",
    "        else:\n",
    "            # Original total analysis\n",
    "            year_results = {}\n",
    "            ndc_counts = {}  # Store NDC counts for each ATC class\n",
    "        \n",
    "        # Step 1: Process each year and collect both financial and NDC count data\n",
    "        for year in years_list:\n",
    "            print(f\"Processing {year}...\", end=\" \")\n",
    "            try:\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                \n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                if records.empty:\n",
    "                    print(\"No ATC records\")\n",
    "                    if by_state:\n",
    "                        # Initialize empty for this year\n",
    "                        pass\n",
    "                    else:\n",
    "                        year_results[year] = pd.DataFrame()\n",
    "                        ndc_counts[year] = pd.DataFrame()\n",
    "                    continue\n",
    "                \n",
    "                # Filter by state if specified\n",
    "                if by_state and state_filter:\n",
    "                    records = records[records['State'].isin(state_filter)]\n",
    "                    if records.empty:\n",
    "                        print(f\"No records for states {state_filter}\")\n",
    "                        continue\n",
    "                \n",
    "                # Create appropriate ATC level column\n",
    "                if level == 'ATC2':\n",
    "                    records['ATC_Level'] = records['ATC4 Class'].str[:3]\n",
    "                elif level == 'ATC3':\n",
    "                    records['ATC_Level'] = records['ATC4 Class'].str[:4]\n",
    "                else:  # ATC4\n",
    "                    records['ATC_Level'] = records['ATC4 Class']\n",
    "                \n",
    "                if by_state:\n",
    "                    # Group by state first\n",
    "                    if state_filter:\n",
    "                        states_to_process = state_filter\n",
    "                    else:\n",
    "                        states_to_process = records['State'].unique()\n",
    "                        all_states.update(states_to_process)\n",
    "                    \n",
    "                    for state in states_to_process:\n",
    "                        state_records = records[records['State'] == state]\n",
    "                        if state_records.empty:\n",
    "                            continue\n",
    "                        \n",
    "                        # Initialize state dictionaries if needed\n",
    "                        if state not in state_year_results:\n",
    "                            state_year_results[state] = {}\n",
    "                            state_ndc_counts[state] = {}\n",
    "                        \n",
    "                        # Aggregate financial data by ATC level for this state\n",
    "                        financial_summary = state_records.groupby('ATC_Level').agg(\n",
    "                            Units_Reimbursed=('Units Reimbursed', 'sum'),\n",
    "                            Number_of_Prescriptions=('Number of Prescriptions', 'sum')\n",
    "                        )\n",
    "                        \n",
    "                        # Count unique NDCs by ATC level for this state\n",
    "                        ndc_count_summary = state_records.groupby('ATC_Level').agg(\n",
    "                            Unique_NDCs=('NDC', 'nunique')\n",
    "                        )\n",
    "                        \n",
    "                        state_year_results[state][year] = financial_summary\n",
    "                        state_ndc_counts[state][year] = ndc_count_summary\n",
    "                    \n",
    "                    print(f\"✓ (States: {len(states_to_process)})\")\n",
    "                else:\n",
    "                    # Original total analysis\n",
    "                    financial_summary = records.groupby('ATC_Level').agg(\n",
    "                        Units_Reimbursed=('Units Reimbursed', 'sum'),\n",
    "                        Number_of_Prescriptions=('Number of Prescriptions', 'sum')\n",
    "                    )\n",
    "                    \n",
    "                    # Count unique NDCs by ATC level\n",
    "                    ndc_count_summary = records.groupby('ATC_Level').agg(\n",
    "                        Unique_NDCs=('NDC', 'nunique')\n",
    "                    )\n",
    "                    \n",
    "                    year_results[year] = financial_summary\n",
    "                    ndc_counts[year] = ndc_count_summary\n",
    "                    \n",
    "                    print(f\"✓ ({len(financial_summary)} classes)\")\n",
    "                    \n",
    "            except FileNotFoundError:\n",
    "                print(f\"✗ File not found\")\n",
    "                if not by_state:\n",
    "                    year_results[year] = pd.DataFrame()\n",
    "                    ndc_counts[year] = pd.DataFrame()\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "                if not by_state:\n",
    "                    year_results[year] = pd.DataFrame()\n",
    "                    ndc_counts[year] = pd.DataFrame()\n",
    "        \n",
    "        if by_state:\n",
    "            # Process each state and create separate Excel files or sheets\n",
    "            states_to_export = state_filter if state_filter else sorted(all_states)\n",
    "            all_output_paths = {}\n",
    "            \n",
    "            for state in states_to_export:\n",
    "                if state not in state_year_results:\n",
    "                    continue\n",
    "                \n",
    "                # Use the state's data as if it were the total data\n",
    "                year_results = state_year_results[state]\n",
    "                ndc_counts = state_ndc_counts[state]\n",
    "                \n",
    "                # Step 2: Build comparison tables for this state\n",
    "                all_classes = set()\n",
    "                for tbl in year_results.values():\n",
    "                    if not tbl.empty:\n",
    "                        all_classes.update(tbl.index.tolist())\n",
    "                \n",
    "                # Also get classes from NDC counts\n",
    "                for tbl in ndc_counts.values():\n",
    "                    if not tbl.empty:\n",
    "                        all_classes.update(tbl.index.tolist())\n",
    "                \n",
    "                if not all_classes:\n",
    "                    print(f\"No data found for state {state}\")\n",
    "                    continue\n",
    "                \n",
    "                # Build comparison tables for Units and Prescriptions\n",
    "                units_comparison = {cls: {y: float(year_results[y].loc[cls, 'Units_Reimbursed']) \n",
    "                                        if y in year_results and not year_results[y].empty and cls in year_results[y].index else 0.0\n",
    "                                        for y in years_list}\n",
    "                                for cls in sorted(all_classes)}\n",
    "                \n",
    "                prescriptions_comparison = {cls: {y: float(year_results[y].loc[cls, 'Number_of_Prescriptions']) \n",
    "                                                if y in year_results and not year_results[y].empty and cls in year_results[y].index else 0.0\n",
    "                                                for y in years_list}\n",
    "                                        for cls in sorted(all_classes)}\n",
    "                \n",
    "                # Build NDC counts comparison table\n",
    "                ndc_comparison = {cls: {y: int(ndc_counts[y].loc[cls, 'Unique_NDCs']) \n",
    "                                    if y in ndc_counts and not ndc_counts[y].empty and cls in ndc_counts[y].index else 0\n",
    "                                    for y in years_list}\n",
    "                                for cls in sorted(all_classes)}\n",
    "                \n",
    "                units_comparison_df = pd.DataFrame(units_comparison).T\n",
    "                prescriptions_comparison_df = pd.DataFrame(prescriptions_comparison).T\n",
    "                ndc_comparison_df = pd.DataFrame(ndc_comparison).T\n",
    "                \n",
    "                # Sort by total units (keep same order across all sheets)\n",
    "                units_total = units_comparison_df.sum(axis=1).sort_values(ascending=False)\n",
    "                units_comparison_df = units_comparison_df.loc[units_total.index]\n",
    "                prescriptions_comparison_df = prescriptions_comparison_df.loc[units_total.index]\n",
    "                ndc_comparison_df = ndc_comparison_df.loc[units_total.index]\n",
    "                \n",
    "                # Step 3: Create cumulative frequency DataFrames\n",
    "                def create_cumulative_df(comparison_df, metric_name):\n",
    "                    totals = comparison_df.sum(axis=1)\n",
    "                    total_sum = totals.sum()\n",
    "                    \n",
    "                    df_data = []\n",
    "                    cumulative_total = 0\n",
    "                    cumulative_pct = 0\n",
    "                    \n",
    "                    for atc_class in totals.index:\n",
    "                        class_total = totals[atc_class]\n",
    "                        cumulative_total += class_total\n",
    "                        percentage = (class_total / total_sum * 100) if total_sum > 0 else 0\n",
    "                        cumulative_pct += percentage\n",
    "                        \n",
    "                        row = {'ATC_Class': atc_class}\n",
    "                        \n",
    "                        # Add year-by-year data\n",
    "                        for year in years_list:\n",
    "                            if metric_name == 'NDCs':\n",
    "                                row[f'{metric_name}_{year}'] = int(comparison_df.loc[atc_class, year])\n",
    "                            else:\n",
    "                                row[f'{metric_name}_{year}'] = round(comparison_df.loc[atc_class, year], 3)\n",
    "                        \n",
    "                        # Add summary columns with proper naming for NDCs\n",
    "                        if metric_name == 'NDCs':\n",
    "                            row[f'Total_{metric_name}'] = int(class_total)\n",
    "                            row['Percentage'] = round(percentage, 2)\n",
    "                            row[f'Cumulative_{metric_name}'] = int(cumulative_total)\n",
    "                            row['Cumulative_Percentage_NDCs'] = round(cumulative_pct, 2)\n",
    "                        else:\n",
    "                            row[f'Total_{metric_name}'] = round(class_total, 3)\n",
    "                            row['Percentage'] = round(percentage, 2)\n",
    "                            row[f'Cumulative_{metric_name}'] = round(cumulative_total, 3)\n",
    "                            row['Cumulative_Percentage'] = round(cumulative_pct, 2)\n",
    "                        \n",
    "                        df_data.append(row)\n",
    "                    \n",
    "                    return pd.DataFrame(df_data)\n",
    "                \n",
    "                units_df = create_cumulative_df(units_comparison_df, 'Units')\n",
    "                prescriptions_df = create_cumulative_df(prescriptions_comparison_df, 'Prescriptions')\n",
    "                ndc_df = create_cumulative_df(ndc_comparison_df, 'NDCs')\n",
    "                \n",
    "                # Step 4: Export to Excel for this state\n",
    "                if output_filename is None:\n",
    "                    state_output_filename = f\"{level}_Cumulative_Analysis_{state}_with_NDC_Counts.xlsx\"\n",
    "                else:\n",
    "                    # Add state to provided filename\n",
    "                    name_parts = output_filename.split('.')\n",
    "                    state_output_filename = f\"{name_parts[0]}_{state}.{name_parts[1]}\"\n",
    "                \n",
    "                output_dir = os.path.join(base_path, \"ATC\\\\exported_analysis\")\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                output_path = os.path.join(output_dir, state_output_filename)\n",
    "                \n",
    "                with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "                    # Main analysis sheets\n",
    "                    units_df.to_excel(writer, sheet_name='Units_Reimbursed', index=False)\n",
    "                    prescriptions_df.to_excel(writer, sheet_name='Prescriptions', index=False)\n",
    "                    \n",
    "                    # NDC counts sheet\n",
    "                    if include_ndc_counts:\n",
    "                        ndc_df.to_excel(writer, sheet_name='NDC_Counts', index=False)\n",
    "    \n",
    "                print(f\"Exported {state} to Excel: {output_path}\")\n",
    "                all_output_paths[state] = output_path\n",
    "            \n",
    "            return all_output_paths\n",
    "        \n",
    "        else:\n",
    "            # Original total analysis continues exactly as before\n",
    "            # Step 2: Build comparison tables\n",
    "            all_classes = set()\n",
    "            for tbl in year_results.values():\n",
    "                if not tbl.empty:\n",
    "                    all_classes.update(tbl.index.tolist())\n",
    "            \n",
    "            # Also get classes from NDC counts\n",
    "            for tbl in ndc_counts.values():\n",
    "                if not tbl.empty:\n",
    "                    all_classes.update(tbl.index.tolist())\n",
    "            \n",
    "            if not all_classes:\n",
    "                print(\"No data found across all years!\")\n",
    "                return None\n",
    "            \n",
    "            # Build comparison tables for Units and Prescriptions (existing)\n",
    "            units_comparison = {cls: {y: float(year_results[y].loc[cls, 'Units_Reimbursed']) \n",
    "                                    if not year_results[y].empty and cls in year_results[y].index else 0.0\n",
    "                                    for y in years_list}\n",
    "                            for cls in sorted(all_classes)}\n",
    "            \n",
    "            prescriptions_comparison = {cls: {y: float(year_results[y].loc[cls, 'Number_of_Prescriptions']) \n",
    "                                            if not year_results[y].empty and cls in year_results[y].index else 0.0\n",
    "                                            for y in years_list}\n",
    "                                    for cls in sorted(all_classes)}\n",
    "            \n",
    "            # Build NDC counts comparison table\n",
    "            ndc_comparison = {cls: {y: int(ndc_counts[y].loc[cls, 'Unique_NDCs']) \n",
    "                                if not ndc_counts[y].empty and cls in ndc_counts[y].index else 0\n",
    "                                for y in years_list}\n",
    "                            for cls in sorted(all_classes)}\n",
    "            \n",
    "            units_comparison_df = pd.DataFrame(units_comparison).T\n",
    "            prescriptions_comparison_df = pd.DataFrame(prescriptions_comparison).T\n",
    "            ndc_comparison_df = pd.DataFrame(ndc_comparison).T\n",
    "            \n",
    "            # Sort by total units (keep same order across all sheets)\n",
    "            units_total = units_comparison_df.sum(axis=1).sort_values(ascending=False)\n",
    "            units_comparison_df = units_comparison_df.loc[units_total.index]\n",
    "            prescriptions_comparison_df = prescriptions_comparison_df.loc[units_total.index]\n",
    "            ndc_comparison_df = ndc_comparison_df.loc[units_total.index]\n",
    "            \n",
    "            # Step 3: Create cumulative frequency DataFrames\n",
    "            def create_cumulative_df(comparison_df, metric_name):\n",
    "                totals = comparison_df.sum(axis=1)\n",
    "                total_sum = totals.sum()\n",
    "                \n",
    "                df_data = []\n",
    "                cumulative_total = 0\n",
    "                cumulative_pct = 0\n",
    "                \n",
    "                for atc_class in totals.index:\n",
    "                    class_total = totals[atc_class]\n",
    "                    cumulative_total += class_total\n",
    "                    percentage = (class_total / total_sum * 100) if total_sum > 0 else 0\n",
    "                    cumulative_pct += percentage\n",
    "                    \n",
    "                    row = {'ATC_Class': atc_class}\n",
    "                    \n",
    "                    # Add year-by-year data\n",
    "                    for year in years_list:\n",
    "                        if metric_name == 'NDCs':\n",
    "                            row[f'{metric_name}_{year}'] = int(comparison_df.loc[atc_class, year])\n",
    "                        else:\n",
    "                            row[f'{metric_name}_{year}'] = round(comparison_df.loc[atc_class, year], 3)\n",
    "                    \n",
    "                    # Add summary columns with proper naming for NDCs\n",
    "                    if metric_name == 'NDCs':\n",
    "                        row[f'Total_{metric_name}'] = int(class_total)\n",
    "                        row['Percentage'] = round(percentage, 2)\n",
    "                        row[f'Cumulative_{metric_name}'] = int(cumulative_total)\n",
    "                        row['Cumulative_Percentage_NDCs'] = round(cumulative_pct, 2)\n",
    "                    else:\n",
    "                        row[f'Total_{metric_name}'] = round(class_total, 3)\n",
    "                        row['Percentage'] = round(percentage, 2)\n",
    "                        row[f'Cumulative_{metric_name}'] = round(cumulative_total, 3)\n",
    "                        row['Cumulative_Percentage'] = round(cumulative_pct, 2)\n",
    "                    \n",
    "                    df_data.append(row)\n",
    "                \n",
    "                return pd.DataFrame(df_data)\n",
    "            \n",
    "            units_df = create_cumulative_df(units_comparison_df, 'Units')\n",
    "            prescriptions_df = create_cumulative_df(prescriptions_comparison_df, 'Prescriptions')\n",
    "            ndc_df = create_cumulative_df(ndc_comparison_df, 'NDCs')\n",
    "            \n",
    "            # Step 4: Export to Excel with multiple sheets\n",
    "            if output_filename is None:\n",
    "                output_filename = f\"{level}_Cumulative_Analysis_with_NDC_Counts.xlsx\"\n",
    "            \n",
    "            output_dir = os.path.join(base_path, \"ATC\\\\exported_analysis\")\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "                # Main analysis sheets\n",
    "                units_df.to_excel(writer, sheet_name='Units_Reimbursed', index=False)\n",
    "                prescriptions_df.to_excel(writer, sheet_name='Prescriptions', index=False)\n",
    "                \n",
    "                # NDC counts sheet - this is what you wanted!\n",
    "                if include_ndc_counts:\n",
    "                    ndc_df.to_excel(writer, sheet_name='NDC_Counts', index=False)\n",
    "    \n",
    "            print(f\"\\nExported to Excel: {output_path}\")\n",
    "            \n",
    "            return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "workflow_execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = NDCATCProcessor(year=2017)\n",
    "processor.clean_sdud_data()           # Clean SDUD data\n",
    "processor.adding_key()                # Add record_id key\n",
    "#analyzer.generate_ndc_txt()          # Generate NDC text file\n",
    "processor.analyze_atc4_mapping() \n",
    "processor.fetch_atc_names()           \n",
    "processor.prepare_final_dataframe()   \n",
    "processor.export_merged_data()  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644c005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "atc2_dist = NDCATCProcessor.analyze_atc_distribution(level='ATC2')\n",
    "atc3_dist = processor.analyze_atc_distribution(level='ATC3')\n",
    "atc4_dist = processor.analyze_atc_distribution(level='ATC4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just checking overlap between files with and without key\n",
    "nokey_path=rf'C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\ATC4_classes\\Classes_notgood\\NDCf_2023_ATC4_classes.csv'\n",
    "keyed_path=rf'C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\ATC4_classes\\NDCNEW_2023_ATC4_classes.csv'\n",
    "\n",
    "# Load them\n",
    "keyed = pd.read_csv(keyed_path, dtype=str)\n",
    "nokey = pd.read_csv(nokey_path, dtype=str)\n",
    "\n",
    "# Normalize NDCs (remove hyphens, pad to 11 digits)\n",
    "for df in [keyed, nokey]:\n",
    "    df[\"NDC\"] = df[\"NDC\"].str.replace(\"-\", \"\", regex=False).str.zfill(11)\n",
    "\n",
    "# --- Summary stats ---\n",
    "summary = {\n",
    "    \"File\": [\"With key (NDCNEW_2024_ATC4_classes)\", \"Without key (NDCf_2024_ATC4_classes)\"],\n",
    "    \"Total rows\": [len(keyed), len(nokey)],\n",
    "    \"Unique NDCs\": [keyed[\"NDC\"].nunique(), nokey[\"NDC\"].nunique()],\n",
    "    \"Mapped NDCs (non-null ATC)\": [\n",
    "        keyed[\"ATC4 Class\"].notna().sum(),\n",
    "        nokey[\"ATC4 Class\"].notna().sum(),\n",
    "    ],\n",
    "}\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# --- Compare overlap of unique NDCs ---\n",
    "ndc_keyed = set(keyed[\"NDC\"].unique())\n",
    "ndc_nokey = set(nokey[\"NDC\"].unique())\n",
    "\n",
    "overlap_ndcs = len(ndc_keyed & ndc_nokey)\n",
    "only_in_nokey = len(ndc_nokey - ndc_keyed)\n",
    "only_in_keyed = len(ndc_keyed - ndc_nokey)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Metric\": [\"Overlap NDCs\", \"Only in without-key file\", \"Only in with-key file\", \"Percent overlap\"],\n",
    "    \"Value\": [overlap_ndcs, only_in_nokey, only_in_keyed, overlap_ndcs / len(ndc_nokey) * 100]\n",
    "})\n",
    "\n",
    "print(\"\\n=== Summary of Each File ===\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== NDC Overlap Comparison ===\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c36c361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ATC2 Cumulative Frequency Analysis Excel for States: IN\n",
      "======================================================================\n",
      "Processing 2017... ✓ (States: 1)\n",
      "Processing 2018... ✓ (States: 1)\n",
      "Processing 2019... ✓ (States: 1)\n",
      "Processing 2020... ✓ (States: 1)\n",
      "Processing 2021... ✓ (States: 1)\n",
      "Processing 2022... ✓ (States: 1)\n",
      "Processing 2023... ✓ (States: 1)\n",
      "Processing 2024... ✓ (States: 1)\n",
      "Exported IN to Excel: c:\\Users\\lholguin\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\exported_analysis\\ATC2_Cumulative_Analysis_IN_with_NDC_Counts.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Indiana only\n",
    "years_list = [2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
    "paths = NDCATC_overview.export_cumulative_frequency_excel(years_list, by_state=True, state_filter='IN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c7700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33987ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new class to analyze correlations\n",
    "class NDCATC_ind:\n",
    "    @staticmethod\n",
    "    def analyze_correlation_by_state_atc(years_list, base_path=None, min_records=10):\n",
    "        from scipy.stats import pearsonr, spearmanr\n",
    "        import os, pandas as pd\n",
    "        \n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(\"CORRELATION ANALYSIS: Units Reimbursed vs Number of Prescriptions\")\n",
    "        print(\"INDIANA ONLY - By ATC Class\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        all_correlations = []\n",
    "        \n",
    "        for year in years_list:\n",
    "            print(f\"Processing {year}...\", end=\" \")\n",
    "            try:\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                \n",
    "                records = df_merged[(df_merged['ATC4 Class'].notna()) & (df_merged['State'] == 'IN')].copy()\n",
    "                if records.empty:\n",
    "                    print(\"No valid Indiana records\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"✓ ({len(records):,} records)\")\n",
    "                \n",
    "                for atc2 in records['ATC2 Class'].unique():\n",
    "                    subset = records[records['ATC2 Class'] == atc2]\n",
    "                    if len(subset) >= min_records:\n",
    "                        try:\n",
    "                            pearson_r, pearson_p = pearsonr(subset['Units Reimbursed'], subset['Number of Prescriptions'])\n",
    "                            spearman_r, spearman_p = spearmanr(subset['Units Reimbursed'], subset['Number of Prescriptions'])\n",
    "                            atc2_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns else ''\n",
    "                            \n",
    "                            all_correlations.append({\n",
    "                                'Year': year, 'State': 'IN', 'ATC2_Class': atc2, 'ATC2_Name': atc2_name,\n",
    "                                'N_Records': len(subset), 'Pearson_r': pearson_r, 'Pearson_p': pearson_p,\n",
    "                                'Spearman_r': spearman_r, 'Spearman_p': spearman_p\n",
    "                            })\n",
    "                        except Exception as e:\n",
    "                            print(f\"\\nError processing IN-{atc2}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "        \n",
    "        correlations_df = pd.DataFrame(all_correlations)\n",
    "        if correlations_df.empty:\n",
    "            print(\"No correlation data collected!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # SUMMARY STATISTICS\n",
    "        print(f\"\\n{'='*70}\\nSUMMARY STATISTICS FOR INDIANA\\n{'='*70}\")\n",
    "        print(f\"Total combinations: {len(correlations_df):,} | Years: {sorted(correlations_df['Year'].unique())} | ATC classes: {len(correlations_df['ATC2_Class'].unique())}\")\n",
    "        \n",
    "        # PEARSON CORRELATION RESULTS\n",
    "        print(f\"\\n{'='*70}\\nPEARSON CORRELATION RESULTS\\n{'='*70}\")\n",
    "        print(f\"Average: {correlations_df['Pearson_r'].mean():.4f} | Range: {correlations_df['Pearson_r'].min():.4f} to {correlations_df['Pearson_r'].max():.4f} | Std Dev: {correlations_df['Pearson_r'].std():.4f}\")\n",
    "        \n",
    "        atc_pearson = correlations_df.groupby(['ATC2_Class', 'ATC2_Name']).agg({\n",
    "            'Pearson_r': ['mean', 'std'], 'Pearson_p': 'mean', 'N_Records': 'sum'\n",
    "        }).round(4)\n",
    "        atc_pearson.columns = ['Avg_Pearson', 'Std_Pearson', 'Avg_P_Value', 'Total_Records']\n",
    "        atc_pearson = atc_pearson.sort_values('Avg_Pearson', ascending=False)\n",
    "        \n",
    "        print(f\"\\nPEARSON BY ATC CLASS (Average across years):\")\n",
    "        print(f\"{'ATC2':<5} {'Name':<25} {'Avg Pearson':<12} {'Std':<8} {'p-val':<8} {'Total N':<8}\\n{'-' * 75}\")\n",
    "        for (atc_class, atc_name), row in atc_pearson.iterrows():\n",
    "            name_short = atc_name[:23] if atc_name else atc_class\n",
    "            print(f\"{atc_class:<5} {name_short:<25} {row['Avg_Pearson']:<12.4f} {row['Std_Pearson']:<8.4f} {row['Avg_P_Value']:<8.4f} {row['Total_Records']:<8.0f}\")\n",
    "        \n",
    "        # SPEARMAN CORRELATION RESULTS\n",
    "        print(f\"\\n{'='*70}\\nSPEARMAN RANK CORRELATION RESULTS\\n{'='*70}\")\n",
    "        print(f\"Average: {correlations_df['Spearman_r'].mean():.4f} | Range: {correlations_df['Spearman_r'].min():.4f} to {correlations_df['Spearman_r'].max():.4f} | Std Dev: {correlations_df['Spearman_r'].std():.4f}\")\n",
    "        \n",
    "        atc_spearman = correlations_df.groupby(['ATC2_Class', 'ATC2_Name']).agg({\n",
    "            'Spearman_r': ['mean', 'std'], 'Spearman_p': 'mean', 'N_Records': 'sum'\n",
    "        }).round(4)\n",
    "        atc_spearman.columns = ['Avg_Spearman', 'Std_Spearman', 'Avg_P_Value', 'Total_Records']\n",
    "        atc_spearman = atc_spearman.sort_values('Avg_Spearman', ascending=False)\n",
    "        \n",
    "        print(f\"\\nSPEARMAN BY ATC CLASS (Average across years):\")\n",
    "        print(f\"{'ATC2':<5} {'Name':<25} {'Avg Spearman':<12} {'Std':<8} {'p-val':<8} {'Total N':<8}\\n{'-' * 75}\")\n",
    "        for (atc_class, atc_name), row in atc_spearman.iterrows():\n",
    "            name_short = atc_name[:23] if atc_name else atc_class\n",
    "            print(f\"{atc_class:<5} {name_short:<25} {row['Avg_Spearman']:<12.4f} {row['Std_Spearman']:<8.4f} {row['Avg_P_Value']:<8.4f} {row['Total_Records']:<8.0f}\")\n",
    "        # YEAR-OVER-YEAR TRENDS\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"YEAR-OVER-YEAR TRENDS (Top 3 by Pearson)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        top_classes = atc_pearson.head(3).index.get_level_values(0).tolist()\n",
    "        for atc_class in top_classes:\n",
    "            atc_data = correlations_df[correlations_df['ATC2_Class'] == atc_class].sort_values('Year')\n",
    "            atc_name = atc_data['ATC2_Name'].iloc[0] if not atc_data.empty else atc_class\n",
    "            \n",
    "            print(f\"\\n{atc_class} - {atc_name[:30]}:\")\n",
    "            print(f\"{'Year':<6} {'Pearson':<8} {'Spearman':<9} {'N':<6}\")\n",
    "            print(\"-\" * 35)\n",
    "            for _, row in atc_data.iterrows():\n",
    "                print(f\"{row['Year']:<6} {row['Pearson_r']:<8.4f} {row['Spearman_r']:<9.4f} {row['N_Records']:<6}\")\n",
    "        \n",
    "        return correlations_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
