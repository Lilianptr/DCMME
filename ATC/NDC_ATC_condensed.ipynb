{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b3e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import shelve\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import re\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import statsmodels \n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.sandbox.stats.runs import runstest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f7f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "user=\"lholguin\"\n",
    "years_list = [2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
    "#user in personal pc1 <- \"asus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aebaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDCATCProcessor:\n",
    "\n",
    "    def __init__(self, year, base_path=None):\n",
    "        self.year = year\n",
    "        self.base_path = base_path or rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        self.df_cleaned = None\n",
    "        self.df_merged = None\n",
    "        self.atc_mapping = None\n",
    "        \n",
    "    def clean_sdud_data(self):\n",
    "\n",
    "        csv_file = os.path.join(self.base_path, f\"SDUD\\\\SDUD{self.year}.csv\")\n",
    "        print(f\"Reading CSV: {csv_file}\")\n",
    "        \n",
    "        df = pd.read_csv(csv_file, dtype={'NDC': 'object'})\n",
    "        print(f\"Initial rows: {len(df):,}\")\n",
    "        \n",
    "        # Filter data\n",
    "        df_filtered = df.dropna(subset=['Units Reimbursed', 'Number of Prescriptions']) #dropping na values\n",
    "        df_filtered = df_filtered[df_filtered['State'] != 'XX']\n",
    "        \n",
    "        print(f\"After cleaning: {len(df_filtered):,} rows, {df_filtered['NDC'].nunique():,} unique NDCs\")\n",
    "        \n",
    "        self.df_cleaned = df_filtered\n",
    "        return self.df_cleaned\n",
    "    \n",
    "    def adding_key(self):\n",
    "        if self.df_cleaned is None:\n",
    "            raise ValueError(\"Run clean_sdud_data() first\")\n",
    "        \n",
    "        self.df_cleaned['record_id'] = (\n",
    "            self.df_cleaned['State'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['Year'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['Quarter'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['Utilization Type'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['NDC'].astype(str)\n",
    "        )\n",
    "        \n",
    "        print(f\"Created {len(self.df_cleaned):,} record IDs\")\n",
    "        return self.df_cleaned\n",
    "    \n",
    "    def generate_ndc_txt(self, output_filename=None):\n",
    "        if 'record_id' not in self.df_cleaned.columns:\n",
    "            raise ValueError(\"Run adding_key() first\")\n",
    "            \n",
    "        output_filename = output_filename or f\"NDCNEW_{self.year}.txt\"\n",
    "        output_path = os.path.join(self.base_path, f\"ATC\\\\text_files\\\\{output_filename}\")\n",
    "        \n",
    "        unique_pairs = self.df_cleaned[['NDC', 'record_id']].drop_duplicates()\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(\"NDC\\trecord_id\\n\")\n",
    "            for _, row in unique_pairs.iterrows():\n",
    "                f.write(f\"{row['NDC']}\\t{row['record_id']}\\n\")\n",
    "        \n",
    "        print(f\"Exported {unique_pairs['record_id'].nunique():,} unique records to {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    #This method was modified by 12/12 to make the table of distribution ATC-level classes before deduplication\n",
    "    #Therefore, the old method \"analyze_atc_distribution\" is discarded\n",
    "    def analyze_atc4_mapping(self):\n",
    "\n",
    "        if 'record_id' not in self.df_cleaned.columns:\n",
    "            raise ValueError(\"Run adding_key() first\")\n",
    "            \n",
    "        atc4_path = os.path.join(self.base_path, f\"ATC\\\\ATC4_classes\\\\NDCNEW_{self.year}_ATC4_classes.csv\")\n",
    "        \n",
    "        # Load ATC4 mapping\n",
    "        df_atc4 = pd.read_csv(atc4_path, dtype={'NDC': 'object', 'record_id': 'string'})\n",
    "        df_atc4['NDC'] = df_atc4['NDC'].str.zfill(11)\n",
    "        \n",
    "        print(f\"ATC4 file: {len(df_atc4):,} rows, {df_atc4['NDC'].nunique():,} unique NDCs\")\n",
    "        \n",
    "        # Ensure consistent types\n",
    "        self.df_cleaned['record_id'] = self.df_cleaned['record_id'].astype('string')\n",
    "        self.df_cleaned['NDC'] = self.df_cleaned['NDC'].astype('object')\n",
    "        \n",
    "        # Merge on both record_id and NDC\n",
    "        self.atc_mapping = pd.merge(\n",
    "            self.df_cleaned,\n",
    "            df_atc4[['record_id', 'NDC', 'ATC4 Class']],\n",
    "            on=['record_id', 'NDC'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Analyze distribution BEFORE deduplication\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"DISTRIBUTION ANALYSIS (BEFORE DEDUPLICATION)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        records = self.atc_mapping[self.atc_mapping['ATC4 Class'].notna()].copy()\n",
    "        \n",
    "        if len(records) > 0:\n",
    "            # Analyze all three ATC levels\n",
    "            for level in ['ATC4', 'ATC3', 'ATC2']:\n",
    "                # Create ATC level column if needed\n",
    "                if level == 'ATC3':\n",
    "                    records['ATC3 Class'] = records['ATC4 Class'].str[:4]\n",
    "                    class_col = 'ATC3 Class'\n",
    "                elif level == 'ATC2':\n",
    "                    records['ATC2 Class'] = records['ATC4 Class'].str[:3]\n",
    "                    class_col = 'ATC2 Class'\n",
    "                else:\n",
    "                    class_col = 'ATC4 Class'\n",
    "                \n",
    "                # Count classes per record_id\n",
    "                per_record = records.groupby('record_id')[class_col].nunique().reset_index()\n",
    "                per_record.columns = ['record_id', 'num_classes']\n",
    "                \n",
    "                distribution = per_record['num_classes'].value_counts().sort_index()\n",
    "                \n",
    "                print(f\"\\n{level} CLASSES PER RECORD_ID:\")\n",
    "                for n_classes, count in distribution.items():\n",
    "                    pct = (count / len(per_record)) * 100\n",
    "                    print(f\"  {n_classes} class(es): {count:,} records ({pct:.1f}%)\")\n",
    "                \n",
    "                print(f\"  Avg {level} per record: {per_record['num_classes'].mean():.2f}\")\n",
    "                print(f\"  Max {level} per record: {per_record['num_classes'].max()}\")\n",
    "        else:\n",
    "            print(\"No records with valid ATC4 mappings found.\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        \n",
    "        # Deduplication after merge by record_id\n",
    "        before_count = len(self.atc_mapping) \n",
    "        self.atc_mapping = self.atc_mapping.drop_duplicates(subset='record_id', keep='first')\n",
    "        \n",
    "        total = len(self.atc_mapping)\n",
    "        mapped = self.atc_mapping['ATC4 Class'].notna().sum()\n",
    "        print(f\"Merged: {total:,} records, {mapped:,} with ATC4 ({mapped/total*100:.1f}%)\")\n",
    "        \n",
    "        missing = total - mapped\n",
    "        if missing > 0:\n",
    "            print(f\"Missing: {missing:,} records, {self.atc_mapping[self.atc_mapping['ATC4 Class'].isna()]['NDC'].nunique():,} unique NDCs\")\n",
    "        \n",
    "        return self.atc_mapping\n",
    "\n",
    "    def fetch_atc_names(self, cache_path=None):\n",
    "        \"\"\"Fetch ATC class names (ATC4, ATC3, ATC2) from RxNav API.\"\"\"\n",
    "        if self.atc_mapping is None:\n",
    "            raise ValueError(\"Must run analyze_atc4_mapping() first\")\n",
    "        \n",
    "        if cache_path is None:\n",
    "            cache_path = os.path.join(self.base_path, \"ATC\\\\cache_files\\\\atc_names_cache\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"FETCHING ATC CLASS NAMES\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Using cache: {cache_path}\")\n",
    "        \n",
    "        # Get only records with valid ATC4 mappings\n",
    "        df_with_atc = self.atc_mapping[self.atc_mapping['ATC4 Class'].notna()].copy()\n",
    "        \n",
    "        # Create ATC3 and ATC2 columns from ATC4\n",
    "        print(\"\\nCreating ATC3 and ATC2 columns from ATC4...\")\n",
    "        df_with_atc['ATC3 Class'] = df_with_atc['ATC4 Class'].str[:4]\n",
    "        df_with_atc['ATC2 Class'] = df_with_atc['ATC4 Class'].str[:3]\n",
    "        \n",
    "        # Get unique codes for each level\n",
    "        unique_atc4 = df_with_atc['ATC4 Class'].dropna().unique()\n",
    "        unique_atc3 = df_with_atc['ATC3 Class'].dropna().unique()\n",
    "        unique_atc2 = df_with_atc['ATC2 Class'].dropna().unique()\n",
    "        \n",
    "        # Filter out invalid codes\n",
    "        unique_atc4 = [c for c in unique_atc4 if c not in ['No ATC Mapping Found', 'No RxCUI Found', '']]\n",
    "        unique_atc3 = [c for c in unique_atc3 if c not in ['No ATC Mapping Found', 'No RxCUI Found', '', 'No ', 'No']]\n",
    "        unique_atc2 = [c for c in unique_atc2 if c not in ['No ATC Mapping Found', 'No RxCUI Found', '', 'No ', 'No']]\n",
    "        \n",
    "        print(f\"\\nUnique codes to fetch:\")\n",
    "        print(f\"  ATC4: {len(unique_atc4)}\")\n",
    "        print(f\"  ATC3: {len(unique_atc3)}\")\n",
    "        print(f\"  ATC2: {len(unique_atc2)}\")\n",
    "        \n",
    "        # Build mappings\n",
    "        atc4_names = {}\n",
    "        atc3_names = {}\n",
    "        atc2_names = {}\n",
    "        \n",
    "        with shelve.open(cache_path) as cache:\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            print(\"\\nFetching ATC4 names...\")\n",
    "            for code in unique_atc4:\n",
    "                atc4_names[code] = self._get_atc_name(code, cache)\n",
    "            \n",
    "            print(\"Fetching ATC3 names...\")\n",
    "            for code in unique_atc3:\n",
    "                atc3_names[code] = self._get_atc_name(code, cache)\n",
    "            \n",
    "            print(\"Fetching ATC2 names...\")\n",
    "            for code in unique_atc2:\n",
    "                atc2_names[code] = self._get_atc_name(code, cache)\n",
    "            \n",
    "            print(f\"\\nTotal processing time: {(datetime.now() - start_time).total_seconds()/60:.1f} minutes\")\n",
    "        \n",
    "        # Apply names to all records in atc_mapping\n",
    "        print(\"\\nApplying names to dataframe...\")\n",
    "        self.atc_mapping['ATC3 Class'] = self.atc_mapping['ATC4 Class'].str[:4]\n",
    "        self.atc_mapping['ATC2 Class'] = self.atc_mapping['ATC4 Class'].str[:3]\n",
    "        \n",
    "        self.atc_mapping['ATC4_Name'] = self.atc_mapping['ATC4 Class'].map(atc4_names).fillna('')\n",
    "        self.atc_mapping['ATC3_Name'] = self.atc_mapping['ATC3 Class'].map(atc3_names).fillna('')\n",
    "        self.atc_mapping['ATC2_Name'] = self.atc_mapping['ATC2 Class'].map(atc2_names).fillna('')\n",
    "        \n",
    "        print(f\"\\nATC names added successfully!\")\n",
    "        print(\"\\nSample output:\")\n",
    "        sample = self.atc_mapping[self.atc_mapping['ATC4 Class'].notna()][['NDC', 'record_id', 'ATC4 Class', 'ATC4_Name', 'ATC3 Class', 'ATC3_Name', 'ATC2 Class', 'ATC2_Name']].head(5)\n",
    "        print(sample.to_string())\n",
    "        \n",
    "        return self.atc_mapping\n",
    "    \n",
    "    def prepare_final_dataframe(self):\n",
    "\n",
    "        if self.atc_mapping is None:\n",
    "            raise ValueError(\"Run fetch_atc_names() first\")\n",
    "        \n",
    "        self.df_merged = self.atc_mapping.copy()\n",
    "        \n",
    "        # Scale units\n",
    "        self.df_merged['Units Reimbursed'] = self.df_merged['Units Reimbursed'] / 1e9\n",
    "        self.df_merged['Number of Prescriptions'] = self.df_merged['Number of Prescriptions'] / 1e6\n",
    "        \n",
    "        total = len(self.df_merged)\n",
    "        mapped = self.df_merged['ATC4 Class'].notna().sum();\n",
    "        \n",
    "        print(f\"\\nFinal Statistics:\")\n",
    "        print(f\"  Records: {total:,} ({mapped:,} with ATC4, {mapped/total*100:.1f}%)\")\n",
    "        print(f\"  Units Reimbursed: {self.df_merged['Units Reimbursed'].sum():.2f} Billion\")\n",
    "        print(f\"  Prescriptions: {self.df_merged['Number of Prescriptions'].sum():.2f} Million\")\n",
    "        \n",
    "        return self.df_merged\n",
    "    \n",
    "    def _get_atc_name(self, atc_code, cache):\n",
    "        \"\"\"Helper: Fetch ATC name from RxNav API with caching.\"\"\"\n",
    "        cache_key = f\"atc_name:{atc_code}\"\n",
    "        if cache_key in cache:\n",
    "            return cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            url = f\"https://rxnav.nlm.nih.gov/REST/rxclass/class/byId.json?classId={atc_code}\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'rxclassMinConceptList' in data and 'rxclassMinConcept' in data['rxclassMinConceptList']:\n",
    "                concepts = data['rxclassMinConceptList']['rxclassMinConcept']\n",
    "                if concepts:\n",
    "                    name = concepts[0].get('className', '')\n",
    "                    cache[cache_key] = name\n",
    "                    return name\n",
    "            \n",
    "            cache[cache_key] = ''\n",
    "            return ''\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving {atc_code}: {e}\")\n",
    "            cache[cache_key] = ''\n",
    "            return ''\n",
    "\n",
    "    def export_merged_data(self, output_filename=None, show_details=True):\n",
    "\n",
    "        if self.df_merged is None:\n",
    "            raise ValueError(\"Run prepare_final_dataframe() first\")\n",
    "            \n",
    "        output_filename = output_filename or f\"merged_NEWdata_{self.year}.csv\"\n",
    "        output_path = os.path.join(self.base_path, f\"ATC\\\\merged_data\\\\{output_filename}\")\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Check duplicates\n",
    "        initial_count = len(self.df_merged)\n",
    "        duplicate_count = self.df_merged['record_id'].duplicated().sum()\n",
    "        \n",
    "        print(f\"\\nDeduplication Check:\")\n",
    "        print(f\"  Before: {initial_count:,} rows\")\n",
    "        print(f\"  Duplicates: {duplicate_count:,}\")\n",
    "        \n",
    "        # Show sample duplicates if requested\n",
    "        if show_details and duplicate_count > 0:\n",
    "            dup_records = self.df_merged[self.df_merged['record_id'].duplicated(keep=False)].sort_values('record_id')\n",
    "            sample_ids = dup_records['record_id'].unique()[:2]\n",
    "            \n",
    "            print(f\"\\nSample duplicate record_ids:\")\n",
    "            for rid in sample_ids:\n",
    "                sample = self.df_merged[self.df_merged['record_id'] == rid][\n",
    "                    ['record_id', 'NDC', 'State', 'ATC4 Class', 'ATC2 Class']\n",
    "                ]\n",
    "                print(f\"\\n{rid}:\")\n",
    "                print(sample.to_string(index=False))\n",
    "        \n",
    "        # Deduplicate and export\n",
    "        df_final = self.df_merged.drop_duplicates(subset='record_id', keep='first')\n",
    "        df_final.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\n  After: {len(df_final):,} rows\")\n",
    "        print(f\"  Removed: {initial_count - len(df_final):,}\")\n",
    "        print(f\"\\nExported to: {output_path}\")\n",
    "\n",
    "        #Showing final atc class mapping in %\n",
    "        final_count = len(df_final)\n",
    "        final_mapped_records = df_final['ATC4 Class'].notna().sum()\n",
    "        #percentages\n",
    "        print(f\"  Mapped Records: {final_mapped_records:,} ({final_mapped_records/final_count*100:.1f}%)\")\n",
    "        final_unmapped_records = final_count - final_mapped_records\n",
    "        final_mapped_ndcs = df_final[df_final['ATC4 Class'].notna()]['NDC'].nunique()\n",
    "        final_unmapped_ndcs = df_final['NDC'].nunique()\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        agg = df_final.groupby('record_id').agg({\n",
    "            'Units Reimbursed': 'sum',\n",
    "            'Number of Prescriptions': 'sum'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nAggregated Totals:\")\n",
    "        print(f\"  Units Reimbursed: {agg['Units Reimbursed'].sum():.3f} Billion\")\n",
    "        print(f\"  Number of Prescriptions: {agg['Number of Prescriptions'].sum():3f} Million\")\n",
    "        \n",
    "        return output_path\n",
    "\n",
    "    #New method to see the years trend in mapping\n",
    "    @staticmethod\n",
    "    def analyze_merged_data_multi_year(years_list, base_path=None):\n",
    "    \n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(\"MULTI-YEAR MERGED DATA ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        results = {}\n",
    "        for year in years_list:\n",
    "            try:\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df = pd.read_csv(csv_path)\n",
    "                \n",
    "                mapped = df['ATC4 Class'].notna().sum()\n",
    "                mapped_ndcs = df[df['ATC4 Class'].notna()]['NDC'].nunique()\n",
    "                total_ndcs = df['NDC'].nunique()\n",
    "                \n",
    "                agg = df.groupby('record_id').agg({\n",
    "                    'Units Reimbursed': 'sum',\n",
    "                    'Number of Prescriptions': 'sum'\n",
    "                })\n",
    "                \n",
    "                results[year] = {\n",
    "                    'total_records': len(df),\n",
    "                    'mapped_records': mapped,\n",
    "                    'mapping_pct': (mapped / len(df) * 100) if len(df) > 0 else 0,\n",
    "                    'total_ndcs': total_ndcs,\n",
    "                    'mapped_ndcs': mapped_ndcs,\n",
    "                    'ndc_mapping_pct': (mapped_ndcs / total_ndcs * 100) if total_ndcs > 0 else 0,\n",
    "                    'units_reimbursed': agg['Units Reimbursed'].sum(),\n",
    "                    'prescriptions': agg['Number of Prescriptions'].sum()\n",
    "                }\n",
    "                \n",
    "                r = results[year]\n",
    "                print(f\"\\n{year}: {r['total_records']:,} records | \"\n",
    "                      f\"{r['mapped_records']:,} mapped ({r['mapping_pct']:.1f}%) | \"\n",
    "                      f\"{r['mapped_ndcs']:,}/{r['total_ndcs']:,} NDCs ({r['ndc_mapping_pct']:.1f}%) | \"\n",
    "                      f\"Units: {r['units_reimbursed']:.3f}B | Rx: {r['prescriptions']:.3f}M\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n{year}: Error - {e}\")\n",
    "                results[year] = None\n",
    "        \n",
    "        # Summary table\n",
    "        valid = {y: r for y, r in results.items() if r is not None}\n",
    "        if valid:\n",
    "            df_summary = pd.DataFrame(valid).T\n",
    "            print(f\"\\n{'='*80}\\nSUMMARY TABLE\\n{'='*80}\")\n",
    "            print(df_summary.to_string())\n",
    "            \n",
    "            if len(valid) > 1:\n",
    "                years = sorted(valid.keys())\n",
    "                first, last = years[0], years[-1]\n",
    "                units_chg = (valid[last]['units_reimbursed'] - valid[first]['units_reimbursed']) / valid[first]['units_reimbursed'] * 100\n",
    "                presc_chg = (valid[last]['prescriptions'] - valid[first]['prescriptions']) / valid[first]['prescriptions'] * 100\n",
    "                \n",
    "                print(f\"\\n{first}→{last}: Units {units_chg:+.1f}%, Prescriptions {presc_chg:+.1f}%\")\n",
    "            \n",
    "            # Create visualizations\n",
    "            print(f\"\\n{'='*80}\\nCREATING VISUALIZATIONS\\n{'='*80}\")\n",
    "            \n",
    "            # Extract data for plotting\n",
    "            years_sorted = sorted(valid.keys())\n",
    "            total_recs = [valid[y]['total_records']/1e6 for y in years_sorted]\n",
    "            mapped_recs = [valid[y]['mapped_records']/1e6 for y in years_sorted]\n",
    "            mapped_ndcs = [valid[y]['mapped_ndcs']/1e3 for y in years_sorted]\n",
    "            units = [valid[y]['units_reimbursed'] for y in years_sorted]\n",
    "            prescriptions = [valid[y]['prescriptions'] for y in years_sorted]\n",
    "            \n",
    "            # Plot 1: Records and NDCs\n",
    "            fig1 = plt.figure(figsize=(10, 6))\n",
    "            ax1 = fig1.add_subplot(111)\n",
    "            ax1_twin = ax1.twinx()\n",
    "            \n",
    "            line1 = ax1.plot(years_sorted, total_recs, marker='o', linewidth=2, \n",
    "                             label='Total Records', color=\"#001F47\", markersize=8)\n",
    "            line2 = ax1.plot(years_sorted, mapped_recs, marker='s', linewidth=2, \n",
    "                             label='Mapped Records', color=\"#00039E\", markersize=8)\n",
    "            line3 = ax1_twin.plot(years_sorted, mapped_ndcs, marker='^', linewidth=2, \n",
    "                                  label='Mapped NDCs', color=\"#62B6FF\", markersize=8)\n",
    "            \n",
    "            ax1.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "            ax1.set_ylabel('Records (Millions)', fontsize=11, fontweight='bold', color='black')\n",
    "            ax1_twin.set_ylabel('Unique NDCs Mapped (Thousands)', fontsize=11, fontweight='bold', color=\"#000000\")\n",
    "            ax1.set_title('Records and NDC Mapping Trends', fontsize=13, fontweight='bold', pad=15)\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.tick_params(axis='y', labelcolor='black')\n",
    "            ax1_twin.tick_params(axis='y', labelcolor=\"#000000\")\n",
    "            \n",
    "            lines = line1 + line2 + line3\n",
    "            labels = [l.get_label() for l in lines]\n",
    "            ax1.legend(lines, labels, loc='upper left', fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Plot 2: Units Reimbursed and Prescriptions\n",
    "            fig2 = plt.figure(figsize=(10, 6))\n",
    "            ax2 = fig2.add_subplot(111)\n",
    "            ax2_twin = ax2.twinx()\n",
    "            \n",
    "            line4 = ax2.plot(years_sorted, units, marker='o', linewidth=2, \n",
    "                             label='Units Reimbursed', color=\"#2F2000\", markersize=8)\n",
    "            line5 = ax2_twin.plot(years_sorted, prescriptions, marker='D', linewidth=2, \n",
    "                                  label='Prescriptions', color=\"#A15C23\", markersize=8)\n",
    "            \n",
    "            ax2.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "            ax2.set_ylabel('Units Reimbursed (Billions)', fontsize=11, fontweight='bold', color=\"#000000\")\n",
    "            ax2_twin.set_ylabel('Number of Prescriptions (Millions)', fontsize=11, fontweight='bold', color=\"#000000\")\n",
    "            ax2.set_title('UR and NoP Trends', fontsize=13, fontweight='bold', pad=15)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.tick_params(axis='y', labelcolor=\"#000000\")\n",
    "            ax2_twin.tick_params(axis='y', labelcolor=\"#000000\")\n",
    "            \n",
    "            lines2 = line4 + line5\n",
    "            labels2 = [l.get_label() for l in lines2]\n",
    "            ax2.legend(lines2, labels2, loc='upper left', fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"✓ Visualizations created successfully\")\n",
    "        \n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f744dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the method export_merged_data\n",
    "#processor = NDCATCProcessor(year=2017)\n",
    "#processor.clean_sdud_data()           # Clean SDUD data\n",
    "#processor.adding_key()                # Add record_id key\n",
    "#analyzer.generate_ndc_txt()          # Generate NDC text file\n",
    "#processor.analyze_atc4_mapping()\n",
    "#processor.fetch_atc_names()           \n",
    "#processor.prepare_final_dataframe()   \n",
    "#processor.export_merged_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd829182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = NDCATCProcessor.analyze_merged_data_multi_year(years_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDCATC_overview:\n",
    "\n",
    "    @staticmethod\n",
    "    def analyze_general_atc_overview_by_state(years_list, base_path=None, state_filter=None):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "\n",
    "        if state_filter:\n",
    "            if isinstance(state_filter, str):\n",
    "                state_filter = [state_filter]\n",
    "            print(f\"Creating ATC2 & ATC3 Overview by State for: {', '.join(state_filter)}\")\n",
    "        else:\n",
    "            print(\"Creating ATC2 & ATC3 Overview by State (All States)\"); print(\"=\"*78)\n",
    "        # Determine if this is national analysis\n",
    "        is_national_analysis = state_filter is None or len(state_filter) == 0\n",
    "        \n",
    "        # Results will be organized by state\n",
    "        state_results = {}\n",
    "        all_states = set()\n",
    "        \n",
    "        # Collect data for all years first\n",
    "        for year in years_list:\n",
    "            print(f\"Processing {year}...\", end=\" \")\n",
    "            try:\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                \n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                if records.empty:\n",
    "                    print(\"No ATC records\")\n",
    "                    continue\n",
    "                \n",
    "                if state_filter:\n",
    "                    records = records[records['State'].isin(state_filter)]\n",
    "                    if records.empty:\n",
    "                        print(f\"No records for states {state_filter}\")\n",
    "                        continue\n",
    "                    states_to_process = state_filter\n",
    "                else:\n",
    "                    states_to_process = records['State'].unique()\n",
    "                    all_states.update(states_to_process)\n",
    "                \n",
    "                for state in states_to_process:\n",
    "                    state_records = records[records['State'] == state]\n",
    "                    if state_records.empty:\n",
    "                        continue\n",
    "                    \n",
    "                    # Initialize state data structure\n",
    "                    if state not in state_results:\n",
    "                        state_results[state] = {\n",
    "                            'atc2_year_results': {},\n",
    "                            'atc3_year_results': {}\n",
    "                        }\n",
    "                    \n",
    "                    # ATC2 summary for this state\n",
    "                    pairs2 = state_records[['record_id', 'NDC', 'ATC2 Class']].drop_duplicates()\n",
    "                    atc2_summary = pairs2.groupby('ATC2 Class').agg(\n",
    "                        Unique_NDCs=('NDC', 'nunique'),\n",
    "                        Total_Records=('record_id', 'nunique')\n",
    "                    ).sort_values('Unique_NDCs', ascending=False)\n",
    "                    atc2_summary['Percentage_of_NDCs'] = (\n",
    "                        atc2_summary['Unique_NDCs'] / pairs2['NDC'].nunique() * 100\n",
    "                    ).round(1)\n",
    "                    \n",
    "                    # ATC3 summary for this state\n",
    "                    pairs3 = state_records[['record_id', 'NDC', 'ATC3 Class']].drop_duplicates()\n",
    "                    atc3_summary = pairs3.groupby('ATC3 Class').agg(\n",
    "                        Unique_NDCs=('NDC', 'nunique'),\n",
    "                        Total_Records=('record_id', 'nunique')\n",
    "                    ).sort_values('Unique_NDCs', ascending=False)\n",
    "                    atc3_summary['Percentage_of_NDCs'] = (\n",
    "                        atc3_summary['Unique_NDCs'] / pairs3['NDC'].nunique() * 100\n",
    "                    ).round(1)\n",
    "                    \n",
    "                    state_results[state]['atc2_year_results'][year] = atc2_summary\n",
    "                    state_results[state]['atc3_year_results'][year] = atc3_summary\n",
    "                \n",
    "                processed_states = len([s for s in states_to_process if s in state_results])\n",
    "                print(f\"✓ (States: {processed_states})\")\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                print(f\"✗ File not found\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "        \n",
    "        # If national analysis, aggregate all states data\n",
    "        if is_national_analysis:\n",
    "    \n",
    "            print(\"=\"*78); print(\"AGGREGATING NATIONAL DATA FROM ALL STATES\"); print(\"=\"*78)\n",
    "       \n",
    "            # Create aggregated national results\n",
    "            national_atc2_year_results = {}\n",
    "            national_atc3_year_results = {}\n",
    "            \n",
    "            for year in years_list:\n",
    "                if year not in [y for state_data in state_results.values() for y in state_data['atc2_year_results'].keys()]:\n",
    "                    continue\n",
    "                \n",
    "                # Aggregate ATC2 data across all states for this year\n",
    "                atc2_combined = []\n",
    "                atc3_combined = []\n",
    "                \n",
    "                for state, state_data in state_results.items():\n",
    "                    if year in state_data['atc2_year_results']:\n",
    "                        df_temp = state_data['atc2_year_results'][year].reset_index()\n",
    "                        atc2_combined.append(df_temp)\n",
    "                    if year in state_data['atc3_year_results']:\n",
    "                        df_temp = state_data['atc3_year_results'][year].reset_index()\n",
    "                        atc3_combined.append(df_temp)\n",
    "                \n",
    "                if atc2_combined:\n",
    "                    atc2_combined_df = pd.concat(atc2_combined, ignore_index=True)\n",
    "                    national_atc2_summary = atc2_combined_df.groupby('ATC2 Class').agg(\n",
    "                        Unique_NDCs=('Unique_NDCs', 'sum'),\n",
    "                        Total_Records=('Total_Records', 'sum')\n",
    "                    ).sort_values('Unique_NDCs', ascending=False)\n",
    "                    total_ndcs = atc2_combined_df['Unique_NDCs'].sum()\n",
    "                    national_atc2_summary['Percentage_of_NDCs'] = (\n",
    "                        national_atc2_summary['Unique_NDCs'] / total_ndcs * 100\n",
    "                    ).round(1)\n",
    "                    national_atc2_year_results[year] = national_atc2_summary\n",
    "                \n",
    "                if atc3_combined:\n",
    "                    atc3_combined_df = pd.concat(atc3_combined, ignore_index=True)\n",
    "                    national_atc3_summary = atc3_combined_df.groupby('ATC3 Class').agg(\n",
    "                        Unique_NDCs=('Unique_NDCs', 'sum'),\n",
    "                        Total_Records=('Total_Records', 'sum')\n",
    "                    ).sort_values('Unique_NDCs', ascending=False)\n",
    "                    total_ndcs = atc3_combined_df['Unique_NDCs'].sum()\n",
    "                    national_atc3_summary['Percentage_of_NDCs'] = (\n",
    "                        national_atc3_summary['Unique_NDCs'] / total_ndcs * 100\n",
    "                    ).round(1)\n",
    "                    national_atc3_year_results[year] = national_atc3_summary\n",
    "            \n",
    "            # Use national aggregated data for processing\n",
    "            state_results['NATIONAL'] = {\n",
    "                'atc2_year_results': national_atc2_year_results,\n",
    "                'atc3_year_results': national_atc3_year_results\n",
    "            }\n",
    "            states_to_analyze = ['NATIONAL']\n",
    "        else:\n",
    "            states_to_analyze = state_filter\n",
    "        \n",
    "        # Process each state (only Indiana or National)\n",
    "        final_state_results = {}\n",
    "        \n",
    "        for state in states_to_analyze:\n",
    "            if state not in state_results:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n\" + \"=\"*60); print(f\"PROCESSING STATE: {state}\"); print(f\"\\n\" + \"=\"*60)\n",
    "            \n",
    "            atc2_year_results = state_results[state]['atc2_year_results']\n",
    "            atc3_year_results = state_results[state]['atc3_year_results']\n",
    "            \n",
    "            print(f\"\\nUNIQUE NDCs PER ATC2 CLASS BY YEAR - {state}\"); print(\"=\"*50)\n",
    "            for year in years_list:\n",
    "                if year in atc2_year_results and not atc2_year_results[year].empty:\n",
    "                    print(f\"\\n{year}: {len(atc2_year_results[year])} classes, \"\n",
    "                        f\"{atc2_year_results[year]['Unique_NDCs'].sum():,} total NDCs\")\n",
    "                    print(\"Top 10:\")\n",
    "                    print(atc2_year_results[year].head(10))\n",
    "            \n",
    "            print(f\"\\nUNIQUE NDCs PER ATC3 CLASS BY YEAR - {state}\");print(\"=\"*50)\n",
    "            for year in years_list:\n",
    "                if year in atc3_year_results and not atc3_year_results[year].empty:\n",
    "                    print(f\"\\n{year}: {len(atc3_year_results[year])} classes, \"\n",
    "                        f\"{atc3_year_results[year]['Unique_NDCs'].sum():,} total NDCs\")\n",
    "                    print(\"Top 10:\")\n",
    "                    print(atc3_year_results[year].head(10))\n",
    "            \n",
    "            # Build comparison tables (same logic as original)\n",
    "            def build_comparison(year_tables):\n",
    "                all_classes = set()\n",
    "                for tbl in year_tables.values():\n",
    "                    if not tbl.empty:\n",
    "                        all_classes.update(tbl.index.tolist())\n",
    "                comp = {cls: {y: int(year_tables[y].loc[cls, 'Unique_NDCs']) \n",
    "                            if y in year_tables and not year_tables[y].empty and cls in year_tables[y].index else 0\n",
    "                            for y in years_list}\n",
    "                        for cls in sorted(all_classes)}\n",
    "                df = pd.DataFrame(comp).T\n",
    "                return df.loc[df.sum(axis=1).sort_values(ascending=False).index]\n",
    "            \n",
    "            atc2_comparison = build_comparison(atc2_year_results)\n",
    "            atc3_comparison = build_comparison(atc3_year_results)\n",
    "            \n",
    "            # Create cumulative frequency tables (same logic as original)\n",
    "            def create_cumulative_frequency_table(comparison_df, level_name):\n",
    "                total_ndcs = comparison_df.sum(axis=1).sort_values(ascending=False)\n",
    "                \n",
    "                freq_table = pd.DataFrame({\n",
    "                    'ATC_Class': total_ndcs.index,\n",
    "                    'Total_Unique_NDCs': total_ndcs.values,\n",
    "                    'Percentage': (total_ndcs.values / total_ndcs.sum() * 100).round(2)\n",
    "                })\n",
    "                \n",
    "                freq_table['Cumulative_NDCs'] = freq_table['Total_Unique_NDCs'].cumsum()\n",
    "                freq_table['Cumulative_Percentage'] = freq_table['Percentage'].cumsum().round(2)\n",
    "                \n",
    "                freq_table.reset_index(drop=True, inplace=True)\n",
    "                freq_table.index = freq_table.index + 1\n",
    "                \n",
    "                return freq_table\n",
    "            \n",
    "            atc2_freq_table = create_cumulative_frequency_table(atc2_comparison, 'ATC2')\n",
    "            atc3_freq_table = create_cumulative_frequency_table(atc3_comparison, 'ATC3')\n",
    "            \n",
    "            # Store results for this state\n",
    "            final_state_results[state] = {\n",
    "                'atc2_year_results': atc2_year_results,\n",
    "                'atc3_year_results': atc3_year_results,\n",
    "                'atc2_comparison': atc2_comparison,\n",
    "                'atc3_comparison': atc3_comparison,\n",
    "                'atc2_freq_table': atc2_freq_table,\n",
    "                'atc3_freq_table': atc3_freq_table\n",
    "            }\n",
    "            \n",
    "            # Create line plots for percentage of NDCs over years\n",
    "            print(f\"\\n{'='*60}\"); print(f\"CREATING VISUALIZATIONS FOR {state}\"); print(f\"\\n{'='*60}\")\n",
    "            \n",
    "            # Helper function to create line plot\n",
    "            def create_percentage_line_plot(year_results, level_name, state_name, top_n=10):\n",
    "                \"\"\"Create line plot showing percentage of NDCs for top classes over years\"\"\"\n",
    "                \n",
    "                # Collect data for plotting\n",
    "                plot_data = {}\n",
    "                years_available = sorted([y for y in years_list if y in year_results and not year_results[y].empty])\n",
    "                \n",
    "                if not years_available:\n",
    "                    print(f\"No data available for {level_name} plot\")\n",
    "                    return\n",
    "                \n",
    "                # Get top classes based on total NDCs across all years\n",
    "                all_classes = set()\n",
    "                for year_df in year_results.values():\n",
    "                    if not year_df.empty:\n",
    "                        all_classes.update(year_df.index)\n",
    "                \n",
    "                # Calculate total NDCs for each class across all years\n",
    "                class_totals = {}\n",
    "                for cls in all_classes:\n",
    "                    total = 0\n",
    "                    for year in years_available:\n",
    "                        if cls in year_results[year].index:\n",
    "                            total += year_results[year].loc[cls, 'Unique_NDCs']\n",
    "                    class_totals[cls] = total\n",
    "                \n",
    "                # Get top N classes\n",
    "                top_classes = sorted(class_totals.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "                top_class_names = [cls for cls, _ in top_classes]\n",
    "                \n",
    "                # Build DataFrame for seaborn\n",
    "                plot_rows = []\n",
    "                for cls in top_class_names:\n",
    "                    for year in years_available:\n",
    "                        if cls in year_results[year].index:\n",
    "                            pct = year_results[year].loc[cls, 'Percentage_of_NDCs']\n",
    "                        else:\n",
    "                            pct = 0\n",
    "                        plot_rows.append({'Year': year, 'ATC Class': cls, 'Percentage': pct})\n",
    "                \n",
    "                plot_df = pd.DataFrame(plot_rows)\n",
    "                \n",
    "                # Create the plot using seaborn\n",
    "                sns.set_style(\"whitegrid\")\n",
    "                fig, ax = plt.subplots(figsize=(14, 8))\n",
    "                \n",
    "                if state_name == 'NATIONAL':\n",
    "                    palette = sns.color_palette(\"husl\", n_colors=top_n)\n",
    "                else:\n",
    "                    palette = sns.color_palette(\"Set2\", n_colors=top_n)\n",
    "                \n",
    "                sns.lineplot(data=plot_df, x='Year', y='Percentage', hue='ATC Class', \n",
    "                            marker='o', linewidth=2, markersize=6, ax=ax, \n",
    "                            palette=palette)\n",
    "                \n",
    "                ax.set_xlabel('Year', fontsize=13, fontweight='bold')\n",
    "                ax.set_ylabel('Percentage of Unique NDCs (%)', fontsize=13, fontweight='bold')\n",
    "                ax.set_title(f'{state_name} - Top {top_n} {level_name} Classes by Percentage of NDCs\\n(Across Years {years_available[0]}-{years_available[-1]})', \n",
    "                            fontsize=14, fontweight='bold', pad=20)\n",
    "                \n",
    "                ax.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), fontsize=9, \n",
    "                         frameon=True, fancybox=True, shadow=True, title='ATC Class')\n",
    "                \n",
    "                ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1f}%'))\n",
    "                \n",
    "                ax.set_xticks(years_available)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"✓ {level_name} line plot created\")\n",
    "            \n",
    "            # Create plots - limit to top 10 for both Indiana and National\n",
    "            create_percentage_line_plot(atc2_year_results, 'ATC2', state, top_n=10)\n",
    "            create_percentage_line_plot(atc3_year_results, 'ATC3', state, top_n=10)\n",
    "        \n",
    "        return final_state_results\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_atc_ndc_details(year, top_n=10, base_path=None):\n",
    "    \n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "            \n",
    "        print(f\"Analyzing ATC-NDC details for {year}...\");print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            # Load the pre-processed CSV file\n",
    "            csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "            df_merged = pd.read_csv(csv_path)\n",
    "            \n",
    "            records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "            if records.empty:\n",
    "                print(\"No records with ATC mapping\")\n",
    "                return pd.DataFrame(), pd.DataFrame()\n",
    "                \n",
    "            records['ATC2 Class'] = records['ATC4 Class'].str[:3]\n",
    "            records['ATC3 Class'] = records['ATC4 Class'].str[:4]\n",
    "            \n",
    "            # ATC2 details\n",
    "            atc2_details = records.groupby('ATC2 Class').agg(\n",
    "                Unique_NDCs=('NDC', 'nunique'),\n",
    "                Total_Records=('record_id', 'nunique')\n",
    "            ).sort_values('Unique_NDCs', ascending=False).head(top_n)\n",
    "            \n",
    "            # ATC3 details\n",
    "            atc3_details = records.groupby('ATC3 Class').agg(\n",
    "                Unique_NDCs=('NDC', 'nunique'),\n",
    "                Total_Records=('record_id', 'nunique')\n",
    "            ).sort_values('Unique_NDCs', ascending=False).head(top_n)\n",
    "            \n",
    "            print(f\"\\nTop {top_n} ATC2 Classes:\")\n",
    "            print(atc2_details)\n",
    "            print(f\"\\nTop {top_n} ATC3 Classes:\")\n",
    "            print(atc3_details)\n",
    "            \n",
    "            return atc2_details, atc3_details\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"✗ File not found: {csv_path}\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error: {e}\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    @staticmethod\n",
    "    def export_cumulative_frequency_excel(years_list, level='ATC2', base_path=None, output_filename=None, include_ndc_counts=True, by_state=False, state_filter=None):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        # Print header\n",
    "        if by_state:\n",
    "            states_msg = ', '.join(state_filter) if state_filter else 'All States'\n",
    "            print(f\"Creating {level} Cumulative Frequency Analysis Excel for States: {states_msg}\")\n",
    "        else:\n",
    "            print(f\"Creating {level} Cumulative Frequency Analysis Excel...\");print(\"=\"*70)\n",
    "        \n",
    "        \n",
    "        # Helper function to create ATC level column\n",
    "        def create_atc_level_column(df, level):\n",
    "            if level == 'ATC2':\n",
    "                return df['ATC4 Class'].str[:3]\n",
    "            elif level == 'ATC3':\n",
    "                return df['ATC4 Class'].str[:4]\n",
    "            else:  # ATC4\n",
    "                return df['ATC4 Class']\n",
    "        \n",
    "        # Helper function to get name mapping\n",
    "        def get_name_mapping(records, level):\n",
    "            name_col_map = {'ATC2': 'ATC2_Name', 'ATC3': 'ATC3_Name', 'ATC4': 'ATC4_Name'}\n",
    "            name_col = name_col_map.get(level)\n",
    "            \n",
    "            if name_col and name_col in records.columns:\n",
    "                return records[['ATC_Level', name_col]].drop_duplicates().set_index('ATC_Level')[name_col].to_dict()\n",
    "            return {}\n",
    "        \n",
    "        # Helper function to process year data\n",
    "        def process_year_data(csv_path, level, state_filter=None):\n",
    "            try:\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                \n",
    "                if records.empty:\n",
    "                    return None, None, None, None\n",
    "                \n",
    "                # Filter by state if needed\n",
    "                if state_filter:\n",
    "                    records = records[records['State'].isin(state_filter)]\n",
    "                    if records.empty:\n",
    "                        return None, None, None, None\n",
    "                \n",
    "                records['ATC_Level'] = create_atc_level_column(records, level)\n",
    "                name_mapping = get_name_mapping(records, level)\n",
    "                \n",
    "                # Aggregate financial data\n",
    "                financial = records.groupby('ATC_Level').agg(\n",
    "                    Units_Reimbursed=('Units Reimbursed', 'sum'),\n",
    "                    Number_of_Prescriptions=('Number of Prescriptions', 'sum')\n",
    "                )\n",
    "                \n",
    "                # Count unique NDCs\n",
    "                ndc_counts = records.groupby('ATC_Level').agg(\n",
    "                    Unique_NDCs=('NDC', 'nunique')\n",
    "                )\n",
    "                \n",
    "                states_processed = records['State'].nunique() if 'State' in records.columns else 1\n",
    "                \n",
    "                return financial, ndc_counts, name_mapping, states_processed\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file: {e}\")\n",
    "                return None, None, None, None\n",
    "        \n",
    "        # Collect data for all years\n",
    "        if by_state:\n",
    "            state_data = {}  # {state: {year: (financial, ndc_counts, name_mapping)}}\n",
    "            all_states = set()\n",
    "        else:\n",
    "            year_results = {}\n",
    "            ndc_counts = {}\n",
    "            name_mapping = {}\n",
    "        \n",
    "        # Process each year\n",
    "        for year in years_list:\n",
    "            print(f\"Processing {year}...\", end=\" \")\n",
    "            csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "            \n",
    "            if by_state:\n",
    "                # Load full data first\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    records = df[df['ATC4 Class'].notna()].copy()\n",
    "                    \n",
    "                    if state_filter:\n",
    "                        states_to_process = state_filter if isinstance(state_filter, list) else [state_filter]\n",
    "                    else:\n",
    "                        states_to_process = records['State'].unique()\n",
    "                        all_states.update(states_to_process)\n",
    "                    \n",
    "                    for state in states_to_process:\n",
    "                        financial, ndcs, names, _ = process_year_data(csv_path, level, [state])\n",
    "                        \n",
    "                        if financial is not None:\n",
    "                            if state not in state_data:\n",
    "                                state_data[state] = {}\n",
    "                            state_data[state][year] = (financial, ndcs, names)\n",
    "                    \n",
    "                    print(f\"✓ (States: {len(states_to_process)})\")\n",
    "                except:\n",
    "                    print(\"✗\")\n",
    "            else:\n",
    "                financial, ndcs, names, _ = process_year_data(csv_path, level)\n",
    "                \n",
    "                if financial is not None:\n",
    "                    year_results[year] = financial\n",
    "                    ndc_counts[year] = ndcs\n",
    "                    name_mapping.update(names)\n",
    "                    print(f\"✓ ({len(financial)} classes)\")\n",
    "                else:\n",
    "                    print(\"✗\")\n",
    "        \n",
    "        # Helper function to build comparison dataframes\n",
    "        def build_comparison_dfs(year_financial_dict, year_ndc_dict, years_list):\n",
    "            # Collect all unique ATC classes\n",
    "            all_classes = set()\n",
    "            for df in year_financial_dict.values():\n",
    "                if df is not None and not df.empty:\n",
    "                    all_classes.update(df.index)\n",
    "            for df in year_ndc_dict.values():\n",
    "                if df is not None and not df.empty:\n",
    "                    all_classes.update(df.index)\n",
    "            \n",
    "            if not all_classes:\n",
    "                return None, None, None\n",
    "            \n",
    "            # Build comparison dictionaries\n",
    "            units_comp = {}\n",
    "            presc_comp = {}\n",
    "            ndc_comp = {}\n",
    "            \n",
    "            for cls in sorted(all_classes):\n",
    "                units_comp[cls] = {\n",
    "                    y: float(year_financial_dict[y].loc[cls, 'Units_Reimbursed']) \n",
    "                    if y in year_financial_dict and year_financial_dict[y] is not None \n",
    "                    and not year_financial_dict[y].empty and cls in year_financial_dict[y].index \n",
    "                    else 0.0 \n",
    "                    for y in years_list\n",
    "                }\n",
    "                \n",
    "                presc_comp[cls] = {\n",
    "                    y: float(year_financial_dict[y].loc[cls, 'Number_of_Prescriptions']) \n",
    "                    if y in year_financial_dict and year_financial_dict[y] is not None \n",
    "                    and not year_financial_dict[y].empty and cls in year_financial_dict[y].index \n",
    "                    else 0.0 \n",
    "                    for y in years_list\n",
    "                }\n",
    "                \n",
    "                ndc_comp[cls] = {\n",
    "                    y: int(year_ndc_dict[y].loc[cls, 'Unique_NDCs']) \n",
    "                    if y in year_ndc_dict and year_ndc_dict[y] is not None \n",
    "                    and not year_ndc_dict[y].empty and cls in year_ndc_dict[y].index \n",
    "                    else 0 \n",
    "                    for y in years_list\n",
    "                }\n",
    "            \n",
    "            # Convert to DataFrames and sort by total units\n",
    "            units_df = pd.DataFrame(units_comp).T\n",
    "            presc_df = pd.DataFrame(presc_comp).T\n",
    "            ndc_df = pd.DataFrame(ndc_comp).T\n",
    "            \n",
    "            units_total = units_df.sum(axis=1).sort_values(ascending=False)\n",
    "            units_df = units_df.loc[units_total.index]\n",
    "            presc_df = presc_df.loc[units_total.index]\n",
    "            ndc_df = ndc_df.loc[units_total.index]\n",
    "            \n",
    "            return units_df, presc_df, ndc_df\n",
    "        \n",
    "        # Helper function to create cumulative frequency DataFrame\n",
    "        def create_cumulative_df(comparison_df, metric_name, name_mapping):\n",
    "            totals = comparison_df.sum(axis=1)\n",
    "            total_sum = totals.sum()\n",
    "            \n",
    "            cumulative_total = 0\n",
    "            df_data = []\n",
    "            \n",
    "            for atc_class in comparison_df.index:\n",
    "                class_total = totals[atc_class]\n",
    "                cumulative_total += class_total\n",
    "                percentage = (class_total / total_sum * 100) if total_sum > 0 else 0\n",
    "                cumulative_pct = (cumulative_total / total_sum * 100) if total_sum > 0 else 0\n",
    "                \n",
    "                row = {'ATC_Class': atc_class}\n",
    "                \n",
    "                # Add ATC name if available\n",
    "                row['ATC_Name'] = name_mapping.get(atc_class, '')\n",
    "                \n",
    "                # Add year-by-year data\n",
    "                for year in years_list:\n",
    "                    if metric_name == 'NDCs':\n",
    "                        row[f'{metric_name}_{year}'] = int(comparison_df.loc[atc_class, year])\n",
    "                    else:\n",
    "                        row[f'{metric_name}_{year}'] = round(comparison_df.loc[atc_class, year], 3)\n",
    "                \n",
    "                # Add summary columns\n",
    "                if metric_name == 'NDCs':\n",
    "                    row[f'Total_{metric_name}'] = int(class_total)\n",
    "                    row['Percentage'] = round(percentage, 2)\n",
    "                    row[f'Cumulative_{metric_name}'] = int(cumulative_total)\n",
    "                    row['Cumulative_Percentage_NDCs'] = round(cumulative_pct, 2)\n",
    "                else:\n",
    "                    row[f'Total_{metric_name}'] = round(class_total, 3)\n",
    "                    row['Percentage'] = round(percentage, 2)\n",
    "                    row[f'Cumulative_{metric_name}'] = round(cumulative_total, 3)\n",
    "                    row['Cumulative_Percentage'] = round(cumulative_pct, 2)\n",
    "                \n",
    "                df_data.append(row)\n",
    "            \n",
    "            return pd.DataFrame(df_data)\n",
    "        \n",
    "        # Export function\n",
    "        def export_to_excel(units_df, prescriptions_df, ndc_df, output_path, include_ndc_counts):\n",
    "            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "                units_df.to_excel(writer, sheet_name='Units_Reimbursed', index=False)\n",
    "                prescriptions_df.to_excel(writer, sheet_name='Prescriptions', index=False)\n",
    "                \n",
    "                if include_ndc_counts:\n",
    "                    ndc_df.to_excel(writer, sheet_name='NDC_Counts', index=False)\n",
    "            \n",
    "            print(f\"Exported to Excel: {output_path}\")\n",
    "        \n",
    "        # Process and export\n",
    "        output_dir = os.path.join(base_path, \"ATC\\\\exported_analysis\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        if by_state:\n",
    "            states_to_export = state_filter if state_filter else sorted(all_states)\n",
    "            all_output_paths = {}\n",
    "            \n",
    "            for state in states_to_export:\n",
    "                if state not in state_data:\n",
    "                    continue\n",
    "                \n",
    "                # Extract data for this state\n",
    "                state_year_financial = {y: data[0] for y, data in state_data[state].items()}\n",
    "                state_year_ndc = {y: data[1] for y, data in state_data[state].items()}\n",
    "                state_name_mapping = {}\n",
    "                for data in state_data[state].values():\n",
    "                    state_name_mapping.update(data[2])\n",
    "                \n",
    "                # Build comparison DataFrames\n",
    "                units_comp_df, presc_comp_df, ndc_comp_df = build_comparison_dfs(\n",
    "                    state_year_financial, state_year_ndc, years_list\n",
    "                )\n",
    "                \n",
    "                if units_comp_df is None:\n",
    "                    continue\n",
    "                \n",
    "                # Create cumulative DataFrames\n",
    "                units_cum = create_cumulative_df(units_comp_df, 'Units', state_name_mapping)\n",
    "                presc_cum = create_cumulative_df(presc_comp_df, 'Prescriptions', state_name_mapping)\n",
    "                ndc_cum = create_cumulative_df(ndc_comp_df, 'NDCs', state_name_mapping)\n",
    "                \n",
    "                # Generate output filename\n",
    "                if output_filename:\n",
    "                    name_parts = output_filename.rsplit('.', 1)\n",
    "                    state_output_filename = f\"{name_parts[0]}_{state}.{name_parts[1]}\"\n",
    "                else:\n",
    "                    state_output_filename = f\"{level}_Cumulative_Analysis_{state}_with_NDC_Counts.xlsx\"\n",
    "                \n",
    "                output_path = os.path.join(output_dir, state_output_filename)\n",
    "                export_to_excel(units_cum, presc_cum, ndc_cum, output_path, include_ndc_counts)\n",
    "                all_output_paths[state] = output_path\n",
    "            \n",
    "            return all_output_paths\n",
    "        \n",
    "        else:\n",
    "            # Build comparison DataFrames\n",
    "            units_comp_df, presc_comp_df, ndc_comp_df = build_comparison_dfs(\n",
    "                year_results, ndc_counts, years_list\n",
    "            )\n",
    "            \n",
    "            if units_comp_df is None:\n",
    "                print(\"No data found!\")\n",
    "                return None\n",
    "            \n",
    "            # Create cumulative DataFrames\n",
    "            units_cum = create_cumulative_df(units_comp_df, 'Units', name_mapping)\n",
    "            presc_cum = create_cumulative_df(presc_comp_df, 'Prescriptions', name_mapping)\n",
    "            ndc_cum = create_cumulative_df(ndc_comp_df, 'NDCs', name_mapping)\n",
    "            \n",
    "            # Generate output filename\n",
    "            if not output_filename:\n",
    "                output_filename = f\"{level}_Cumulative_Analysis_with_NDC_Counts.xlsx\"\n",
    "            \n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            export_to_excel(units_cum, presc_cum, ndc_cum, output_path, include_ndc_counts)\n",
    "            \n",
    "            return output_path\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_cumulative_80_analysis(base_path=None, cumulative_threshold=80.0):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(f\"COMPARATIVE ANALYSIS: Indiana vs National ATC2 Classes at {cumulative_threshold}% Threshold\");print(\"=\"*85)\n",
    "        \n",
    "        # File paths\n",
    "        indiana_file = os.path.join(base_path, \"ATC\\\\exported_analysis\\\\ATC2_Cumulative_Analysis_IN_with_NDC_Counts.xlsx\")\n",
    "        national_file = os.path.join(base_path, \"ATC\\\\exported_analysis\\\\ATC2_Cumulative_Analysis_with_NDC_Counts.xlsx\")\n",
    "        \n",
    "        # Load data\n",
    "        try:\n",
    "            indiana_units = pd.read_excel(indiana_file, sheet_name='Units_Reimbursed')\n",
    "            indiana_prescriptions = pd.read_excel(indiana_file, sheet_name='Prescriptions')\n",
    "            national_units = pd.read_excel(national_file, sheet_name='Units_Reimbursed')\n",
    "            national_prescriptions = pd.read_excel(national_file, sheet_name='Prescriptions')\n",
    "            \n",
    "            print(f\"✓ Loaded Indiana data: {len(indiana_units)} ATC2 classes\")\n",
    "            print(f\"✓ Loaded National data: {len(national_units)} ATC2 classes\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"✗ Error: Required Excel files not found. Please run export functions first.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading files: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Helper function to get classes at threshold\n",
    "        def get_classes_at_threshold(df, threshold):\n",
    "            df_sorted = df.sort_values('Cumulative_Percentage').reset_index(drop=True)\n",
    "            threshold_idx = df_sorted[df_sorted['Cumulative_Percentage'] >= threshold].index\n",
    "            \n",
    "            if len(threshold_idx) > 0:\n",
    "                return df_sorted.iloc[:threshold_idx[0] + 1]['ATC_Class'].tolist()\n",
    "            return df_sorted['ATC_Class'].tolist()\n",
    "        \n",
    "        # Get classes at threshold for each dataset\n",
    "        in_units_80 = get_classes_at_threshold(indiana_units, cumulative_threshold)\n",
    "        in_presc_80 = get_classes_at_threshold(indiana_prescriptions, cumulative_threshold)\n",
    "        nat_units_80 = get_classes_at_threshold(national_units, cumulative_threshold)\n",
    "        nat_presc_80 = get_classes_at_threshold(national_prescriptions, cumulative_threshold)\n",
    "        \n",
    "        # Calculate overlaps\n",
    "        in_overlap = set(in_units_80) & set(in_presc_80)\n",
    "        in_only_units = set(in_units_80) - set(in_presc_80)\n",
    "        in_only_presc = set(in_presc_80) - set(in_units_80)\n",
    "        \n",
    "        nat_overlap = set(nat_units_80) & set(nat_presc_80)\n",
    "        nat_only_units = set(nat_units_80) - set(nat_presc_80)\n",
    "        nat_only_presc = set(nat_presc_80) - set(nat_units_80)\n",
    "        \n",
    "        # Print Indiana Analysis\n",
    "        print(f\"\\n{'='*60}\"); print(\"1. INDIANA ANALYSIS\"); print(f\"\\nClasses reaching {cumulative_threshold}% cumulative:\"); print(f\"  Units Reimbursed: {len(in_units_80)} classes\"); print(f\"  Prescriptions: {len(in_presc_80)} classes\")\n",
    "        print(f\"\\nComparison:\"); print(f\"  Overlap (both metrics): {len(in_overlap)} classes\"); print(f\"  Only in Units: {len(in_only_units)} classes\"); print(f\"  Only in Prescriptions: {len(in_only_presc)} classes\")\n",
    "        \n",
    "        # Print National Analysis\n",
    "        print(f\"\\n{'='*60}\"); print(\"2. NATIONAL ANALYSIS\"); print(f\"\\n{'='*60}\"); print(f\"\\nClasses reaching {cumulative_threshold}% cumulative:\")\n",
    "        print(f\"  Units Reimbursed: {len(nat_units_80)} classes\"); print(f\"  Prescriptions: {len(nat_presc_80)} classes\")\n",
    "        print(f\"\\nComparison:\"); print(f\"  Overlap (both metrics): {len(nat_overlap)} classes\"); print(f\"  Only in Units: {len(nat_only_units)} classes\"); print(f\"  Only in Prescriptions: {len(nat_only_presc)} classes\")\n",
    "        \n",
    "        # Helper function for detailed comparison\n",
    "        def print_detailed_comparison(set1, set2, metric_name):\n",
    "            overlap = set1 & set2\n",
    "            only_in = set1 - set2\n",
    "            only_nat = set2 - set1\n",
    "            \n",
    "            print(\"-\" * 50); print(f\"\\n{metric_name.upper()} - Detailed Comparison:\")\n",
    "            print(f\"Classes in BOTH Indiana and National ({len(overlap)}):\")\n",
    "            print(f\"  {sorted(overlap) if overlap else 'None'}\")\n",
    "            print(f\"\\nClasses ONLY in Indiana ({len(only_in)}):\")\n",
    "            print(f\"  {sorted(only_in) if only_in else 'None'}\")\n",
    "            print(f\"\\nClasses ONLY in National ({len(only_nat)}):\")\n",
    "            print(f\"  {sorted(only_nat) if only_nat else 'None'}\")\n",
    "        \n",
    "        # Print detailed comparisons\n",
    "        print(f\"\\n{'='*85}\"); print(\"3. DETAILED CLASS ANALYSIS\"); print(f\"\\n{'='*85}\")\n",
    "        \n",
    "        print_detailed_comparison(set(in_units_80), set(nat_units_80), \"Units Reimbursed\")\n",
    "        print_detailed_comparison(set(in_presc_80), set(nat_presc_80), \"Prescriptions\")\n",
    "        \n",
    "        # Helper function to calculate category totals\n",
    "        def get_category_totals(df_units, df_presc, class_list):\n",
    "            if not class_list:\n",
    "                return 0.0, 0.0\n",
    "            units_total = df_units[df_units['ATC_Class'].isin(class_list)]['Total_Units'].sum()\n",
    "            presc_total = df_presc[df_presc['ATC_Class'].isin(class_list)]['Total_Prescriptions'].sum()\n",
    "            return units_total, presc_total\n",
    "        \n",
    "        # Calculate totals for all categories\n",
    "        in_units_only_u, in_units_only_p = get_category_totals(indiana_units, indiana_prescriptions, list(in_only_units))\n",
    "        in_presc_only_u, in_presc_only_p = get_category_totals(indiana_units, indiana_prescriptions, list(in_only_presc))\n",
    "        in_overlap_u, in_overlap_p = get_category_totals(indiana_units, indiana_prescriptions, list(in_overlap))\n",
    "        \n",
    "        nat_units_only_u, nat_units_only_p = get_category_totals(national_units, national_prescriptions, list(nat_only_units))\n",
    "        nat_presc_only_u, nat_presc_only_p = get_category_totals(national_units, national_prescriptions, list(nat_only_presc))\n",
    "        nat_overlap_u, nat_overlap_p = get_category_totals(national_units, national_prescriptions, list(nat_overlap))\n",
    "        \n",
    "        # Create summary DataFrame\n",
    "        totals_summary = pd.DataFrame({\n",
    "            'Geography': ['Indiana', 'Indiana', 'Indiana', 'National', 'National', 'National'],\n",
    "            'Category': ['Only in Units', 'Only in Prescriptions', 'In Both (Overlap)'] * 2,\n",
    "            'Num_Classes': [\n",
    "                len(in_only_units), len(in_only_presc), len(in_overlap),\n",
    "                len(nat_only_units), len(nat_only_presc), len(nat_overlap)\n",
    "            ],\n",
    "            'Total_Units': [\n",
    "                in_units_only_u, in_presc_only_u, in_overlap_u,\n",
    "                nat_units_only_u, nat_presc_only_u, nat_overlap_u\n",
    "            ],\n",
    "            'Total_Prescriptions': [\n",
    "                in_units_only_p, in_presc_only_p, in_overlap_p,\n",
    "                nat_units_only_p, nat_presc_only_p, nat_overlap_p\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{'='*85}\")\n",
    "        print(\"4. SUMMARY TOTALS\")\n",
    "        print(f\"{'='*85}\")\n",
    "        print(totals_summary.to_string(index=False))\n",
    "        \n",
    "        # Return comprehensive results\n",
    "        return {\n",
    "            'totals_summary': totals_summary,\n",
    "            'indiana': {\n",
    "                'units_80': in_units_80,\n",
    "                'prescriptions_80': in_presc_80,\n",
    "                'overlap': list(in_overlap),\n",
    "                'only_units': list(in_only_units),\n",
    "                'only_prescriptions': list(in_only_presc)\n",
    "            },\n",
    "            'national': {\n",
    "                'units_80': nat_units_80,\n",
    "                'prescriptions_80': nat_presc_80,\n",
    "                'overlap': list(nat_overlap),\n",
    "                'only_units': list(nat_only_units),\n",
    "                'only_prescriptions': list(nat_only_presc)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_pareto_charts(base_path=None, top_n=30):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(\"CREATING PARETO CHARTS: Indiana vs National\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # File paths\n",
    "        indiana_file = os.path.join(base_path, \"ATC\\\\exported_analysis\\\\ATC2_Cumulative_Analysis_IN_with_NDC_Counts.xlsx\")\n",
    "        national_file = os.path.join(base_path, \"ATC\\\\exported_analysis\\\\ATC2_Cumulative_Analysis_with_NDC_Counts.xlsx\")\n",
    "        \n",
    "        # Load data\n",
    "        try:\n",
    "            indiana_units = pd.read_excel(indiana_file, sheet_name='Units_Reimbursed')\n",
    "            indiana_prescriptions = pd.read_excel(indiana_file, sheet_name='Prescriptions')\n",
    "            national_units = pd.read_excel(national_file, sheet_name='Units_Reimbursed')\n",
    "            national_prescriptions = pd.read_excel(national_file, sheet_name='Prescriptions')\n",
    "            print(\"✓ Data loaded successfully\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"✗ Error: Required Excel files not found. Please run export functions first.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading files: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Create figure with 2x2 subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(24, 16))\n",
    "        \n",
    "        def create_single_pareto(ax, df, metric_name, geography, top_n):\n",
    "            \"\"\"Create a single Pareto chart\"\"\"\n",
    "            df_top = df.head(top_n).copy()\n",
    "            \n",
    "            # Prepare data\n",
    "            classes = df_top['ATC_Class']\n",
    "            if metric_name == 'Units':\n",
    "                values = df_top['Total_Units']\n",
    "                y_label = 'Total Units Reimbursed (Billions)'\n",
    "            else:\n",
    "                values = df_top['Total_Prescriptions']\n",
    "                y_label = 'Total Prescriptions (Millions)'\n",
    "            \n",
    "            cumulative_pct = df_top['Cumulative_Percentage']\n",
    "            \n",
    "            # Create bar chart\n",
    "            bars = ax.bar(range(len(classes)), values, alpha=0.7, color='steelblue')\n",
    "            ax.set_xlabel('ATC2 Classes', fontsize=12)\n",
    "            ax.set_ylabel(y_label, fontsize=12, color='steelblue')\n",
    "            ax.tick_params(axis='y', labelcolor='steelblue')\n",
    "            ax.set_xticks(range(len(classes)))\n",
    "            ax.set_xticklabels(classes, rotation=90, ha='center', fontsize=8)\n",
    "            \n",
    "            # Add value labels on top 3 bars\n",
    "            for i in range(min(3, len(bars))):\n",
    "                height = bars[i].get_height()\n",
    "                ax.text(bars[i].get_x() + bars[i].get_width()/2., height,\n",
    "                       f'{height:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "            \n",
    "            # Create cumulative percentage line\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.plot(range(len(classes)), cumulative_pct, color='red', marker='o', linewidth=2, markersize=3)\n",
    "            ax2.set_ylabel('Cumulative Percentage (%)', fontsize=12, color='red')\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            ax2.set_ylim(0, 100)\n",
    "            \n",
    "            # Add 80% threshold line\n",
    "            # Add 80% threshold line\n",
    "            ax2.axhline(y=80, color='red', linestyle='--', alpha=0.7, linewidth=1)\n",
    "            ax2.text(len(classes)*0.7, 82, '80%', color='red', fontweight='bold')\n",
    "            \n",
    "            # Title\n",
    "            ax.set_title(f'{geography} - {metric_name}\\n(Top {top_n} ATC2 Classes)', \n",
    "                        fontsize=12, fontweight='bold', pad=15)\n",
    "        \n",
    "        # Create all four Pareto charts\n",
    "        create_single_pareto(axes[0,0], indiana_units, 'Units', 'INDIANA', top_n)\n",
    "        create_single_pareto(axes[0,1], national_units, 'Units', 'NATIONAL', top_n)\n",
    "        create_single_pareto(axes[1,0], indiana_prescriptions, 'Prescriptions', 'INDIANA', top_n)\n",
    "        create_single_pareto(axes[1,1], national_prescriptions, 'Prescriptions', 'NATIONAL', top_n)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Helper function to print Pareto summary\n",
    "        def print_pareto_summary(df, metric_name, geography):\n",
    "            df_sorted = df.sort_values('Cumulative_Percentage').reset_index(drop=True)\n",
    "            threshold_idx = df_sorted[df_sorted['Cumulative_Percentage'] >= 80].index\n",
    "            \n",
    "            if len(threshold_idx) > 0:\n",
    "                classes_80 = df_sorted.iloc[:threshold_idx[0] + 1]\n",
    "            else:\n",
    "                classes_80 = df_sorted\n",
    "            \n",
    "            total_classes = len(df)\n",
    "            classes_for_80 = len(classes_80)\n",
    "            pct_classes_for_80 = (classes_for_80 / total_classes) * 100\n",
    "            \n",
    "            if metric_name == 'Units':\n",
    "                total_value = df['Total_Units'].sum()\n",
    "                value_80 = classes_80['Total_Units'].sum()\n",
    "            else:\n",
    "                total_value = df['Total_Prescriptions'].sum()\n",
    "                value_80 = classes_80['Total_Prescriptions'].sum()\n",
    "            \n",
    "            actual_pct_covered = (value_80 / total_value) * 100\n",
    "            \n",
    "            print(f\"\\n{geography} - {metric_name}:\")\n",
    "            print(f\"  Total ATC2 classes: {total_classes}\")\n",
    "            print(f\"  Classes needed for ~80%: {classes_for_80} ({pct_classes_for_80:.1f}% of classes)\")\n",
    "            print(f\"  Actual coverage: {actual_pct_covered:.1f}%\")\n",
    "            print(f\"  Total {metric_name.lower()}: {total_value:.3f}\")\n",
    "            print(f\"  Value in top classes: {value_80:.3f}\")\n",
    "            print(f\"  Top 5 classes:\")\n",
    "            for i, (_, row) in enumerate(df.head(5).iterrows(), 1):\n",
    "                value = row['Total_Units'] if metric_name == 'Units' else row['Total_Prescriptions']\n",
    "                print(f\"    {i}. {row['ATC_Class']}: {value:.3f} ({row['Percentage']:.1f}%)\")\n",
    "        \n",
    "        # Print summaries\n",
    "        print(f\"\\n{'='*80}\"); print(\"PARETO ANALYSIS SUMMARY\"); print(f\"\\n{'='*80}\")\n",
    "        \n",
    "        print_pareto_summary(indiana_units, 'Units', 'INDIANA')\n",
    "        print_pareto_summary(national_units, 'Units', 'NATIONAL')\n",
    "        print_pareto_summary(indiana_prescriptions, 'Prescriptions', 'INDIANA')\n",
    "        print_pareto_summary(national_prescriptions, 'Prescriptions', 'NATIONAL')\n",
    "        \n",
    "        # Compare top classes\n",
    "        print(f\"\\n{'='*80}\"); print(\"KEY INSIGHTS - Top 10 Classes Comparison\"); print(f\"\\n{'='*80}\")\n",
    "        \n",
    "        in_top_units = set(indiana_units.head(10)['ATC_Class'])\n",
    "        nat_top_units = set(national_units.head(10)['ATC_Class'])\n",
    "        in_top_presc = set(indiana_prescriptions.head(10)['ATC_Class'])\n",
    "        nat_top_presc = set(national_prescriptions.head(10)['ATC_Class'])\n",
    "        \n",
    "        units_overlap = in_top_units & nat_top_units\n",
    "        presc_overlap = in_top_presc & nat_top_presc\n",
    "        \n",
    "        print(f\"\\nUnits Reimbursed - Common classes: {len(units_overlap)}/10\")\n",
    "        print(f\"  Shared: {sorted(units_overlap)}\")\n",
    "        print(f\"  Only Indiana: {sorted(in_top_units - nat_top_units)}\")\n",
    "        print(f\"  Only National: {sorted(nat_top_units - in_top_units)}\")\n",
    "        \n",
    "        print(f\"\\nPrescriptions - Common classes: {len(presc_overlap)}/10\")\n",
    "        print(f\"  Shared: {sorted(presc_overlap)}\")\n",
    "        print(f\"  Only Indiana: {sorted(in_top_presc - nat_top_presc)}\")\n",
    "        print(f\"  Only National: {sorted(nat_top_presc - in_top_presc)}\")\n",
    "        \n",
    "        return {\n",
    "            'indiana_units': indiana_units,\n",
    "            'indiana_prescriptions': indiana_prescriptions,\n",
    "            'national_units': national_units,\n",
    "            'national_prescriptions': national_prescriptions\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_top_states_by_atc_class(years_list, atc_level='ATC2', top_n_states=10, top_n_classes=15, base_path=None):\n",
    "        \n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(f\"ANALYZING TOP {top_n_states} STATES BY {atc_level} CLASS\")\n",
    "        print(f\"Years: {years_list}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Helper function to create ATC level column\n",
    "        def create_atc_level_column(df, level):\n",
    "            if level == 'ATC2':\n",
    "                return df['ATC4 Class'].str[:3]\n",
    "            elif level == 'ATC3':\n",
    "                return df['ATC4 Class'].str[:4]\n",
    "            else:  # ATC4\n",
    "                return df['ATC4 Class']\n",
    "        \n",
    "        # Collect all data across years\n",
    "        all_data = []\n",
    "        \n",
    "        for year in years_list:\n",
    "            print(f\"Processing {year}...\", end=\" \")\n",
    "            try:\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                \n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                if records.empty:\n",
    "                    print(\"No ATC records\")\n",
    "                    continue\n",
    "                \n",
    "                records['ATC_Level'] = create_atc_level_column(records, atc_level)\n",
    "                records['Year'] = year\n",
    "                all_data.append(records)\n",
    "                \n",
    "                print(f\"✓ ({len(records):,} records)\")\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                print(\"✗ File not found\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"No data found!\")\n",
    "            return None\n",
    "        \n",
    "        # Combine all years\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"\\nTotal records across all years: {len(combined_df):,}\")\n",
    "        \n",
    "        # Get top ATC classes nationally (by total units)\n",
    "        print(f\"\\nFinding top {top_n_classes} {atc_level} classes nationally...\")\n",
    "        national_summary = combined_df.groupby('ATC_Level').agg(\n",
    "            Total_Units=('Units Reimbursed', 'sum'),\n",
    "            Total_Prescriptions=('Number of Prescriptions', 'sum'),\n",
    "            Unique_NDCs=('NDC', 'nunique'),\n",
    "            Total_Records=('record_id', 'nunique')\n",
    "        ).sort_values('Total_Units', ascending=False)\n",
    "        \n",
    "        top_classes = national_summary.head(top_n_classes).index.tolist()\n",
    "        \n",
    "        print(f\"Top {len(top_classes)} {atc_level} classes by total units:\")\n",
    "        for i, cls in enumerate(top_classes[:10], 1):\n",
    "            units = national_summary.loc[cls, 'Total_Units']\n",
    "            presc = national_summary.loc[cls, 'Total_Prescriptions']\n",
    "            print(f\"  {i:2d}. {cls}: {units:.2f}B units, {presc:.2f}M prescriptions\")\n",
    "        \n",
    "        if len(top_classes) > 10:\n",
    "            print(f\"  ... and {len(top_classes) - 10} more classes\")\n",
    "        \n",
    "        # Analyze each top class by state\n",
    "        class_results = {}\n",
    "        \n",
    "        print(f\"\\n{'='*80}\"); print(f\"ANALYZING TOP STATES FOR EACH {atc_level} CLASS\"); print(f\"\\n{'='*80}\")\n",
    "        \n",
    "        for i, atc_class in enumerate(top_classes, 1):\n",
    "            print(f\"\\n{i:2d}. Analyzing {atc_class}...\")\n",
    "            \n",
    "            class_data = combined_df[combined_df['ATC_Level'] == atc_class].copy()\n",
    "            \n",
    "            if class_data.empty:\n",
    "                continue\n",
    "            \n",
    "            # Aggregate by state\n",
    "            state_summary = class_data.groupby('State').agg(\n",
    "                Total_Units=('Units Reimbursed', 'sum'),\n",
    "                Total_Prescriptions=('Number of Prescriptions', 'sum'),\n",
    "                Unique_NDCs=('NDC', 'nunique'),\n",
    "                Total_Records=('record_id', 'nunique')\n",
    "            )\n",
    "            \n",
    "            # Calculate percentages\n",
    "            total_class_units = state_summary['Total_Units'].sum()\n",
    "            total_class_presc = state_summary['Total_Prescriptions'].sum()\n",
    "            \n",
    "            state_summary['Units_Percentage'] = (\n",
    "                state_summary['Total_Units'] / total_class_units * 100\n",
    "            ).round(2)\n",
    "            \n",
    "            state_summary['Prescriptions_Percentage'] = (\n",
    "                state_summary['Total_Prescriptions'] / total_class_presc * 100\n",
    "            ).round(2)\n",
    "            \n",
    "            # Sort by units and get top states\n",
    "            top_states_units = state_summary.sort_values('Total_Units', ascending=False).head(top_n_states)\n",
    "            top_states_presc = state_summary.sort_values('Total_Prescriptions', ascending=False).head(top_n_states)\n",
    "            \n",
    "            # Store results\n",
    "            class_results[atc_class] = {\n",
    "                'total_units': total_class_units,\n",
    "                'total_prescriptions': total_class_presc,\n",
    "                'top_states_units': top_states_units,\n",
    "                'top_states_prescriptions': top_states_presc,\n",
    "                'all_states': state_summary\n",
    "            }\n",
    "            \n",
    "            # Print summary for this class\n",
    "            print(f\"   Total: {total_class_units:.3f}B units, {total_class_presc:.3f}M prescriptions\")\n",
    "            print(f\"   States with data: {len(state_summary)}\")\n",
    "            \n",
    "            # Print top 5 states by units\n",
    "            print(f\"   Top 5 by Units:\")\n",
    "            for j, (state, row) in enumerate(top_states_units.head(5).iterrows(), 1):\n",
    "                print(f\"     {j}. {state}: {row['Total_Units']:.3f}B ({row['Units_Percentage']:.1f}%)\")\n",
    "            \n",
    "            # Print top 5 states by prescriptions\n",
    "            print(f\"   Top 5 by Prescriptions:\")\n",
    "            for j, (state, row) in enumerate(top_states_presc.head(5).iterrows(), 1):\n",
    "                print(f\"     {j}. {state}: {row['Total_Prescriptions']:.3f}M ({row['Prescriptions_Percentage']:.1f}%)\")\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        print(f\"\\n{'='*80}\"); print(\"CREATING SUMMARY VISUALIZATIONS\"); print(f\"\\n{'='*80}\")\n",
    "        \n",
    "        # 1. Heatmap of top classes vs top states\n",
    "        print(\"Creating heatmap of Units by State and ATC Class...\")\n",
    "        \n",
    "        # Get top 10 states overall\n",
    "        state_totals = combined_df.groupby('State')['Units Reimbursed'].sum().sort_values(ascending=False)\n",
    "        top_states_overall = state_totals.head(10).index.tolist()\n",
    "        \n",
    "        # Create matrix for heatmap\n",
    "        heatmap_data = []\n",
    "        heatmap_index = []\n",
    "        heatmap_columns = top_classes[:10]  # Top 10 classes\n",
    "        \n",
    "        for state in top_states_overall:\n",
    "            row_data = []\n",
    "            for atc_class in heatmap_columns:\n",
    "                if atc_class in class_results and state in class_results[atc_class]['all_states'].index:\n",
    "                    value = class_results[atc_class]['all_states'].loc[state, 'Total_Units']\n",
    "                else:\n",
    "                    value = 0\n",
    "                row_data.append(value)\n",
    "            heatmap_data.append(row_data)\n",
    "            heatmap_index.append(state)\n",
    "        \n",
    "        heatmap_df = pd.DataFrame(heatmap_data, index=heatmap_index, columns=heatmap_columns)\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        sns.heatmap(heatmap_df, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "                    cbar_kws={'label': 'Units Reimbursed (Billions)'})\n",
    "        plt.title(f'Units Reimbursed by State and {atc_level} Class\\n(Top 10 States vs Top 10 Classes)', \n",
    "                  fontweight='bold', pad=20)\n",
    "        plt.xlabel(f'{atc_level} Classes', fontweight='bold')\n",
    "        plt.ylabel('States', fontweight='bold')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # 2. Bar chart comparing IN vs top 3 states for top 5 classes\n",
    "        if 'IN' in state_totals.index:\n",
    "            print(f\"\\nCreating comparison: Indiana vs Top 3 States for Top 5 {atc_level} Classes...\")\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 1, figsize=(15, 12))\n",
    "            \n",
    "            # Prepare data for comparison\n",
    "            comparison_data_units = []\n",
    "            comparison_data_presc = []\n",
    "            \n",
    "            for atc_class in top_classes[:5]:\n",
    "                if atc_class not in class_results:\n",
    "                    continue\n",
    "                    \n",
    "                state_data_units = class_results[atc_class]['all_states']['Total_Units']\n",
    "                state_data_presc = class_results[atc_class]['all_states']['Total_Prescriptions']\n",
    "                \n",
    "                # Get top 3 states (excluding IN if it's in top 3)\n",
    "                top_3_states = state_data_units.drop('IN', errors='ignore').head(3)\n",
    "                in_value_units = state_data_units.get('IN', 0)\n",
    "                \n",
    "                top_3_states_presc = state_data_presc.drop('IN', errors='ignore').head(3)\n",
    "                in_value_presc = state_data_presc.get('IN', 0)\n",
    "                \n",
    "                # Add to comparison data\n",
    "                for state, value in top_3_states.items():\n",
    "                    comparison_data_units.append({\n",
    "                        'ATC_Class': atc_class,\n",
    "                        'State': state,\n",
    "                        'Value': value,\n",
    "                        'Type': 'Top 3'\n",
    "                    })\n",
    "                \n",
    "                for state, value in top_3_states_presc.items():\n",
    "                    comparison_data_presc.append({\n",
    "                        'ATC_Class': atc_class,\n",
    "                        'State': state,\n",
    "                        'Value': value,\n",
    "                        'Type': 'Top 3'\n",
    "                    })\n",
    "                \n",
    "                # Add Indiana data\n",
    "                comparison_data_units.append({\n",
    "                    'ATC_Class': atc_class,\n",
    "                    'State': 'IN',\n",
    "                    'Value': in_value_units,\n",
    "                    'Type': 'Indiana'\n",
    "                })\n",
    "                \n",
    "                comparison_data_presc.append({\n",
    "                    'ATC_Class': atc_class,\n",
    "                    'State': 'IN',\n",
    "                    'Value': in_value_presc,\n",
    "                    'Type': 'Indiana'\n",
    "                })\n",
    "            \n",
    "            # Create comparison plots\n",
    "            comp_df_units = pd.DataFrame(comparison_data_units)\n",
    "            comp_df_presc = pd.DataFrame(comparison_data_presc)\n",
    "            \n",
    "            # Units plot\n",
    "            sns.barplot(data=comp_df_units, x='ATC_Class', y='Value', hue='State', ax=axes[0])\n",
    "            axes[0].set_title('Units Reimbursed: Indiana vs Top 3 States by ATC Class', fontweight='bold')\n",
    "            axes[0].set_ylabel('Units Reimbursed (Billions)', fontweight='bold')\n",
    "            axes[0].set_xlabel('')\n",
    "            axes[0].legend(title='State', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            \n",
    "            # Prescriptions plot\n",
    "            sns.barplot(data=comp_df_presc, x='ATC_Class', y='Value', hue='State', ax=axes[1])\n",
    "            axes[1].set_title('Prescriptions: Indiana vs Top 3 States by ATC Class', fontweight='bold')\n",
    "            axes[1].set_ylabel('Prescriptions (Millions)', fontweight='bold')\n",
    "            axes[1].set_xlabel(f'{atc_level} Classes', fontweight='bold')\n",
    "            axes[1].legend(title='State', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            \n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\n{'='*80}\"); print(\"SUMMARY STATISTICS\"); print(f\"\\n{'='*80}\")     \n",
    "        print(f\"\\nOverall Statistics:\"); print(f\"  Total {atc_level} classes analyzed: {len(class_results)}\"); print(f\"  Total states in data: {combined_df['State'].nunique()}\")\n",
    "        print(f\"  Total records: {len(combined_df):,}\"); print(f\"  Total units: {combined_df['Units Reimbursed'].sum():.2f}B\"); print(f\"  Total prescriptions: {combined_df['Number of Prescriptions'].sum():.2f}M\")\n",
    "        \n",
    "        # State concentration analysis\n",
    "        print(f\"\\nState Concentration Analysis (Top 5 vs Bottom 5 states):\")\n",
    "        \n",
    "        state_totals_units = combined_df.groupby('State')['Units Reimbursed'].sum().sort_values(ascending=False)\n",
    "        state_totals_presc = combined_df.groupby('State')['Number of Prescriptions'].sum().sort_values(ascending=False)\n",
    "        \n",
    "        top_5_states_pct_units = (state_totals_units.head(5).sum() / state_totals_units.sum() * 100)\n",
    "        top_5_states_pct_presc = (state_totals_presc.head(5).sum() / state_totals_presc.sum() * 100)\n",
    "        \n",
    "        print(f\"  Top 5 states account for {top_5_states_pct_units:.1f}% of total units\")\n",
    "        print(f\"  Top 5 states account for {top_5_states_pct_presc:.1f}% of total prescriptions\")\n",
    "        \n",
    "        top_5_states = state_totals_units.head(5)\n",
    "        print(f\"  Top 5 states: {', '.join(top_5_states.index)}\")\n",
    "        \n",
    "        return {\n",
    "            'class_results': class_results,\n",
    "            'national_summary': national_summary,\n",
    "            'top_classes': top_classes,\n",
    "            'state_totals_units': state_totals_units,\n",
    "            'state_totals_prescriptions': state_totals_presc,\n",
    "            'heatmap_data': heatmap_df,\n",
    "            'combined_data': combined_df\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4601386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the method analyze_general_atc_overview_by_state for Indiana\n",
    "print(\"Analyzing Indiana State...\")\n",
    "indiana_results = NDCATC_overview.analyze_general_atc_overview_by_state(years_list, state_filter=['IN'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Call the method analyze_general_atc_overview_by_state for National (all states)\n",
    "print(\"Analyzing National (All States)...\")\n",
    "national_results = NDCATC_overview.analyze_general_atc_overview_by_state(years_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just checking overlap between files with and without key\n",
    "nokey_path=rf'C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\ATC4_classes\\Classes_notgood\\NDCf_2020_ATC4_classes.csv'\n",
    "keyed_path=rf'C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\ATC4_classes\\NDCNEW_2020_ATC4_classes.csv'\n",
    "# Load them\n",
    "keyed = pd.read_csv(keyed_path, dtype=str)\n",
    "nokey = pd.read_csv(nokey_path, dtype=str)\n",
    "\n",
    "# Normalize NDCs (remove hyphens, pad to 11 digits)\n",
    "for df in [keyed, nokey]:\n",
    "    df[\"NDC\"] = df[\"NDC\"].str.replace(\"-\", \"\", regex=False).str.zfill(11)\n",
    "\n",
    "# --- Summary stats ---\n",
    "summary = {\n",
    "    \"File\": [\"With key (NDCNEW_2024_ATC4_classes)\", \"Without key (NDCf_2024_ATC4_classes)\"],\n",
    "    \"Total rows\": [len(keyed), len(nokey)],\n",
    "    \"Unique NDCs\": [keyed[\"NDC\"].nunique(), nokey[\"NDC\"].nunique()],\n",
    "    \"Mapped NDCs (non-null ATC)\": [\n",
    "        keyed[\"ATC4 Class\"].notna().sum(),\n",
    "        nokey[\"ATC4 Class\"].notna().sum(),\n",
    "    ],\n",
    "}\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# --- Compare overlap of unique NDCs ---\n",
    "ndc_keyed = set(keyed[\"NDC\"].unique())\n",
    "ndc_nokey = set(nokey[\"NDC\"].unique())\n",
    "\n",
    "overlap_ndcs = len(ndc_keyed & ndc_nokey)\n",
    "only_in_nokey = len(ndc_nokey - ndc_keyed)\n",
    "only_in_keyed = len(ndc_keyed - ndc_nokey)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Metric\": [\"Overlap NDCs\", \"Only in without-key file\", \"Only in with-key file\", \"Percent overlap\"],\n",
    "    \"Value\": [overlap_ndcs, only_in_nokey, only_in_keyed, overlap_ndcs / len(ndc_nokey) * 100]\n",
    "})\n",
    "\n",
    "print(\"\\n=== Summary of Each File ===\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== NDC Overlap Comparison ===\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndc_pareto_results = NDCATC_overview.create_pareto_charts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c36c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Generating only one excel file for all the states\n",
    "#output_path = NDCATC_overview.export_cumulative_frequency_excel(years_list, by_state=False)  # This combines all states into one analysis\n",
    "#NDCATC_overview.export_cumulative_frequency_excel(years_list,  level='ATC2','ATC4'by_state=True,state_filter=['IN'])\n",
    "\n",
    "#print(\"\\nComparing Indiana vs National...\")\n",
    "#ind_vs_national = NDCATC_overview.compare_cumulative_80_analysis()\n",
    "\n",
    "#ndc_pareto_results = NDCATC_overview.create_pareto_charts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
