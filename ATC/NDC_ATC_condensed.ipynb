{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2b3e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import shelve\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import statsmodels \n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.sandbox.stats.runs import runstest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f7f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "user=\"Lilian\"\n",
    "years_list = [2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
    "#user in personal pc1 <- \"asus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73aebaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDCATCProcessor:\n",
    "\n",
    "    def __init__(self, year, base_path=None):\n",
    "        self.year = year\n",
    "        self.base_path = base_path or rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        self.df_cleaned = None\n",
    "        self.df_merged = None\n",
    "        self.atc_mapping = None\n",
    "        \n",
    "    def clean_sdud_data(self):\n",
    "\n",
    "        csv_file = os.path.join(self.base_path, f\"SDUD\\\\SDUD{self.year}.csv\")\n",
    "        print(f\"Reading CSV: {csv_file}\")\n",
    "        \n",
    "        df = pd.read_csv(csv_file, dtype={'NDC': 'object'})\n",
    "        print(f\"Initial rows: {len(df):,}\")\n",
    "        \n",
    "        # Filter data\n",
    "        df_filtered = df.dropna(subset=['Units Reimbursed', 'Number of Prescriptions']) #dropping na values\n",
    "        df_filtered = df_filtered[df_filtered['State'] != 'XX']\n",
    "        \n",
    "        print(f\"After cleaning: {len(df_filtered):,} rows, {df_filtered['NDC'].nunique():,} unique NDCs\")\n",
    "        \n",
    "        self.df_cleaned = df_filtered\n",
    "        return self.df_cleaned\n",
    "    \n",
    "    def adding_key(self):\n",
    "        if self.df_cleaned is None:\n",
    "            raise ValueError(\"Run clean_sdud_data() first\")\n",
    "        \n",
    "        self.df_cleaned['record_id'] = (\n",
    "            self.df_cleaned['State'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['Year'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['Quarter'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['Utilization Type'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['NDC'].astype(str)\n",
    "        )\n",
    "        \n",
    "        print(f\"Created {len(self.df_cleaned):,} record IDs\")\n",
    "        return self.df_cleaned\n",
    "    \n",
    "    def generate_ndc_txt(self, output_filename=None):\n",
    "        if 'record_id' not in self.df_cleaned.columns:\n",
    "            raise ValueError(\"Run adding_key() first\")\n",
    "            \n",
    "        output_filename = output_filename or f\"NDCNEW_{self.year}.txt\"\n",
    "        output_path = os.path.join(self.base_path, f\"ATC\\\\text_files\\\\{output_filename}\")\n",
    "        \n",
    "        unique_pairs = self.df_cleaned[['NDC', 'record_id']].drop_duplicates()\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(\"NDC\\trecord_id\\n\")\n",
    "            for _, row in unique_pairs.iterrows():\n",
    "                f.write(f\"{row['NDC']}\\t{row['record_id']}\\n\")\n",
    "        \n",
    "        print(f\"Exported {unique_pairs['record_id'].nunique():,} unique records to {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def analyze_atc4_mapping(self):\n",
    "\n",
    "        if 'record_id' not in self.df_cleaned.columns:\n",
    "            raise ValueError(\"Run adding_key() first\")\n",
    "            \n",
    "        atc4_path = os.path.join(self.base_path, f\"ATC\\\\ATC4_classes\\\\NDCNEW_{self.year}_ATC4_classes.csv\")\n",
    "        \n",
    "        # Load ATC4 mapping\n",
    "        df_atc4 = pd.read_csv(atc4_path, dtype={'NDC': 'object', 'record_id': 'string'})\n",
    "        df_atc4['NDC'] = df_atc4['NDC'].str.zfill(11)\n",
    "        \n",
    "        print(f\"ATC4 file: {len(df_atc4):,} rows, {df_atc4['NDC'].nunique():,} unique NDCs\")\n",
    "        \n",
    "        # Ensure consistent types\n",
    "        self.df_cleaned['record_id'] = self.df_cleaned['record_id'].astype('string')\n",
    "        self.df_cleaned['NDC'] = self.df_cleaned['NDC'].astype('object')\n",
    "        \n",
    "        # Merge on both record_id and NDC\n",
    "        self.atc_mapping = pd.merge(\n",
    "            self.df_cleaned,\n",
    "            df_atc4[['record_id', 'NDC', 'ATC4 Class']],\n",
    "            on=['record_id', 'NDC'],\n",
    "            how='left'\n",
    "        )\n",
    "        #deduplication after merge by record_id\n",
    "        before_count=len(self.atc_mapping) \n",
    "        self.atc_mapping=self.atc_mapping.drop_duplicates(subset='record_id', keep='first')\n",
    "        \n",
    "        total = len(self.atc_mapping)\n",
    "     \n",
    "        mapped = self.atc_mapping['ATC4 Class'].notna().sum()\n",
    "        print(f\"Merged: {total:,} records, {mapped:,} with ATC4 ({mapped/total*100:.1f}%)\")\n",
    "        \n",
    "        missing = total - mapped\n",
    "        if missing > 0:\n",
    "            print(f\"Missing: {missing:,} records, {self.atc_mapping[self.atc_mapping['ATC4 Class'].isna()]['NDC'].nunique():,} unique NDCs\")\n",
    "        \n",
    "        return self.atc_mapping\n",
    "    \n",
    "    def analyze_atc_distribution(self, level='ATC3'):\n",
    "\n",
    "        if self.atc_mapping is None:\n",
    "            raise ValueError(\"Run analyze_atc4_mapping() first\")\n",
    "        \n",
    "        records = self.atc_mapping[self.atc_mapping['ATC4 Class'].notna()].copy()\n",
    "        \n",
    "        if len(records) == 0:\n",
    "            print(\"No records with valid ATC4 mappings.\")\n",
    "            return None\n",
    "        \n",
    "        # Create ATC level column if needed\n",
    "        if level == 'ATC3':\n",
    "            records['ATC3 Class'] = records['ATC4 Class'].str[:4]\n",
    "            class_col = 'ATC3 Class'\n",
    "        elif level == 'ATC2':\n",
    "            records['ATC2 Class'] = records['ATC4 Class'].str[:3]\n",
    "            class_col = 'ATC2 Class'\n",
    "        else:\n",
    "            class_col = 'ATC4 Class'\n",
    "        \n",
    "        # Count classes per record_id\n",
    "        per_record = records.groupby('record_id')[class_col].nunique().reset_index()\n",
    "        per_record.columns = ['record_id', 'num_classes']\n",
    "        \n",
    "        distribution = per_record['num_classes'].value_counts().sort_index()\n",
    "        \n",
    "        print(f\"\\n{level} CLASSES PER RECORD_ID:\")\n",
    "        for n_classes, count in distribution.items():\n",
    "            pct = (count / len(per_record)) * 100\n",
    "            print(f\"  {n_classes} class(es): {count:,} records ({pct:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"  Avg {level} per record: {per_record['num_classes'].mean():.2f}\")\n",
    "        print(f\"  Max {level} per record: {per_record['num_classes'].max()}\")\n",
    "        \n",
    "        return per_record\n",
    "\n",
    "    def fetch_atc_names(self, cache_path=None):\n",
    "        \"\"\"Fetch ATC class names (ATC4, ATC3, ATC2) from RxNav API.\"\"\"\n",
    "        if self.atc_mapping is None:\n",
    "            raise ValueError(\"Must run analyze_atc4_mapping() first\")\n",
    "        \n",
    "        if cache_path is None:\n",
    "            cache_path = os.path.join(self.base_path, \"ATC\\\\cache_files\\\\atc_names_cache\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"FETCHING ATC CLASS NAMES\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Using cache: {cache_path}\")\n",
    "        \n",
    "        # Get only records with valid ATC4 mappings\n",
    "        df_with_atc = self.atc_mapping[self.atc_mapping['ATC4 Class'].notna()].copy()\n",
    "        \n",
    "        # Create ATC3 and ATC2 columns from ATC4\n",
    "        print(\"\\nCreating ATC3 and ATC2 columns from ATC4...\")\n",
    "        df_with_atc['ATC3 Class'] = df_with_atc['ATC4 Class'].str[:4]\n",
    "        df_with_atc['ATC2 Class'] = df_with_atc['ATC4 Class'].str[:3]\n",
    "        \n",
    "        # Get unique codes for each level\n",
    "        unique_atc4 = df_with_atc['ATC4 Class'].dropna().unique()\n",
    "        unique_atc3 = df_with_atc['ATC3 Class'].dropna().unique()\n",
    "        unique_atc2 = df_with_atc['ATC2 Class'].dropna().unique()\n",
    "        \n",
    "        # Filter out invalid codes\n",
    "        unique_atc4 = [c for c in unique_atc4 if c not in ['No ATC Mapping Found', 'No RxCUI Found', '']]\n",
    "        unique_atc3 = [c for c in unique_atc3 if c not in ['No ATC Mapping Found', 'No RxCUI Found', '', 'No ', 'No']]\n",
    "        unique_atc2 = [c for c in unique_atc2 if c not in ['No ATC Mapping Found', 'No RxCUI Found', '', 'No ', 'No']]\n",
    "        \n",
    "        print(f\"\\nUnique codes to fetch:\")\n",
    "        print(f\"  ATC4: {len(unique_atc4)}\")\n",
    "        print(f\"  ATC3: {len(unique_atc3)}\")\n",
    "        print(f\"  ATC2: {len(unique_atc2)}\")\n",
    "        \n",
    "        # Build mappings\n",
    "        atc4_names = {}\n",
    "        atc3_names = {}\n",
    "        atc2_names = {}\n",
    "        \n",
    "        with shelve.open(cache_path) as cache:\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            print(\"\\nFetching ATC4 names...\")\n",
    "            for code in unique_atc4:\n",
    "                atc4_names[code] = self._get_atc_name(code, cache)\n",
    "            \n",
    "            print(\"Fetching ATC3 names...\")\n",
    "            for code in unique_atc3:\n",
    "                atc3_names[code] = self._get_atc_name(code, cache)\n",
    "            \n",
    "            print(\"Fetching ATC2 names...\")\n",
    "            for code in unique_atc2:\n",
    "                atc2_names[code] = self._get_atc_name(code, cache)\n",
    "            \n",
    "            print(f\"\\nTotal processing time: {(datetime.now() - start_time).total_seconds()/60:.1f} minutes\")\n",
    "        \n",
    "        # Apply names to all records in atc_mapping\n",
    "        print(\"\\nApplying names to dataframe...\")\n",
    "        self.atc_mapping['ATC3 Class'] = self.atc_mapping['ATC4 Class'].str[:4]\n",
    "        self.atc_mapping['ATC2 Class'] = self.atc_mapping['ATC4 Class'].str[:3]\n",
    "        \n",
    "        self.atc_mapping['ATC4_Name'] = self.atc_mapping['ATC4 Class'].map(atc4_names).fillna('')\n",
    "        self.atc_mapping['ATC3_Name'] = self.atc_mapping['ATC3 Class'].map(atc3_names).fillna('')\n",
    "        self.atc_mapping['ATC2_Name'] = self.atc_mapping['ATC2 Class'].map(atc2_names).fillna('')\n",
    "        \n",
    "        print(f\"\\nATC names added successfully!\")\n",
    "        print(\"\\nSample output:\")\n",
    "        sample = self.atc_mapping[self.atc_mapping['ATC4 Class'].notna()][['NDC', 'record_id', 'ATC4 Class', 'ATC4_Name', 'ATC3 Class', 'ATC3_Name', 'ATC2 Class', 'ATC2_Name']].head(5)\n",
    "        print(sample.to_string())\n",
    "        \n",
    "        return self.atc_mapping\n",
    "    \n",
    "    def prepare_final_dataframe(self):\n",
    "\n",
    "        if self.atc_mapping is None:\n",
    "            raise ValueError(\"Run fetch_atc_names() first\")\n",
    "        \n",
    "        self.df_merged = self.atc_mapping.copy()\n",
    "        \n",
    "        # Scale units\n",
    "        self.df_merged['Units Reimbursed'] = self.df_merged['Units Reimbursed'] / 1e9\n",
    "        self.df_merged['Number of Prescriptions'] = self.df_merged['Number of Prescriptions'] / 1e6\n",
    "        \n",
    "        total = len(self.df_merged)\n",
    "        mapped = self.df_merged['ATC4 Class'].notna().sum()\n",
    "        \n",
    "        print(f\"\\nFinal Statistics:\")\n",
    "        print(f\"  Records: {total:,} ({mapped:,} with ATC4, {mapped/total*100:.1f}%)\")\n",
    "        print(f\"  Units Reimbursed: {self.df_merged['Units Reimbursed'].sum():.2f} Billion\")\n",
    "        print(f\"  Prescriptions: {self.df_merged['Number of Prescriptions'].sum():.2f} Million\")\n",
    "        \n",
    "        return self.df_merged\n",
    "    \n",
    "    def _get_atc_name(self, atc_code, cache):\n",
    "        \"\"\"Helper: Fetch ATC name from RxNav API with caching.\"\"\"\n",
    "        cache_key = f\"atc_name:{atc_code}\"\n",
    "        if cache_key in cache:\n",
    "            return cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            url = f\"https://rxnav.nlm.nih.gov/REST/rxclass/class/byId.json?classId={atc_code}\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'rxclassMinConceptList' in data and 'rxclassMinConcept' in data['rxclassMinConceptList']:\n",
    "                concepts = data['rxclassMinConceptList']['rxclassMinConcept']\n",
    "                if concepts:\n",
    "                    name = concepts[0].get('className', '')\n",
    "                    cache[cache_key] = name\n",
    "                    return name\n",
    "            \n",
    "            cache[cache_key] = ''\n",
    "            return ''\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving {atc_code}: {e}\")\n",
    "            cache[cache_key] = ''\n",
    "            return ''\n",
    "\n",
    "    def export_merged_data(self, output_filename=None, show_details=True):\n",
    "\n",
    "        if self.df_merged is None:\n",
    "            raise ValueError(\"Run prepare_final_dataframe() first\")\n",
    "            \n",
    "        output_filename = output_filename or f\"merged_NEWdata_{self.year}.csv\"\n",
    "        output_path = os.path.join(self.base_path, f\"ATC\\\\merged_data\\\\{output_filename}\")\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Check duplicates\n",
    "        initial_count = len(self.df_merged)\n",
    "        duplicate_count = self.df_merged['record_id'].duplicated().sum()\n",
    "        \n",
    "        print(f\"\\nDeduplication Check:\")\n",
    "        print(f\"  Before: {initial_count:,} rows\")\n",
    "        print(f\"  Duplicates: {duplicate_count:,}\")\n",
    "        \n",
    "        # Show sample duplicates if requested\n",
    "        if show_details and duplicate_count > 0:\n",
    "            dup_records = self.df_merged[self.df_merged['record_id'].duplicated(keep=False)].sort_values('record_id')\n",
    "            sample_ids = dup_records['record_id'].unique()[:2]\n",
    "            \n",
    "            print(f\"\\nSample duplicate record_ids:\")\n",
    "            for rid in sample_ids:\n",
    "                sample = self.df_merged[self.df_merged['record_id'] == rid][\n",
    "                    ['record_id', 'NDC', 'State', 'ATC4 Class', 'ATC2 Class']\n",
    "                ]\n",
    "                print(f\"\\n{rid}:\")\n",
    "                print(sample.to_string(index=False))\n",
    "        \n",
    "        # Deduplicate and export\n",
    "        df_final = self.df_merged.drop_duplicates(subset='record_id', keep='first')\n",
    "        df_final.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\n  After: {len(df_final):,} rows\")\n",
    "        print(f\"  Removed: {initial_count - len(df_final):,}\")\n",
    "        print(f\"\\nExported to: {output_path}\")\n",
    "\n",
    "        #Showing final atc class mapping in %\n",
    "        final_count = len(df_final)\n",
    "        final_mapped_records = df_final['ATC4 Class'].notna().sum()\n",
    "        final_unmapped_records = final_count - final_mapped_records\n",
    "        final_mapped_ndcs=df_final[df_final['ATC4 Class'].notna()]['NDC'].nunique()\n",
    "        final_unmapped_ndcs=df_final['NDC'].nunique()\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        agg = df_final.groupby('record_id').agg({\n",
    "            'Units Reimbursed': 'sum',\n",
    "            'Number of Prescriptions': 'sum'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nAggregated Totals:\")\n",
    "        print(f\"  Units Reimbursed: {agg['Units Reimbursed'].sum():.3f} Billion\")\n",
    "        print(f\"  Number of Prescriptions: {agg['Number of Prescriptions'].sum():3f} Million\")\n",
    "        \n",
    "        return output_path\n",
    "    \n",
    "    def export_unscaled_data(self, output_filename=None, show_details=True):\n",
    "        \"\"\"Export merged data without scaling units.\"\"\"\n",
    "        if self.atc_mapping is None:\n",
    "            raise ValueError(\"Run fetch_atc_names() first\")\n",
    "        \n",
    "        output_filename = output_filename or f\"MUD_{self.year}.csv\"\n",
    "        output_path = os.path.join(self.base_path, f\"ATC\\\\merged_data\\\\unscaled_data\\\\{output_filename}\")\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Use atc_mapping directly (no scaling)\n",
    "        df_unscaled = self.atc_mapping.copy()\n",
    "        \n",
    "        # Check duplicates\n",
    "        initial_count = len(df_unscaled)\n",
    "        duplicate_count = df_unscaled['record_id'].duplicated().sum()\n",
    "        \n",
    "        print(f\"\\nDeduplication Check:\")\n",
    "        print(f\"  Before: {initial_count:,} rows\")\n",
    "        print(f\"  Duplicates: {duplicate_count:,}\")\n",
    "        \n",
    "        # Show sample duplicates if requested\n",
    "        if show_details and duplicate_count > 0:\n",
    "            dup_records = df_unscaled[df_unscaled['record_id'].duplicated(keep=False)].sort_values('record_id')\n",
    "            sample_ids = dup_records['record_id'].unique()[:2]\n",
    "            \n",
    "            print(f\"\\nSample duplicate record_ids:\")\n",
    "            for rid in sample_ids:\n",
    "                sample = df_unscaled[df_unscaled['record_id'] == rid][\n",
    "                    ['record_id', 'NDC', 'State', 'ATC4 Class', 'ATC2 Class']\n",
    "                ]\n",
    "                print(f\"\\n{rid}:\")\n",
    "                print(sample.to_string(index=False))\n",
    "        \n",
    "        # Deduplicate and export\n",
    "        df_final = df_unscaled.drop_duplicates(subset='record_id', keep='first')\n",
    "        df_final.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\n  After: {len(df_final):,} rows\")\n",
    "        print(f\"  Removed: {initial_count - len(df_final):,}\")\n",
    "        print(f\"\\nExported to: {output_path}\")\n",
    "        \n",
    "        # Show final ATC class mapping\n",
    "        final_count = len(df_final)\n",
    "        final_mapped_records = df_final['ATC4 Class'].notna().sum()\n",
    "        \n",
    "        # Aggregate metrics (unscaled)\n",
    "        agg = df_final.groupby('record_id').agg({\n",
    "            'Units Reimbursed': 'sum',\n",
    "            'Number of Prescriptions': 'sum'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nAggregated Totals (Unscaled):\")\n",
    "        print(f\"  Units Reimbursed: {agg['Units Reimbursed'].sum():,.0f}\")\n",
    "        print(f\"  Number of Prescriptions: {agg['Number of Prescriptions'].sum():,.0f}\")\n",
    "        print(f\"  Mapped Records: {final_mapped_records:,} ({final_mapped_records/final_count*100:.1f}%)\")\n",
    "        \n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c736ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDCATC_overview:\n",
    "\n",
    "    @staticmethod\n",
    "    def create_multi_year_distribution_analysis(years_list, base_path=None):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "            \n",
    "        print(\"Creating Multi-Year ATC Distribution Analysis...\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        results = {\n",
    "            'ATC4_1_class': {}, 'ATC4_2_classes': {}, 'ATC4_3+_classes': {},\n",
    "            'ATC3_1_class': {}, 'ATC3_2_classes': {}, 'ATC3_3+_classes': {},\n",
    "            'ATC2_1_class': {}, 'ATC2_2_classes': {}, 'ATC2_3+_classes': {}\n",
    "        }\n",
    "        \n",
    "        for year in years_list:\n",
    "            print(f\"Processing {year}...\", end=\" \")\n",
    "            try:\n",
    "                # Load the pre-processed CSV file\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                \n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                if records.empty:\n",
    "                    print(\"No ATC records\")\n",
    "                    for key in results.keys():\n",
    "                        results[key][year] = \"N/A\"\n",
    "                    continue\n",
    "                    \n",
    "                records['ATC2 Class'] = records['ATC4 Class'].str[:3]\n",
    "                records['ATC3 Class'] = records['ATC4 Class'].str[:4]\n",
    "                \n",
    "                # Calculate distributions for each level\n",
    "                for level, col in [('ATC4', 'ATC4 Class'), ('ATC3', 'ATC3 Class'), ('ATC2', 'ATC2 Class')]:\n",
    "                    per_record = records.groupby('record_id')[col].nunique()\n",
    "                    dist = per_record.value_counts().sort_index()\n",
    "                    total = len(per_record)\n",
    "                    \n",
    "                    results[f'{level}_1_class'][year] = f\"{(dist.get(1, 0) / total * 100):.1f}%\"\n",
    "                    results[f'{level}_2_classes'][year] = f\"{(dist.get(2, 0) / total * 100):.1f}%\"\n",
    "                    results[f'{level}_3+_classes'][year] = f\"{(dist[dist.index >= 3].sum() / total * 100):.1f}%\"\n",
    "                \n",
    "                print(\"✓\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"✗ File not found: {csv_path}\")\n",
    "                for key in results.keys():\n",
    "                    results[key][year] = \"N/A\"\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "                for key in results.keys():\n",
    "                    results[key][year] = \"N/A\"\n",
    "        \n",
    "        df_percentages = pd.DataFrame(results).T\n",
    "        print(f\"\\nATC DISTRIBUTION PERCENTAGES ACROSS YEARS\")\n",
    "        print(\"=\"*60)\n",
    "        print(df_percentages)\n",
    "        return df_percentages\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_general_atc_overview_by_state(years_list, base_path=None, state_filter=None):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "\n",
    "        if state_filter:\n",
    "            if isinstance(state_filter, str):\n",
    "                state_filter = [state_filter]\n",
    "            print(f\"Creating ATC2 & ATC3 Overview by State for: {', '.join(state_filter)}\")\n",
    "        else:\n",
    "            print(\"Creating ATC2 & ATC3 Overview by State (All States)\")\n",
    "        print(\"=\"*78)\n",
    "        \n",
    "        # Results will be organized by state\n",
    "        state_results = {}\n",
    "        all_states = set()\n",
    "        \n",
    "        # Collect data for all years first\n",
    "        for year in years_list:\n",
    "            print(f\"Processing {year}...\", end=\" \")\n",
    "            try:\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                \n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                if records.empty:\n",
    "                    print(\"No ATC records\")\n",
    "                    continue\n",
    "                \n",
    "                # Filter by state if specified\n",
    "                if state_filter:\n",
    "                    records = records[records['State'].isin(state_filter)]\n",
    "                    if records.empty:\n",
    "                        print(f\"No records for states {state_filter}\")\n",
    "                        continue\n",
    "                    states_to_process = state_filter\n",
    "                else:\n",
    "                    states_to_process = records['State'].unique()\n",
    "                    all_states.update(states_to_process)\n",
    "                \n",
    "                for state in states_to_process:\n",
    "                    state_records = records[records['State'] == state]\n",
    "                    if state_records.empty:\n",
    "                        continue\n",
    "                    \n",
    "                    # Initialize state data structure\n",
    "                    if state not in state_results:\n",
    "                        state_results[state] = {\n",
    "                            'atc2_year_results': {},\n",
    "                            'atc3_year_results': {}\n",
    "                        }\n",
    "                    \n",
    "                    # ATC2 summary for this state\n",
    "                    pairs2 = state_records[['record_id', 'NDC', 'ATC2 Class']].drop_duplicates()\n",
    "                    atc2_summary = pairs2.groupby('ATC2 Class').agg(\n",
    "                        Unique_NDCs=('NDC', 'nunique'),\n",
    "                        Total_Records=('record_id', 'nunique')\n",
    "                    ).sort_values('Unique_NDCs', ascending=False)\n",
    "                    atc2_summary['Percentage_of_NDCs'] = (\n",
    "                        atc2_summary['Unique_NDCs'] / pairs2['NDC'].nunique() * 100\n",
    "                    ).round(1)\n",
    "                    \n",
    "                    # ATC3 summary for this state\n",
    "                    pairs3 = state_records[['record_id', 'NDC', 'ATC3 Class']].drop_duplicates()\n",
    "                    atc3_summary = pairs3.groupby('ATC3 Class').agg(\n",
    "                        Unique_NDCs=('NDC', 'nunique'),\n",
    "                        Total_Records=('record_id', 'nunique')\n",
    "                    ).sort_values('Unique_NDCs', ascending=False)\n",
    "                    atc3_summary['Percentage_of_NDCs'] = (\n",
    "                        atc3_summary['Unique_NDCs'] / pairs3['NDC'].nunique() * 100\n",
    "                    ).round(1)\n",
    "                    \n",
    "                    state_results[state]['atc2_year_results'][year] = atc2_summary\n",
    "                    state_results[state]['atc3_year_results'][year] = atc3_summary\n",
    "                \n",
    "                processed_states = len([s for s in states_to_process if s in state_results])\n",
    "                print(f\"✓ (States: {processed_states})\")\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                print(f\"✗ File not found\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "        \n",
    "        # Process each state with the same format as original method\n",
    "        final_state_results = {}\n",
    "        \n",
    "        states_to_analyze = state_filter if state_filter else sorted(all_states)\n",
    "        \n",
    "        for state in states_to_analyze:\n",
    "            if state not in state_results:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n\" + \"=\"*60)\n",
    "            print(f\"PROCESSING STATE: {state}\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            atc2_year_results = state_results[state]['atc2_year_results']\n",
    "            atc3_year_results = state_results[state]['atc3_year_results']\n",
    "            \n",
    "            # Print summaries for this state (same format as original)\n",
    "            print(f\"\\nUNIQUE NDCs PER ATC2 CLASS BY YEAR - {state}\")\n",
    "            print(\"=\"*50)\n",
    "            for year in years_list:\n",
    "                if year in atc2_year_results and not atc2_year_results[year].empty:\n",
    "                    print(f\"\\n{year}: {len(atc2_year_results[year])} classes, \"\n",
    "                        f\"{atc2_year_results[year]['Unique_NDCs'].sum():,} total NDCs\")\n",
    "                    print(\"Top 10:\")\n",
    "                    print(atc2_year_results[year].head(10))\n",
    "            \n",
    "            print(f\"\\nUNIQUE NDCs PER ATC3 CLASS BY YEAR - {state}\")\n",
    "            print(\"=\"*50)\n",
    "            for year in years_list:\n",
    "                if year in atc3_year_results and not atc3_year_results[year].empty:\n",
    "                    print(f\"\\n{year}: {len(atc3_year_results[year])} classes, \"\n",
    "                        f\"{atc3_year_results[year]['Unique_NDCs'].sum():,} total NDCs\")\n",
    "                    print(\"Top 10:\")\n",
    "                    print(atc3_year_results[year].head(10))\n",
    "            \n",
    "            # Build comparison tables (same logic as original)\n",
    "            def build_comparison(year_tables):\n",
    "                all_classes = set()\n",
    "                for tbl in year_tables.values():\n",
    "                    if not tbl.empty:\n",
    "                        all_classes.update(tbl.index.tolist())\n",
    "                comp = {cls: {y: int(year_tables[y].loc[cls, 'Unique_NDCs']) \n",
    "                            if y in year_tables and not year_tables[y].empty and cls in year_tables[y].index else 0\n",
    "                            for y in years_list}\n",
    "                        for cls in sorted(all_classes)}\n",
    "                df = pd.DataFrame(comp).T\n",
    "                return df.loc[df.sum(axis=1).sort_values(ascending=False).index]\n",
    "            \n",
    "            atc2_comparison = build_comparison(atc2_year_results)\n",
    "            atc3_comparison = build_comparison(atc3_year_results)\n",
    "            \n",
    "            # Create cumulative frequency tables (same logic as original)\n",
    "            def create_cumulative_frequency_table(comparison_df, level_name):\n",
    "                total_ndcs = comparison_df.sum(axis=1).sort_values(ascending=False)\n",
    "                \n",
    "                freq_table = pd.DataFrame({\n",
    "                    'ATC_Class': total_ndcs.index,\n",
    "                    'Total_Unique_NDCs': total_ndcs.values,\n",
    "                    'Percentage': (total_ndcs.values / total_ndcs.sum() * 100).round(2)\n",
    "                })\n",
    "                \n",
    "                freq_table['Cumulative_NDCs'] = freq_table['Total_Unique_NDCs'].cumsum()\n",
    "                freq_table['Cumulative_Percentage'] = freq_table['Percentage'].cumsum().round(2)\n",
    "                \n",
    "                freq_table.reset_index(drop=True, inplace=True)\n",
    "                freq_table.index = freq_table.index + 1\n",
    "                \n",
    "                return freq_table\n",
    "            \n",
    "            atc2_freq_table = create_cumulative_frequency_table(atc2_comparison, 'ATC2')\n",
    "            atc3_freq_table = create_cumulative_frequency_table(atc3_comparison, 'ATC3')\n",
    "            \n",
    "            # Store results for this state\n",
    "            final_state_results[state] = {\n",
    "                'atc2_year_results': atc2_year_results,\n",
    "                'atc3_year_results': atc3_year_results,\n",
    "                'atc2_comparison': atc2_comparison,\n",
    "                'atc3_comparison': atc3_comparison,\n",
    "                'atc2_freq_table': atc2_freq_table,\n",
    "                'atc3_freq_table': atc3_freq_table\n",
    "            }\n",
    "        \n",
    "        return final_state_results\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_atc_ndc_details(year, top_n=10, base_path=None):\n",
    "    \n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "            \n",
    "        print(f\"Analyzing ATC-NDC details for {year}...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            # Load the pre-processed CSV file\n",
    "            csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "            df_merged = pd.read_csv(csv_path)\n",
    "            \n",
    "            records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "            if records.empty:\n",
    "                print(\"No records with ATC mapping\")\n",
    "                return pd.DataFrame(), pd.DataFrame()\n",
    "                \n",
    "            records['ATC2 Class'] = records['ATC4 Class'].str[:3]\n",
    "            records['ATC3 Class'] = records['ATC4 Class'].str[:4]\n",
    "            \n",
    "            # ATC2 details\n",
    "            atc2_details = records.groupby('ATC2 Class').agg(\n",
    "                Unique_NDCs=('NDC', 'nunique'),\n",
    "                Total_Records=('record_id', 'nunique')\n",
    "            ).sort_values('Unique_NDCs', ascending=False).head(top_n)\n",
    "            \n",
    "            # ATC3 details\n",
    "            atc3_details = records.groupby('ATC3 Class').agg(\n",
    "                Unique_NDCs=('NDC', 'nunique'),\n",
    "                Total_Records=('record_id', 'nunique')\n",
    "            ).sort_values('Unique_NDCs', ascending=False).head(top_n)\n",
    "            \n",
    "            print(f\"\\nTop {top_n} ATC2 Classes:\")\n",
    "            print(atc2_details)\n",
    "            print(f\"\\nTop {top_n} ATC3 Classes:\")\n",
    "            print(atc3_details)\n",
    "            \n",
    "            return atc2_details, atc3_details\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"✗ File not found: {csv_path}\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error: {e}\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    @staticmethod\n",
    "    def export_cumulative_frequency_excel(years_list, level='ATC2', base_path=None, output_filename=None, \n",
    "                                          include_ndc_counts=True, by_state=False, state_filter=None):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        # Print header\n",
    "        if by_state:\n",
    "            states_msg = ', '.join(state_filter) if state_filter else 'All States'\n",
    "            print(f\"Creating {level} Cumulative Frequency Analysis Excel for States: {states_msg}\")\n",
    "        else:\n",
    "            print(f\"Creating {level} Cumulative Frequency Analysis Excel...\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Helper function to create ATC level column\n",
    "        def create_atc_level_column(df, level):\n",
    "            if level == 'ATC2':\n",
    "                return df['ATC4 Class'].str[:3]\n",
    "            elif level == 'ATC3':\n",
    "                return df['ATC4 Class'].str[:4]\n",
    "            else:  # ATC4\n",
    "                return df['ATC4 Class']\n",
    "        \n",
    "        # Helper function to get name mapping\n",
    "        def get_name_mapping(records, level):\n",
    "            name_col_map = {'ATC2': 'ATC2_Name', 'ATC3': 'ATC3_Name', 'ATC4': 'ATC4_Name'}\n",
    "            name_col = name_col_map.get(level)\n",
    "            \n",
    "            if name_col and name_col in records.columns:\n",
    "                return records[['ATC_Level', name_col]].drop_duplicates().set_index('ATC_Level')[name_col].to_dict()\n",
    "            return {}\n",
    "        \n",
    "        # Helper function to process year data\n",
    "        def process_year_data(csv_path, level, state_filter=None):\n",
    "            try:\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                \n",
    "                if records.empty:\n",
    "                    return None, None, None, None\n",
    "                \n",
    "                # Filter by state if needed\n",
    "                if state_filter:\n",
    "                    records = records[records['State'].isin(state_filter)]\n",
    "                    if records.empty:\n",
    "                        return None, None, None, None\n",
    "                \n",
    "                records['ATC_Level'] = create_atc_level_column(records, level)\n",
    "                name_mapping = get_name_mapping(records, level)\n",
    "                \n",
    "                # Aggregate financial data\n",
    "                financial = records.groupby('ATC_Level').agg(\n",
    "                    Units_Reimbursed=('Units Reimbursed', 'sum'),\n",
    "                    Number_of_Prescriptions=('Number of Prescriptions', 'sum')\n",
    "                )\n",
    "                \n",
    "                # Count unique NDCs\n",
    "                ndc_counts = records.groupby('ATC_Level').agg(\n",
    "                    Unique_NDCs=('NDC', 'nunique')\n",
    "                )\n",
    "                \n",
    "                states_processed = records['State'].nunique() if 'State' in records.columns else 1\n",
    "                \n",
    "                return financial, ndc_counts, name_mapping, states_processed\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file: {e}\")\n",
    "                return None, None, None, None\n",
    "        \n",
    "        # Collect data for all years\n",
    "        if by_state:\n",
    "            state_data = {}  # {state: {year: (financial, ndc_counts, name_mapping)}}\n",
    "            all_states = set()\n",
    "        else:\n",
    "            year_results = {}\n",
    "            ndc_counts = {}\n",
    "            name_mapping = {}\n",
    "        \n",
    "        # Process each year\n",
    "        for year in years_list:\n",
    "            print(f\"Processing {year}...\", end=\" \")\n",
    "            csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "            \n",
    "            if by_state:\n",
    "                # Load full data first\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    records = df[df['ATC4 Class'].notna()].copy()\n",
    "                    \n",
    "                    if state_filter:\n",
    "                        states_to_process = state_filter if isinstance(state_filter, list) else [state_filter]\n",
    "                    else:\n",
    "                        states_to_process = records['State'].unique()\n",
    "                        all_states.update(states_to_process)\n",
    "                    \n",
    "                    for state in states_to_process:\n",
    "                        financial, ndcs, names, _ = process_year_data(csv_path, level, [state])\n",
    "                        \n",
    "                        if financial is not None:\n",
    "                            if state not in state_data:\n",
    "                                state_data[state] = {}\n",
    "                            state_data[state][year] = (financial, ndcs, names)\n",
    "                    \n",
    "                    print(f\"✓ (States: {len(states_to_process)})\")\n",
    "                except:\n",
    "                    print(\"✗\")\n",
    "            else:\n",
    "                financial, ndcs, names, _ = process_year_data(csv_path, level)\n",
    "                \n",
    "                if financial is not None:\n",
    "                    year_results[year] = financial\n",
    "                    ndc_counts[year] = ndcs\n",
    "                    name_mapping.update(names)\n",
    "                    print(f\"✓ ({len(financial)} classes)\")\n",
    "                else:\n",
    "                    print(\"✗\")\n",
    "        \n",
    "        # Helper function to build comparison dataframes\n",
    "        def build_comparison_dfs(year_financial_dict, year_ndc_dict, years_list):\n",
    "            # Collect all unique ATC classes\n",
    "            all_classes = set()\n",
    "            for df in year_financial_dict.values():\n",
    "                if df is not None and not df.empty:\n",
    "                    all_classes.update(df.index)\n",
    "            for df in year_ndc_dict.values():\n",
    "                if df is not None and not df.empty:\n",
    "                    all_classes.update(df.index)\n",
    "            \n",
    "            if not all_classes:\n",
    "                return None, None, None\n",
    "            \n",
    "            # Build comparison dictionaries\n",
    "            units_comp = {}\n",
    "            presc_comp = {}\n",
    "            ndc_comp = {}\n",
    "            \n",
    "            for cls in sorted(all_classes):\n",
    "                units_comp[cls] = {\n",
    "                    y: float(year_financial_dict[y].loc[cls, 'Units_Reimbursed']) \n",
    "                    if y in year_financial_dict and year_financial_dict[y] is not None \n",
    "                    and not year_financial_dict[y].empty and cls in year_financial_dict[y].index \n",
    "                    else 0.0 \n",
    "                    for y in years_list\n",
    "                }\n",
    "                \n",
    "                presc_comp[cls] = {\n",
    "                    y: float(year_financial_dict[y].loc[cls, 'Number_of_Prescriptions']) \n",
    "                    if y in year_financial_dict and year_financial_dict[y] is not None \n",
    "                    and not year_financial_dict[y].empty and cls in year_financial_dict[y].index \n",
    "                    else 0.0 \n",
    "                    for y in years_list\n",
    "                }\n",
    "                \n",
    "                ndc_comp[cls] = {\n",
    "                    y: int(year_ndc_dict[y].loc[cls, 'Unique_NDCs']) \n",
    "                    if y in year_ndc_dict and year_ndc_dict[y] is not None \n",
    "                    and not year_ndc_dict[y].empty and cls in year_ndc_dict[y].index \n",
    "                    else 0 \n",
    "                    for y in years_list\n",
    "                }\n",
    "            \n",
    "            # Convert to DataFrames and sort by total units\n",
    "            units_df = pd.DataFrame(units_comp).T\n",
    "            presc_df = pd.DataFrame(presc_comp).T\n",
    "            ndc_df = pd.DataFrame(ndc_comp).T\n",
    "            \n",
    "            units_total = units_df.sum(axis=1).sort_values(ascending=False)\n",
    "            units_df = units_df.loc[units_total.index]\n",
    "            presc_df = presc_df.loc[units_total.index]\n",
    "            ndc_df = ndc_df.loc[units_total.index]\n",
    "            \n",
    "            return units_df, presc_df, ndc_df\n",
    "        \n",
    "        # Helper function to create cumulative frequency DataFrame\n",
    "        def create_cumulative_df(comparison_df, metric_name, name_mapping):\n",
    "            totals = comparison_df.sum(axis=1)\n",
    "            total_sum = totals.sum()\n",
    "            \n",
    "            cumulative_total = 0\n",
    "            df_data = []\n",
    "            \n",
    "            for atc_class in comparison_df.index:\n",
    "                class_total = totals[atc_class]\n",
    "                cumulative_total += class_total\n",
    "                percentage = (class_total / total_sum * 100) if total_sum > 0 else 0\n",
    "                cumulative_pct = (cumulative_total / total_sum * 100) if total_sum > 0 else 0\n",
    "                \n",
    "                row = {'ATC_Class': atc_class}\n",
    "                \n",
    "                # Add ATC name if available\n",
    "                row['ATC_Name'] = name_mapping.get(atc_class, '')\n",
    "                \n",
    "                # Add year-by-year data\n",
    "                for year in years_list:\n",
    "                    if metric_name == 'NDCs':\n",
    "                        row[f'{metric_name}_{year}'] = int(comparison_df.loc[atc_class, year])\n",
    "                    else:\n",
    "                        row[f'{metric_name}_{year}'] = round(comparison_df.loc[atc_class, year], 3)\n",
    "                \n",
    "                # Add summary columns\n",
    "                if metric_name == 'NDCs':\n",
    "                    row[f'Total_{metric_name}'] = int(class_total)\n",
    "                    row['Percentage'] = round(percentage, 2)\n",
    "                    row[f'Cumulative_{metric_name}'] = int(cumulative_total)\n",
    "                    row['Cumulative_Percentage_NDCs'] = round(cumulative_pct, 2)\n",
    "                else:\n",
    "                    row[f'Total_{metric_name}'] = round(class_total, 3)\n",
    "                    row['Percentage'] = round(percentage, 2)\n",
    "                    row[f'Cumulative_{metric_name}'] = round(cumulative_total, 3)\n",
    "                    row['Cumulative_Percentage'] = round(cumulative_pct, 2)\n",
    "                \n",
    "                df_data.append(row)\n",
    "            \n",
    "            return pd.DataFrame(df_data)\n",
    "        \n",
    "        # Export function\n",
    "        def export_to_excel(units_df, prescriptions_df, ndc_df, output_path, include_ndc_counts):\n",
    "            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "                units_df.to_excel(writer, sheet_name='Units_Reimbursed', index=False)\n",
    "                prescriptions_df.to_excel(writer, sheet_name='Prescriptions', index=False)\n",
    "                \n",
    "                if include_ndc_counts:\n",
    "                    ndc_df.to_excel(writer, sheet_name='NDC_Counts', index=False)\n",
    "            \n",
    "            print(f\"Exported to Excel: {output_path}\")\n",
    "        \n",
    "        # Process and export\n",
    "        output_dir = os.path.join(base_path, \"ATC\\\\exported_analysis\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        if by_state:\n",
    "            states_to_export = state_filter if state_filter else sorted(all_states)\n",
    "            all_output_paths = {}\n",
    "            \n",
    "            for state in states_to_export:\n",
    "                if state not in state_data:\n",
    "                    continue\n",
    "                \n",
    "                # Extract data for this state\n",
    "                state_year_financial = {y: data[0] for y, data in state_data[state].items()}\n",
    "                state_year_ndc = {y: data[1] for y, data in state_data[state].items()}\n",
    "                state_name_mapping = {}\n",
    "                for data in state_data[state].values():\n",
    "                    state_name_mapping.update(data[2])\n",
    "                \n",
    "                # Build comparison DataFrames\n",
    "                units_comp_df, presc_comp_df, ndc_comp_df = build_comparison_dfs(\n",
    "                    state_year_financial, state_year_ndc, years_list\n",
    "                )\n",
    "                \n",
    "                if units_comp_df is None:\n",
    "                    continue\n",
    "                \n",
    "                # Create cumulative DataFrames\n",
    "                units_cum = create_cumulative_df(units_comp_df, 'Units', state_name_mapping)\n",
    "                presc_cum = create_cumulative_df(presc_comp_df, 'Prescriptions', state_name_mapping)\n",
    "                ndc_cum = create_cumulative_df(ndc_comp_df, 'NDCs', state_name_mapping)\n",
    "                \n",
    "                # Generate output filename\n",
    "                if output_filename:\n",
    "                    name_parts = output_filename.rsplit('.', 1)\n",
    "                    state_output_filename = f\"{name_parts[0]}_{state}.{name_parts[1]}\"\n",
    "                else:\n",
    "                    state_output_filename = f\"{level}_Cumulative_Analysis_{state}_with_NDC_Counts.xlsx\"\n",
    "                \n",
    "                output_path = os.path.join(output_dir, state_output_filename)\n",
    "                export_to_excel(units_cum, presc_cum, ndc_cum, output_path, include_ndc_counts)\n",
    "                all_output_paths[state] = output_path\n",
    "            \n",
    "            return all_output_paths\n",
    "        \n",
    "        else:\n",
    "            # Build comparison DataFrames\n",
    "            units_comp_df, presc_comp_df, ndc_comp_df = build_comparison_dfs(\n",
    "                year_results, ndc_counts, years_list\n",
    "            )\n",
    "            \n",
    "            if units_comp_df is None:\n",
    "                print(\"No data found!\")\n",
    "                return None\n",
    "            \n",
    "            # Create cumulative DataFrames\n",
    "            units_cum = create_cumulative_df(units_comp_df, 'Units', name_mapping)\n",
    "            presc_cum = create_cumulative_df(presc_comp_df, 'Prescriptions', name_mapping)\n",
    "            ndc_cum = create_cumulative_df(ndc_comp_df, 'NDCs', name_mapping)\n",
    "            \n",
    "            # Generate output filename\n",
    "            if not output_filename:\n",
    "                output_filename = f\"{level}_Cumulative_Analysis_with_NDC_Counts.xlsx\"\n",
    "            \n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            export_to_excel(units_cum, presc_cum, ndc_cum, output_path, include_ndc_counts)\n",
    "            \n",
    "            return output_path\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_cumulative_80_analysis(base_path=None, cumulative_threshold=80.0):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(f\"COMPARATIVE ANALYSIS: Indiana vs National ATC2 Classes at {cumulative_threshold}% Threshold\")\n",
    "        print(\"=\"*85)\n",
    "        \n",
    "        # File paths\n",
    "        indiana_file = os.path.join(base_path, \"ATC\\\\exported_analysis\\\\ATC2_Cumulative_Analysis_IN_with_NDC_Counts.xlsx\")\n",
    "        national_file = os.path.join(base_path, \"ATC\\\\exported_analysis\\\\ATC2_Cumulative_Analysis_with_NDC_Counts.xlsx\")\n",
    "        \n",
    "        # Load data\n",
    "        try:\n",
    "            indiana_units = pd.read_excel(indiana_file, sheet_name='Units_Reimbursed')\n",
    "            indiana_prescriptions = pd.read_excel(indiana_file, sheet_name='Prescriptions')\n",
    "            national_units = pd.read_excel(national_file, sheet_name='Units_Reimbursed')\n",
    "            national_prescriptions = pd.read_excel(national_file, sheet_name='Prescriptions')\n",
    "            \n",
    "            print(f\"✓ Loaded Indiana data: {len(indiana_units)} ATC2 classes\")\n",
    "            print(f\"✓ Loaded National data: {len(national_units)} ATC2 classes\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"✗ Error: Required Excel files not found. Please run export functions first.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading files: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Helper function to get classes at threshold\n",
    "        def get_classes_at_threshold(df, threshold):\n",
    "            df_sorted = df.sort_values('Cumulative_Percentage').reset_index(drop=True)\n",
    "            threshold_idx = df_sorted[df_sorted['Cumulative_Percentage'] >= threshold].index\n",
    "            \n",
    "            if len(threshold_idx) > 0:\n",
    "                return df_sorted.iloc[:threshold_idx[0] + 1]['ATC_Class'].tolist()\n",
    "            return df_sorted['ATC_Class'].tolist()\n",
    "        \n",
    "        # Get classes at threshold for each dataset\n",
    "        in_units_80 = get_classes_at_threshold(indiana_units, cumulative_threshold)\n",
    "        in_presc_80 = get_classes_at_threshold(indiana_prescriptions, cumulative_threshold)\n",
    "        nat_units_80 = get_classes_at_threshold(national_units, cumulative_threshold)\n",
    "        nat_presc_80 = get_classes_at_threshold(national_prescriptions, cumulative_threshold)\n",
    "        \n",
    "        # Calculate overlaps\n",
    "        in_overlap = set(in_units_80) & set(in_presc_80)\n",
    "        in_only_units = set(in_units_80) - set(in_presc_80)\n",
    "        in_only_presc = set(in_presc_80) - set(in_units_80)\n",
    "        \n",
    "        nat_overlap = set(nat_units_80) & set(nat_presc_80)\n",
    "        nat_only_units = set(nat_units_80) - set(nat_presc_80)\n",
    "        nat_only_presc = set(nat_presc_80) - set(nat_units_80)\n",
    "        \n",
    "        # Print Indiana Analysis\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"1. INDIANA ANALYSIS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nClasses reaching {cumulative_threshold}% cumulative:\")\n",
    "        print(f\"  Units Reimbursed: {len(in_units_80)} classes\")\n",
    "        print(f\"  Prescriptions: {len(in_presc_80)} classes\")\n",
    "        print(f\"\\nComparison:\")\n",
    "        print(f\"  Overlap (both metrics): {len(in_overlap)} classes\")\n",
    "        print(f\"  Only in Units: {len(in_only_units)} classes\")\n",
    "        print(f\"  Only in Prescriptions: {len(in_only_presc)} classes\")\n",
    "        \n",
    "        # Print National Analysis\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"2. NATIONAL ANALYSIS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nClasses reaching {cumulative_threshold}% cumulative:\")\n",
    "        print(f\"  Units Reimbursed: {len(nat_units_80)} classes\")\n",
    "        print(f\"  Prescriptions: {len(nat_presc_80)} classes\")\n",
    "        print(f\"\\nComparison:\")\n",
    "        print(f\"  Overlap (both metrics): {len(nat_overlap)} classes\")\n",
    "        print(f\"  Only in Units: {len(nat_only_units)} classes\")\n",
    "        print(f\"  Only in Prescriptions: {len(nat_only_presc)} classes\")\n",
    "        \n",
    "        # Helper function for detailed comparison\n",
    "        def print_detailed_comparison(set1, set2, metric_name):\n",
    "            overlap = set1 & set2\n",
    "            only_in = set1 - set2\n",
    "            only_nat = set2 - set1\n",
    "            \n",
    "            print(f\"\\n{metric_name.upper()} - Detailed Comparison:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Classes in BOTH Indiana and National ({len(overlap)}):\")\n",
    "            print(f\"  {sorted(overlap) if overlap else 'None'}\")\n",
    "            print(f\"\\nClasses ONLY in Indiana ({len(only_in)}):\")\n",
    "            print(f\"  {sorted(only_in) if only_in else 'None'}\")\n",
    "            print(f\"\\nClasses ONLY in National ({len(only_nat)}):\")\n",
    "            print(f\"  {sorted(only_nat) if only_nat else 'None'}\")\n",
    "        \n",
    "        # Print detailed comparisons\n",
    "        print(f\"\\n{'='*85}\")\n",
    "        print(\"3. DETAILED CLASS ANALYSIS\")\n",
    "        print(f\"{'='*85}\")\n",
    "        \n",
    "        print_detailed_comparison(set(in_units_80), set(nat_units_80), \"Units Reimbursed\")\n",
    "        print_detailed_comparison(set(in_presc_80), set(nat_presc_80), \"Prescriptions\")\n",
    "        \n",
    "        # Helper function to calculate category totals\n",
    "        def get_category_totals(df_units, df_presc, class_list):\n",
    "            if not class_list:\n",
    "                return 0.0, 0.0\n",
    "            units_total = df_units[df_units['ATC_Class'].isin(class_list)]['Total_Units'].sum()\n",
    "            presc_total = df_presc[df_presc['ATC_Class'].isin(class_list)]['Total_Prescriptions'].sum()\n",
    "            return units_total, presc_total\n",
    "        \n",
    "        # Calculate totals for all categories\n",
    "        in_units_only_u, in_units_only_p = get_category_totals(indiana_units, indiana_prescriptions, list(in_only_units))\n",
    "        in_presc_only_u, in_presc_only_p = get_category_totals(indiana_units, indiana_prescriptions, list(in_only_presc))\n",
    "        in_overlap_u, in_overlap_p = get_category_totals(indiana_units, indiana_prescriptions, list(in_overlap))\n",
    "        \n",
    "        nat_units_only_u, nat_units_only_p = get_category_totals(national_units, national_prescriptions, list(nat_only_units))\n",
    "        nat_presc_only_u, nat_presc_only_p = get_category_totals(national_units, national_prescriptions, list(nat_only_presc))\n",
    "        nat_overlap_u, nat_overlap_p = get_category_totals(national_units, national_prescriptions, list(nat_overlap))\n",
    "        \n",
    "        # Create summary DataFrame\n",
    "        totals_summary = pd.DataFrame({\n",
    "            'Geography': ['Indiana', 'Indiana', 'Indiana', 'National', 'National', 'National'],\n",
    "            'Category': ['Only in Units', 'Only in Prescriptions', 'In Both (Overlap)'] * 2,\n",
    "            'Num_Classes': [\n",
    "                len(in_only_units), len(in_only_presc), len(in_overlap),\n",
    "                len(nat_only_units), len(nat_only_presc), len(nat_overlap)\n",
    "            ],\n",
    "            'Total_Units': [\n",
    "                in_units_only_u, in_presc_only_u, in_overlap_u,\n",
    "                nat_units_only_u, nat_presc_only_u, nat_overlap_u\n",
    "            ],\n",
    "            'Total_Prescriptions': [\n",
    "                in_units_only_p, in_presc_only_p, in_overlap_p,\n",
    "                nat_units_only_p, nat_presc_only_p, nat_overlap_p\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{'='*85}\")\n",
    "        print(\"4. SUMMARY TOTALS\")\n",
    "        print(f\"{'='*85}\")\n",
    "        print(totals_summary.to_string(index=False))\n",
    "        \n",
    "        # Return comprehensive results\n",
    "        return {\n",
    "            'totals_summary': totals_summary,\n",
    "            'indiana': {\n",
    "                'units_80': in_units_80,\n",
    "                'prescriptions_80': in_presc_80,\n",
    "                'overlap': list(in_overlap),\n",
    "                'only_units': list(in_only_units),\n",
    "                'only_prescriptions': list(in_only_presc)\n",
    "            },\n",
    "            'national': {\n",
    "                'units_80': nat_units_80,\n",
    "                'prescriptions_80': nat_presc_80,\n",
    "                'overlap': list(nat_overlap),\n",
    "                'only_units': list(nat_only_units),\n",
    "                'only_prescriptions': list(nat_only_presc)\n",
    "            }\n",
    "        }\n",
    "    @staticmethod\n",
    "    def create_pareto_charts(base_path=None, top_n=30):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(\"CREATING PARETO CHARTS: Indiana vs National\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # File paths\n",
    "        indiana_file = os.path.join(base_path, \"ATC\\\\exported_analysis\\\\ATC2_Cumulative_Analysis_IN_with_NDC_Counts.xlsx\")\n",
    "        national_file = os.path.join(base_path, \"ATC\\\\exported_analysis\\\\ATC2_Cumulative_Analysis_with_NDC_Counts.xlsx\")\n",
    "        \n",
    "        # Load data\n",
    "        try:\n",
    "            indiana_units = pd.read_excel(indiana_file, sheet_name='Units_Reimbursed')\n",
    "            indiana_prescriptions = pd.read_excel(indiana_file, sheet_name='Prescriptions')\n",
    "            national_units = pd.read_excel(national_file, sheet_name='Units_Reimbursed')\n",
    "            national_prescriptions = pd.read_excel(national_file, sheet_name='Prescriptions')\n",
    "            print(\"✓ Data loaded successfully\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"✗ Error: Required Excel files not found. Please run export functions first.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading files: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Create figure with 2x2 subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(24, 16))\n",
    "        \n",
    "        def create_single_pareto(ax, df, metric_name, geography, top_n):\n",
    "            \"\"\"Create a single Pareto chart\"\"\"\n",
    "            df_top = df.head(top_n).copy()\n",
    "            \n",
    "            # Prepare data\n",
    "            classes = df_top['ATC_Class']\n",
    "            if metric_name == 'Units':\n",
    "                values = df_top['Total_Units']\n",
    "                y_label = 'Total Units Reimbursed (Billions)'\n",
    "            else:\n",
    "                values = df_top['Total_Prescriptions']\n",
    "                y_label = 'Total Prescriptions (Millions)'\n",
    "            \n",
    "            cumulative_pct = df_top['Cumulative_Percentage']\n",
    "            \n",
    "            # Create bar chart\n",
    "            bars = ax.bar(range(len(classes)), values, alpha=0.7, color='steelblue')\n",
    "            ax.set_xlabel('ATC2 Classes', fontsize=12)\n",
    "            ax.set_ylabel(y_label, fontsize=12, color='steelblue')\n",
    "            ax.tick_params(axis='y', labelcolor='steelblue')\n",
    "            ax.set_xticks(range(len(classes)))\n",
    "            ax.set_xticklabels(classes, rotation=90, ha='center', fontsize=8)\n",
    "            \n",
    "            # Add value labels on top 3 bars\n",
    "            for i in range(min(3, len(bars))):\n",
    "                height = bars[i].get_height()\n",
    "                ax.text(bars[i].get_x() + bars[i].get_width()/2., height,\n",
    "                       f'{height:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "            \n",
    "            # Create cumulative percentage line\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.plot(range(len(classes)), cumulative_pct, color='red', marker='o', linewidth=2, markersize=3)\n",
    "            ax2.set_ylabel('Cumulative Percentage (%)', fontsize=12, color='red')\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            ax2.set_ylim(0, 100)\n",
    "            \n",
    "            # Add 80% threshold line\n",
    "            ax2.axhline(y=80, color='red', linestyle='--', alpha=0.7, linewidth=1)\n",
    "            ax2.text(len(classes)*0.7, 82, '80%', color='red', fontweight='bold')\n",
    "            \n",
    "            # Title\n",
    "            ax.set_title(f'{geography} - {metric_name}\\n(Top {top_n} ATC2 Classes)', \n",
    "                        fontsize=12, fontweight='bold', pad=15)\n",
    "        \n",
    "        # Create all four Pareto charts\n",
    "        create_single_pareto(axes[0,0], indiana_units, 'Units', 'INDIANA', top_n)\n",
    "        create_single_pareto(axes[0,1], national_units, 'Units', 'NATIONAL', top_n)\n",
    "        create_single_pareto(axes[1,0], indiana_prescriptions, 'Prescriptions', 'INDIANA', top_n)\n",
    "        create_single_pareto(axes[1,1], national_prescriptions, 'Prescriptions', 'NATIONAL', top_n)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Helper function to print Pareto summary\n",
    "        def print_pareto_summary(df, metric_name, geography):\n",
    "            df_sorted = df.sort_values('Cumulative_Percentage').reset_index(drop=True)\n",
    "            threshold_idx = df_sorted[df_sorted['Cumulative_Percentage'] >= 80].index\n",
    "            \n",
    "            if len(threshold_idx) > 0:\n",
    "                classes_80 = df_sorted.iloc[:threshold_idx[0] + 1]\n",
    "            else:\n",
    "                classes_80 = df_sorted\n",
    "            \n",
    "            total_classes = len(df)\n",
    "            classes_for_80 = len(classes_80)\n",
    "            pct_classes_for_80 = (classes_for_80 / total_classes) * 100\n",
    "            \n",
    "            if metric_name == 'Units':\n",
    "                total_value = df['Total_Units'].sum()\n",
    "                value_80 = classes_80['Total_Units'].sum()\n",
    "            else:\n",
    "                total_value = df['Total_Prescriptions'].sum()\n",
    "                value_80 = classes_80['Total_Prescriptions'].sum()\n",
    "            \n",
    "            actual_pct_covered = (value_80 / total_value) * 100\n",
    "            \n",
    "            print(f\"\\n{geography} - {metric_name}:\")\n",
    "            print(f\"  Total ATC2 classes: {total_classes}\")\n",
    "            print(f\"  Classes needed for ~80%: {classes_for_80} ({pct_classes_for_80:.1f}% of classes)\")\n",
    "            print(f\"  Actual coverage: {actual_pct_covered:.1f}%\")\n",
    "            print(f\"  Total {metric_name.lower()}: {total_value:.3f}\")\n",
    "            print(f\"  Value in top classes: {value_80:.3f}\")\n",
    "            print(f\"  Top 5 classes:\")\n",
    "            for i, (_, row) in enumerate(df.head(5).iterrows(), 1):\n",
    "                value = row['Total_Units'] if metric_name == 'Units' else row['Total_Prescriptions']\n",
    "                print(f\"    {i}. {row['ATC_Class']}: {value:.3f} ({row['Percentage']:.1f}%)\")\n",
    "        \n",
    "        # Print summaries\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PARETO ANALYSIS SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print_pareto_summary(indiana_units, 'Units', 'INDIANA')\n",
    "        print_pareto_summary(national_units, 'Units', 'NATIONAL')\n",
    "        print_pareto_summary(indiana_prescriptions, 'Prescriptions', 'INDIANA')\n",
    "        print_pareto_summary(national_prescriptions, 'Prescriptions', 'NATIONAL')\n",
    "        \n",
    "        # Compare top classes\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"KEY INSIGHTS - Top 10 Classes Comparison\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        in_top_units = set(indiana_units.head(10)['ATC_Class'])\n",
    "        nat_top_units = set(national_units.head(10)['ATC_Class'])\n",
    "        in_top_presc = set(indiana_prescriptions.head(10)['ATC_Class'])\n",
    "        nat_top_presc = set(national_prescriptions.head(10)['ATC_Class'])\n",
    "        \n",
    "        units_overlap = in_top_units & nat_top_units\n",
    "        presc_overlap = in_top_presc & nat_top_presc\n",
    "        \n",
    "        print(f\"\\nUnits Reimbursed - Common classes: {len(units_overlap)}/10\")\n",
    "        print(f\"  Shared: {sorted(units_overlap)}\")\n",
    "        print(f\"  Only Indiana: {sorted(in_top_units - nat_top_units)}\")\n",
    "        print(f\"  Only National: {sorted(nat_top_units - in_top_units)}\")\n",
    "        \n",
    "        print(f\"\\nPrescriptions - Common classes: {len(presc_overlap)}/10\")\n",
    "        print(f\"  Shared: {sorted(presc_overlap)}\")\n",
    "        print(f\"  Only Indiana: {sorted(in_top_presc - nat_top_presc)}\")\n",
    "        print(f\"  Only National: {sorted(nat_top_presc - in_top_presc)}\")\n",
    "        \n",
    "        return {\n",
    "            'indiana_units': indiana_units,\n",
    "            'indiana_prescriptions': indiana_prescriptions,\n",
    "            'national_units': national_units,\n",
    "            'national_prescriptions': national_prescriptions\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "workflow_execution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATC4 file: 4,336,231 rows, 32,203 unique NDCs\n"
     ]
    }
   ],
   "source": [
    "processor = NDCATCProcessor(year=2024)\n",
    "processor.clean_sdud_data()           # Clean SDUD data\n",
    "processor.adding_key()                # Add record_id key\n",
    "#analyzer.generate_ndc_txt()          # Generate NDC text file\n",
    "processor.analyze_atc4_mapping() \n",
    "processor.fetch_atc_names() \n",
    "#processor.export_unscaled_data()\n",
    "\n",
    "processor.prepare_final_dataframe()   \n",
    "processor.export_merged_data()  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just checking overlap between files with and without key\n",
    "nokey_path=rf'C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\ATC4_classes\\Classes_notgood\\NDCf_2020_ATC4_classes.csv'\n",
    "keyed_path=rf'C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\ATC4_classes\\NDCNEW_2020_ATC4_classes.csv'\n",
    "# Load them\n",
    "keyed = pd.read_csv(keyed_path, dtype=str)\n",
    "nokey = pd.read_csv(nokey_path, dtype=str)\n",
    "\n",
    "# Normalize NDCs (remove hyphens, pad to 11 digits)\n",
    "for df in [keyed, nokey]:\n",
    "    df[\"NDC\"] = df[\"NDC\"].str.replace(\"-\", \"\", regex=False).str.zfill(11)\n",
    "\n",
    "# --- Summary stats ---\n",
    "summary = {\n",
    "    \"File\": [\"With key (NDCNEW_2024_ATC4_classes)\", \"Without key (NDCf_2024_ATC4_classes)\"],\n",
    "    \"Total rows\": [len(keyed), len(nokey)],\n",
    "    \"Unique NDCs\": [keyed[\"NDC\"].nunique(), nokey[\"NDC\"].nunique()],\n",
    "    \"Mapped NDCs (non-null ATC)\": [\n",
    "        keyed[\"ATC4 Class\"].notna().sum(),\n",
    "        nokey[\"ATC4 Class\"].notna().sum(),\n",
    "    ],\n",
    "}\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# --- Compare overlap of unique NDCs ---\n",
    "ndc_keyed = set(keyed[\"NDC\"].unique())\n",
    "ndc_nokey = set(nokey[\"NDC\"].unique())\n",
    "\n",
    "overlap_ndcs = len(ndc_keyed & ndc_nokey)\n",
    "only_in_nokey = len(ndc_nokey - ndc_keyed)\n",
    "only_in_keyed = len(ndc_keyed - ndc_nokey)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Metric\": [\"Overlap NDCs\", \"Only in without-key file\", \"Only in with-key file\", \"Percent overlap\"],\n",
    "    \"Value\": [overlap_ndcs, only_in_nokey, only_in_keyed, overlap_ndcs / len(ndc_nokey) * 100]\n",
    "})\n",
    "\n",
    "print(\"\\n=== Summary of Each File ===\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== NDC Overlap Comparison ===\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c36c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indiana only\n",
    "\n",
    "#Generating only one excel file for all the states\n",
    "#output_path = NDCATC_overview.export_cumulative_frequency_excel(years_list, by_state=False)  # This combines all states into one analysis\n",
    "#NDCATC_overview.export_cumulative_frequency_excel(years_list,  level='ATC2','ATC4'by_state=True,state_filter=['IN'])\n",
    "\n",
    "print(\"\\nComparing Indiana vs National...\")\n",
    "ind_vs_national = NDCATC_overview.compare_cumulative_80_analysis()\n",
    "\n",
    "ndc_pareto_results = NDCATC_overview.create_pareto_charts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e502d111",
   "metadata": {},
   "source": [
    "Doing the statistical analysis.\n",
    "The unscaled data is analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a33987ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDCATC_ind:  #Creating a new class to analyze correlations\n",
    "    \n",
    "    @staticmethod\n",
    "    def covariance_look(years_list, base_path=None, min_records=25):\n",
    "        \n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(\"COVARIANCE ANALYSIS: Units Reimbursed vs Number of Prescriptions\")\n",
    "        print(\"NATIONAL ANALYSIS - By ATC Class\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        all_within_class_cov = []\n",
    "        all_data_for_between = {}\n",
    "        \n",
    "        # Load and process all data\n",
    "        for year in years_list:\n",
    "            print(f\"Processing {year}...\", end=\" \")\n",
    "            try:\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\unscaled_data\\\\MUD_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                \n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                if records.empty:\n",
    "                    print(\"No valid records\")\n",
    "                    continue\n",
    "                \n",
    "                # Scale both variables by 1e6\n",
    "                records['Units Reimbursed'] = records['Units Reimbursed'] / 1e3\n",
    "                records['Number of Prescriptions'] = records['Number of Prescriptions'] / 1e3        \n",
    "\n",
    "                print(f\"✓ ({len(records):,} records)\")\n",
    "                all_data_for_between[year] = records\n",
    "                \n",
    "                # Calculate within-class covariances for this year\n",
    "                for atc2 in records['ATC2 Class'].unique():\n",
    "                    subset = records[records['ATC2 Class'] == atc2]\n",
    "                    if len(subset) >= min_records:\n",
    "                        try:\n",
    "                            # Calculate covariance\n",
    "                            cov_matrix = np.cov(subset['Units Reimbursed'], subset['Number of Prescriptions'])\n",
    "                            covariance = cov_matrix[0, 1]\n",
    "                            \n",
    "                            atc2_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns else ''\n",
    "                            \n",
    "                            all_within_class_cov.append({\n",
    "                                'Year': year,\n",
    "                                'ATC2_Class': atc2,\n",
    "                                'ATC2_Name': atc2_name,\n",
    "                                'N_Records': len(subset),\n",
    "                                'Within_Class_Covariance': covariance,\n",
    "                                'Mean_Units': subset['Units Reimbursed'].mean(),\n",
    "                                'Mean_Prescriptions': subset['Number of Prescriptions'].mean()\n",
    "                            })\n",
    "                        except Exception as e:\n",
    "                            print(f\"\\nError processing {atc2}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "        \n",
    "        within_class_df = pd.DataFrame(all_within_class_cov)\n",
    "        \n",
    "        if within_class_df.empty:\n",
    "            print(\"No within-class covariance data collected!\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "        \n",
    "        # ==================== WITHIN-CLASS ANALYSIS ====================\n",
    "        print(f\"\\n{'='*70}\\n1. WITHIN-CLASS COVARIANCE ANALYSIS\\n{'='*70}\")\n",
    "        print(f\"Total class-year combinations: {len(within_class_df):,}\")\n",
    "        print(f\"Years analyzed: {sorted(within_class_df['Year'].unique())}\")\n",
    "        print(f\"Unique ATC2 classes: {len(within_class_df['ATC2_Class'].unique())}\")\n",
    "        \n",
    "        print(f\"\\nOverall Within-Class Covariance Statistics:\")\n",
    "        print(f\"  Mean: {within_class_df['Within_Class_Covariance'].mean():,.10f}\")\n",
    "        print(f\"  Median: {within_class_df['Within_Class_Covariance'].median():,.10f}\")\n",
    "        print(f\"  Std Dev: {within_class_df['Within_Class_Covariance'].std():,.10f}\")\n",
    "        print(f\"  Range: {within_class_df['Within_Class_Covariance'].min():,.10f} to {within_class_df['Within_Class_Covariance'].max():,.10f}\")\n",
    "        \n",
    "        # Aggregate by ATC class across all years\n",
    "        atc_within_summary = within_class_df.groupby(['ATC2_Class', 'ATC2_Name']).agg({\n",
    "            'Within_Class_Covariance': ['mean', 'std'],\n",
    "            'N_Records': 'sum'\n",
    "        }).round(2)\n",
    "        atc_within_summary.columns = ['Avg_Within_Cov', 'Std_Within_Cov', 'Total_Records']\n",
    "        atc_within_summary = atc_within_summary.sort_values('Avg_Within_Cov', ascending=False)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\\nWITHIN-CLASS COVARIANCE BY ATC2 CLASS (Averaged across years)\\n{'='*70}\")\n",
    "        print(f\"{'ATC2':<5} {'Name':<30} {'Avg Cov':>15} {'Std':>12} {'Total N':>10}\")\n",
    "        print(\"-\" * 80)\n",
    "        for (atc_class, atc_name), row in atc_within_summary.head(15).iterrows():\n",
    "            name_short = atc_name[:28] if atc_name else atc_class\n",
    "            print(f\"{atc_class:<5} {name_short:<30} {row['Avg_Within_Cov']:>15,.10f} {row['Std_Within_Cov']:>12,.10f} {row['Total_Records']:>10,.0f}\")\n",
    "        \n",
    "        # ==================== BETWEEN-CLASS ANALYSIS ====================\n",
    "        print(f\"\\n{'='*70}\\n2. BETWEEN-CLASS COVARIANCE ANALYSIS\\n{'='*70}\")\n",
    "        \n",
    "        between_class_results = []\n",
    "        \n",
    "        for year in sorted(all_data_for_between.keys()):\n",
    "            year_data = all_data_for_between[year]\n",
    "            \n",
    "            # Get class means for this year\n",
    "            class_means = year_data.groupby('ATC2 Class').agg({\n",
    "                'Units Reimbursed': 'mean',\n",
    "                'Number of Prescriptions': 'mean',\n",
    "                'ATC2_Name': 'first'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Filter classes with sufficient records\n",
    "            class_counts = year_data['ATC2 Class'].value_counts()\n",
    "            valid_classes = class_counts[class_counts >= min_records].index\n",
    "            class_means = class_means[class_means['ATC2 Class'].isin(valid_classes)]\n",
    "            \n",
    "            if len(class_means) < 2:\n",
    "                print(f\"{year}: Not enough classes for between-class analysis\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate between-class covariance\n",
    "            between_cov = np.cov(\n",
    "                class_means['Units Reimbursed'],\n",
    "                class_means['Number of Prescriptions']\n",
    "            )[0, 1]\n",
    "            \n",
    "            between_class_results.append({\n",
    "                'Year': year,\n",
    "                'Between_Class_Covariance': between_cov,\n",
    "                'N_Classes': len(class_means),\n",
    "                'Total_Records': len(year_data)\n",
    "            })\n",
    "            \n",
    "            print(f\"Year {year}: Between-Class Cov = {between_cov:>15,.10f} (across {len(class_means)} classes, {len(year_data):,} records)\")\n",
    "        \n",
    "        between_class_df = pd.DataFrame(between_class_results)\n",
    "        \n",
    "        if not between_class_df.empty:\n",
    "            print(f\"\\n{'='*70}\\nBETWEEN-CLASS COVARIANCE SUMMARY\\n{'='*70}\")\n",
    "            print(f\"Average across years: {between_class_df['Between_Class_Covariance'].mean():,.10f}\")\n",
    "            print(f\"Median: {between_class_df['Between_Class_Covariance'].median():,.10f}\")\n",
    "            print(f\"Std Dev: {between_class_df['Between_Class_Covariance'].std():,.10f}\")\n",
    "            print(f\"Range: {between_class_df['Between_Class_Covariance'].min():,.10f} to {between_class_df['Between_Class_Covariance'].max():,.10f}\")\n",
    "            \n",
    "            # Overall covariance (all data pooled across all classes and years)\n",
    "            print(f\"\\n{'='*70}\\n3. OVERALL COVARIANCE (All classes and years combined)\\n{'='*70}\")\n",
    "            all_combined = pd.concat(all_data_for_between.values(), ignore_index=True)\n",
    "            overall_cov = np.cov(\n",
    "                all_combined['Units Reimbursed'],\n",
    "                all_combined['Number of Prescriptions']\n",
    "            )[0, 1]\n",
    "            print(f\"Overall Covariance: {overall_cov:,.10f} ({len(all_combined):,} total records)\")\n",
    "            \n",
    "            # Comparison\n",
    "            print(f\"\\n{'='*70}\\nCOMPARISON\\n{'='*70}\")\n",
    "            avg_within = within_class_df['Within_Class_Covariance'].mean()\n",
    "            avg_between = between_class_df['Between_Class_Covariance'].mean()\n",
    "            print(f\"Average Within-Class Covariance:  {avg_within:>15,.10f}\")\n",
    "            print(f\"Average Between-Class Covariance: {avg_between:>15,.10f}\")\n",
    "            print(f\"Overall Covariance:               {overall_cov:>15,.10f}\")\n",
    "            print(f\"\\nRatio (Between/Within):           {avg_between/avg_within if avg_within != 0 else 'undefined':>15.4f}\")\n",
    "        \n",
    "        return within_class_df, between_class_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def correlation_look(years_list, base_path=None, min_records=25, state_filter=None, include_national=True):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        results = {}\n",
    "        # STATE ANALYSIS\n",
    "        if state_filter is not None:\n",
    "            state_scope = f\"{state_filter}\" if isinstance(state_filter, str) else f\"{', '.join(state_filter)}\"\n",
    "            print(\"CORRELATION ANALYSIS: Units Reimbursed vs Number of Prescriptions\")\n",
    "            print(f\"{state_scope.upper()} ONLY - By ATC Class\")\n",
    "            print(\"=\"*70)\n",
    "            \n",
    "            state_correlations = []\n",
    "            \n",
    "            for year in years_list:\n",
    "                print(f\"Processing {year} ({state_scope})...\", end=\" \")\n",
    "                try:\n",
    "                    csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                    df_merged = pd.read_csv(csv_path)\n",
    "                    \n",
    "                    # Apply state filter\n",
    "                    if isinstance(state_filter, str):\n",
    "                        records = df_merged[(df_merged['ATC4 Class'].notna()) & (df_merged['State'] == state_filter)].copy()\n",
    "                    else:  # list of states\n",
    "                        records = df_merged[(df_merged['ATC4 Class'].notna()) & (df_merged['State'].isin(state_filter))].copy()\n",
    "                    \n",
    "                    if records.empty:\n",
    "                        print(f\"No valid {state_scope} records\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"✓ ({len(records):,} records)\")\n",
    "                    \n",
    "                    for atc2 in records['ATC2 Class'].unique():\n",
    "                        subset = records[records['ATC2 Class'] == atc2]\n",
    "                        if len(subset) >= min_records:\n",
    "                            try:\n",
    "                                pearson_r, pearson_p = pearsonr(subset['Units Reimbursed'], subset['Number of Prescriptions'])\n",
    "                                spearman_r, spearman_p = spearmanr(subset['Units Reimbursed'], subset['Number of Prescriptions'])\n",
    "                                atc2_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns else ''\n",
    "                                \n",
    "                                state_correlations.append({\n",
    "                                    'Year': year, 'State': state_scope, 'ATC2_Class': atc2, 'ATC2_Name': atc2_name,\n",
    "                                    'N_Records': len(subset), 'Pearson_r': pearson_r, 'Pearson_p': pearson_p,\n",
    "                                    'Spearman_r': spearman_r, 'Spearman_p': spearman_p\n",
    "                                })\n",
    "                            except Exception as e:\n",
    "                                print(f\"\\nError processing {state_scope}-{atc2}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Error: {e}\")\n",
    "            \n",
    "            state_correlations_df = pd.DataFrame(state_correlations)\n",
    "            if not state_correlations_df.empty:\n",
    "                results['state_specific'] = state_correlations_df\n",
    "                \n",
    "                # STATE-SPECIFIC SUMMARY STATISTICS\n",
    "                print(f\"\\n{'='*70}\\nSUMMARY STATISTICS FOR {state_scope.upper()}\\n{'='*70}\")\n",
    "                print(f\"Total combinations: {len(state_correlations_df):,} | Years: {sorted(state_correlations_df['Year'].unique())} | ATC classes: {len(state_correlations_df['ATC2_Class'].unique())}\")\n",
    "                \n",
    "                # STATE-SPECIFIC PEARSON RESULTS\n",
    "                print(f\"\\n{'='*70}\\nPEARSON CORRELATION RESULTS - {state_scope.upper()}\\n{'='*70}\")\n",
    "                print(f\"Average: {state_correlations_df['Pearson_r'].mean():.4f} | Range: {state_correlations_df['Pearson_r'].min():.4f} to {state_correlations_df['Pearson_r'].max():.4f} | Std Dev: {state_correlations_df['Pearson_r'].std():.4f}\")\n",
    "                \n",
    "                atc_pearson_state = state_correlations_df.groupby(['ATC2_Class', 'ATC2_Name']).agg({\n",
    "                    'Pearson_r': ['mean', 'std'], 'Pearson_p': 'mean', 'N_Records': 'sum'\n",
    "                }).round(4)\n",
    "                atc_pearson_state.columns = ['Avg_Pearson', 'Std_Pearson', 'Avg_P_Value', 'Total_Records']\n",
    "                atc_pearson_state = atc_pearson_state.sort_values('Avg_Pearson', ascending=False)\n",
    "                \n",
    "                print(f\"\\nPEARSON BY ATC CLASS (Average across years) - {state_scope.upper()}:\")\n",
    "                print(f\"{'ATC2':<5} {'Name':<25} {'Avg Pearson':<12} {'Std':<8} {'p-val':<8} {'Total N':<8}\\n{'-' * 75}\")\n",
    "                for (atc_class, atc_name), row in atc_pearson_state.head(10).iterrows():\n",
    "                    name_short = atc_name[:23] if atc_name else atc_class\n",
    "                    print(f\"{atc_class:<5} {name_short:<25} {row['Avg_Pearson']:<12.4f} {row['Std_Pearson']:<8.4f} {row['Avg_P_Value']:<8.4f} {row['Total_Records']:<8.0f}\")\n",
    "                \n",
    "                # STATE-SPECIFIC SPEARMAN RESULTS\n",
    "                print(f\"\\n{'='*70}\\nSPEARMAN RANK CORRELATION RESULTS - {state_scope.upper()}\\n{'='*70}\")\n",
    "                print(f\"Average: {state_correlations_df['Spearman_r'].mean():.4f} | Range: {state_correlations_df['Spearman_r'].min():.4f} to {state_correlations_df['Spearman_r'].max():.4f} | Std Dev: {state_correlations_df['Spearman_r'].std():.4f}\")\n",
    "            else:\n",
    "                print(f\"No correlation data collected for {state_scope}!\")\n",
    "                results['state_specific'] = pd.DataFrame()\n",
    "                \n",
    "        # NATIONAL ANALYSIS\n",
    "        if include_national:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(\"CORRELATION ANALYSIS: Units Reimbursed vs Number of Prescriptions\")\n",
    "            print(\"NATIONAL (ALL STATES) - By ATC Class\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            national_correlations = []\n",
    "            \n",
    "            for year in years_list:\n",
    "                print(f\"Processing {year} (National)...\", end=\" \")\n",
    "                try:\n",
    "                    csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                    df_merged = pd.read_csv(csv_path)\n",
    "                    \n",
    "                    # Use all states\n",
    "                    records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                    \n",
    "                    if records.empty:\n",
    "                        print(\"No valid national records\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"✓ ({len(records):,} records)\")\n",
    "                    \n",
    "                    for atc2 in records['ATC2 Class'].unique():\n",
    "                        subset = records[records['ATC2 Class'] == atc2]\n",
    "                        if len(subset) >= min_records:\n",
    "                            try:\n",
    "                                pearson_r, pearson_p = pearsonr(subset['Units Reimbursed'], subset['Number of Prescriptions'])\n",
    "                                spearman_r, spearman_p = spearmanr(subset['Units Reimbursed'], subset['Number of Prescriptions'])\n",
    "                                atc2_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns else ''\n",
    "                                \n",
    "                                national_correlations.append({\n",
    "                                    'Year': year, 'Scope': 'National', 'ATC2_Class': atc2, 'ATC2_Name': atc2_name,\n",
    "                                    'N_Records': len(subset), 'Pearson_r': pearson_r, 'Pearson_p': pearson_p,\n",
    "                                    'Spearman_r': spearman_r, 'Spearman_p': spearman_p\n",
    "                                })\n",
    "                            except Exception as e:\n",
    "                                print(f\"\\nError processing National-{atc2}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Error: {e}\")\n",
    "            \n",
    "            national_correlations_df = pd.DataFrame(national_correlations)\n",
    "            if not national_correlations_df.empty:\n",
    "                results['national'] = national_correlations_df\n",
    "                \n",
    "                # NATIONAL SUMMARY STATISTICS\n",
    "                print(f\"\\n{'='*70}\\nSUMMARY STATISTICS FOR NATIONAL\\n{'='*70}\")\n",
    "                print(f\"Total combinations: {len(national_correlations_df):,} | Years: {sorted(national_correlations_df['Year'].unique())} | ATC classes: {len(national_correlations_df['ATC2_Class'].unique())}\")\n",
    "                \n",
    "                # NATIONAL PEARSON RESULTS\n",
    "                print(f\"\\n{'='*70}\\nPEARSON CORRELATION RESULTS - NATIONAL\\n{'='*70}\")\n",
    "                print(f\"Average: {national_correlations_df['Pearson_r'].mean():.4f} | Range: {national_correlations_df['Pearson_r'].min():.4f} to {national_correlations_df['Pearson_r'].max():.4f} | Std Dev: {national_correlations_df['Pearson_r'].std():.4f}\")\n",
    "                \n",
    "                atc_pearson_national = national_correlations_df.groupby(['ATC2_Class', 'ATC2_Name']).agg({\n",
    "                    'Pearson_r': ['mean', 'std'], 'Pearson_p': 'mean', 'N_Records': 'sum'\n",
    "                }).round(4)\n",
    "                atc_pearson_national.columns = ['Avg_Pearson', 'Std_Pearson', 'Avg_P_Value', 'Total_Records']\n",
    "                atc_pearson_national = atc_pearson_national.sort_values('Avg_Pearson', ascending=False)\n",
    "                \n",
    "                print(f\"\\nPEARSON BY ATC CLASS (Average across years) - NATIONAL:\")\n",
    "                print(f\"{'ATC2':<5} {'Name':<25} {'Avg Pearson':<12} {'Std':<8} {'p-val':<8} {'Total N':<8}\\n{'-' * 75}\")\n",
    "                for (atc_class, atc_name), row in atc_pearson_national.head(10).iterrows():\n",
    "                    name_short = atc_name[:23] if atc_name else atc_class\n",
    "                    print(f\"{atc_class:<5} {name_short:<25} {row['Avg_Pearson']:<12.4f} {row['Std_Pearson']:<8.4f} {row['Avg_P_Value']:<8.4f} {row['Total_Records']:<8.0f}\")\n",
    "                \n",
    "                # NATIONAL SPEARMAN RESULTS\n",
    "                print(f\"\\n{'='*70}\\nSPEARMAN RANK CORRELATION RESULTS - NATIONAL\\n{'='*70}\")\n",
    "                print(f\"Average: {national_correlations_df['Spearman_r'].mean():.4f} | Range: {national_correlations_df['Spearman_r'].min():.4f} to {national_correlations_df['Spearman_r'].max():.4f} | Std Dev: {national_correlations_df['Spearman_r'].std():.4f}\")\n",
    "                \n",
    "                # COMPARISON SECTION (if both analyses were performed)\n",
    "                if state_filter is not None and 'state_specific' in results and not results['state_specific'].empty:\n",
    "                    print(f\"\\n{'='*80}\")\n",
    "                    print(f\"COMPARISON: {state_scope.upper()} vs NATIONAL\")\n",
    "                    print(f\"{'='*80}\")\n",
    "                    \n",
    "                    state_avg_pearson = results['state_specific']['Pearson_r'].mean()\n",
    "                    national_avg_pearson = national_correlations_df['Pearson_r'].mean()\n",
    "                    \n",
    "                    print(f\"{state_scope.upper()} Average Pearson Correlation:  {state_avg_pearson:.4f}\")\n",
    "                    print(f\"National Average Pearson Correlation: {national_avg_pearson:.4f}\")\n",
    "                    print(f\"Difference (National - {state_scope.upper()}):         {national_avg_pearson - state_avg_pearson:.4f}\")\n",
    "                    \n",
    "                    # Find common ATC classes for detailed comparison\n",
    "                    state_classes = set(results['state_specific']['ATC2_Class'].unique())\n",
    "                    national_classes = set(national_correlations_df['ATC2_Class'].unique())\n",
    "                    common_classes = state_classes.intersection(national_classes)\n",
    "                    \n",
    "                    if common_classes:\n",
    "                        print(f\"\\nDETAILED COMPARISON (Common ATC Classes):\")\n",
    "                        print(f\"{'ATC2':<5} {'Name':<20} {f'{state_scope} Pearson':<12} {'National Pearson':<16} {'Difference':<10}\")\n",
    "                        print(\"-\" * 75)\n",
    "                        \n",
    "                        for atc_class in sorted(list(common_classes))[:10]:  # Top 10\n",
    "                            state_avg = results['state_specific'][results['state_specific']['ATC2_Class'] == atc_class]['Pearson_r'].mean()\n",
    "                            national_avg = national_correlations_df[national_correlations_df['ATC2_Class'] == atc_class]['Pearson_r'].mean()\n",
    "                            diff = national_avg - state_avg\n",
    "                            atc_name = national_correlations_df[national_correlations_df['ATC2_Class'] == atc_class]['ATC2_Name'].iloc[0]\n",
    "                            name_short = atc_name[:18] if atc_name else atc_class\n",
    "                            \n",
    "                            print(f\"{atc_class:<5} {name_short:<20} {state_avg:<12.4f} {national_avg:<16.4f} {diff:<10.4f}\")\n",
    "                \n",
    "                # YEAR-OVER-YEAR TRENDS (National)\n",
    "                print(f\"\\n{'='*70}\")\n",
    "                print(\"YEAR-OVER-YEAR TRENDS - NATIONAL (Top 3 by Pearson)\")\n",
    "                print(f\"{'='*70}\")\n",
    "                \n",
    "                top_classes = atc_pearson_national.head(3).index.get_level_values(0).tolist()\n",
    "                for atc_class in top_classes:\n",
    "                    atc_data = national_correlations_df[national_correlations_df['ATC2_Class'] == atc_class].sort_values('Year')\n",
    "                    atc_name = atc_data['ATC2_Name'].iloc[0] if not atc_data.empty else atc_class\n",
    "                    \n",
    "                    print(f\"\\n{atc_class} - {atc_name[:30]}:\")\n",
    "                    print(f\"{'Year':<6} {'Pearson':<8} {'Spearman':<9} {'N':<6}\")\n",
    "                    print(\"-\" * 35)\n",
    "                    for _, row in atc_data.iterrows():\n",
    "                        print(f\"{row['Year']:<6} {row['Pearson_r']:<8.4f} {row['Spearman_r']:<9.4f} {row['N_Records']:<6}\")\n",
    "            else:\n",
    "                print(\"No national correlation data collected!\")\n",
    "                results['national'] = pd.DataFrame()\n",
    "        \n",
    "        # Ensure at least empty DataFrames are returned for missing analyses\n",
    "        if 'state_specific' not in results:\n",
    "            results['state_specific'] = pd.DataFrame()\n",
    "        if 'national' not in results:\n",
    "            results['national'] = pd.DataFrame()\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_units_vs_prescriptions_by_atc(years_list, base_path=None, min_records=25, include_negative=True):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(\"Creating plots for Indiana ATC classes...\")\n",
    "        \n",
    "        # Combine all years of data\n",
    "        all_data = []\n",
    "        for year in years_list:\n",
    "            try:\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                records = df_merged[(df_merged['ATC4 Class'].notna()) & (df_merged['State'] == 'IN')].copy()\n",
    "                records['Year'] = year\n",
    "                all_data.append(records)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {year}: {e}\")\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"No data loaded!\")\n",
    "            return\n",
    "        \n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # Get ATC classes with sufficient data and calculate correlations\n",
    "        atc_counts = combined_df['ATC2 Class'].value_counts()\n",
    "        sufficient_data_classes = atc_counts[atc_counts >= min_records].index\n",
    "        \n",
    "        # Calculate correlations for all classes with sufficient data\n",
    "        class_correlations = {}\n",
    "        for atc_class in sufficient_data_classes:\n",
    "            subset = combined_df[combined_df['ATC2 Class'] == atc_class]\n",
    "            if len(subset) > 1:\n",
    "                corr = subset['Number of Prescriptions'].corr(subset['Units Reimbursed'])\n",
    "                class_correlations[atc_class] = corr\n",
    "        \n",
    "        # Select classes to plot\n",
    "        if include_negative:\n",
    "            # Get top positive correlations and all negative correlations\n",
    "            positive_corrs = {k: v for k, v in class_correlations.items() if v >= 0}\n",
    "            negative_corrs = {k: v for k, v in class_correlations.items() if v < 0}\n",
    "            \n",
    "            # Sort positive by correlation (descending) and negative by correlation (ascending, most negative first)\n",
    "            positive_sorted = sorted(positive_corrs.items(), key=lambda x: x[1], reverse=True)\n",
    "            negative_sorted = sorted(negative_corrs.items(), key=lambda x: x[1])\n",
    "            \n",
    "            # Take top 8 positive and all negative (up to 4 more)\n",
    "            selected_positive = [x[0] for x in positive_sorted[:8]]\n",
    "            selected_negative = [x[0] for x in negative_sorted[:4]]\n",
    "            \n",
    "            valid_atc_classes = selected_positive + selected_negative\n",
    "            \n",
    "            print(f\"\\nSelected classes: {len(selected_positive)} positive correlations + {len(selected_negative)} negative correlations\")\n",
    "            if selected_negative:\n",
    "                print(f\"Negative correlation classes: {selected_negative}\")\n",
    "        else:\n",
    "            # Original behavior - top classes by count\n",
    "            valid_atc_classes = sufficient_data_classes[:12]\n",
    "        \n",
    "        # Determine grid size based on number of classes\n",
    "        n_classes = len(valid_atc_classes)\n",
    "        if n_classes <= 6:\n",
    "            rows, cols = 2, 3\n",
    "        elif n_classes <= 9:\n",
    "            rows, cols = 3, 3\n",
    "        elif n_classes <= 12:\n",
    "            rows, cols = 3, 4\n",
    "        else:\n",
    "            rows, cols = 4, 4\n",
    "            valid_atc_classes = valid_atc_classes[:16]  # Limit to 16 for display\n",
    "        \n",
    "        # Set up the plot grid\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols*5, rows*4))\n",
    "        if rows == 1 or cols == 1:\n",
    "            axes = axes.flatten() if hasattr(axes, 'flatten') else [axes]\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(valid_atc_classes)))\n",
    "        \n",
    "        for i, atc_class in enumerate(valid_atc_classes):\n",
    "            subset = combined_df[combined_df['ATC2 Class'] == atc_class]\n",
    "            atc_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns and not subset['ATC2_Name'].isna().all() else atc_class\n",
    "            \n",
    "            # Create scatter plot\n",
    "            axes[i].scatter(subset['Number of Prescriptions'], \n",
    "                           subset['Units Reimbursed'], \n",
    "                           alpha=0.6, color=colors[i], s=20)\n",
    "            \n",
    "            # Add trend line\n",
    "            if len(subset) > 1:\n",
    "                z = np.polyfit(subset['Number of Prescriptions'], subset['Units Reimbursed'], 1)\n",
    "                p = np.poly1d(z)\n",
    "                axes[i].plot(subset['Number of Prescriptions'], p(subset['Number of Prescriptions']), \n",
    "                            \"r--\", alpha=0.8, linewidth=1)\n",
    "            \n",
    "            # Format axes\n",
    "            axes[i].set_xlabel('Number of Prescriptions')\n",
    "            axes[i].set_ylabel('Units Reimbursed')\n",
    "            axes[i].set_title(f'{atc_class}\\n{atc_name[:30]}', fontsize=10)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add correlation coefficient with color coding\n",
    "            if len(subset) > 1:\n",
    "                corr = subset['Number of Prescriptions'].corr(subset['Units Reimbursed'])\n",
    "                color = 'red' if corr < 0 else 'blue'\n",
    "                axes[i].text(0.05, 0.95, f'r = {corr:.3f}', \n",
    "                            transform=axes[i].transAxes, \n",
    "                            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8, edgecolor=color),\n",
    "                            fontsize=9, color=color)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for j in range(len(valid_atc_classes), len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        title_suffix = \" (Including Negative Correlations)\" if include_negative else \"\"\n",
    "        plt.suptitle(f'Indiana: Units Reimbursed vs Number of Prescriptions by ATC2 Class{title_suffix}\\n(All Years Combined)', \n",
    "                     fontsize=16, y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary table\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PLOT SUMMARY - INDIANA ATC CLASSES\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"{'ATC2':<5} {'Name':<30} {'Records':<8} {'Correlation':<12} {'Type':<8}\")\n",
    "        print(\"-\" * 90)\n",
    "        \n",
    "        for atc_class in valid_atc_classes:\n",
    "            subset = combined_df[combined_df['ATC2 Class'] == atc_class]\n",
    "            atc_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns and not subset['ATC2_Name'].isna().all() else atc_class\n",
    "            corr = subset['Number of Prescriptions'].corr(subset['Units Reimbursed'])\n",
    "            corr_type = \"Negative\" if corr < 0 else \"Positive\"\n",
    "            \n",
    "            print(f\"{atc_class:<5} {atc_name[:28]:<30} {len(subset):<8} {corr:<12.4f} {corr_type:<8}\")\n",
    "        \n",
    "        return combined_df[combined_df['ATC2 Class'].isin(valid_atc_classes)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def runs_test_analysis(years_list, base_path=None, min_records=25):\n",
    "        \n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(\"RUNS TEST ANALYSIS: Testing for Randomness in Units Reimbursed and Number of Prescriptions\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        all_results = {\n",
    "            'state_atc2': [],\n",
    "            'national_atc2': [], \n",
    "            'atc2_quarter': []\n",
    "        }\n",
    "        \n",
    "        # Load all data first\n",
    "        all_data = {}\n",
    "        for year in years_list:\n",
    "            print(f\"Loading {year} data...\", end=\" \")\n",
    "            try:\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                records['ATC2 Class'] = records['ATC4 Class'].str[:3]\n",
    "                all_data[year] = records\n",
    "                print(f\"✓ ({len(records):,} records)\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PANEL 1: STATE x ATC2 CLASSES RUNS TEST\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Panel 1: State x ATC2 classes\n",
    "        for year in years_list:\n",
    "            if year not in all_data:\n",
    "                continue\n",
    "                \n",
    "            records = all_data[year]\n",
    "            \n",
    "            for state in records['State'].unique():\n",
    "                state_data = records[records['State'] == state]\n",
    "                \n",
    "                for atc2 in state_data['ATC2 Class'].unique():\n",
    "                    subset = state_data[state_data['ATC2 Class'] == atc2]\n",
    "                    \n",
    "                    if len(subset) >= min_records:\n",
    "                        try:\n",
    "                            # Get ATC2 name\n",
    "                            atc2_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns else ''\n",
    "                            \n",
    "                            # Sort by Year and Quarter to create sequence\n",
    "                            subset_sorted = subset.sort_values(['Year', 'Quarter'])\n",
    "                            \n",
    "                            # Test Units Reimbursed\n",
    "                            units_values = subset_sorted['Units Reimbursed'].values\n",
    "                            units_median = np.median(units_values)\n",
    "                            units_binary = (units_values > units_median).astype(int)\n",
    "                            \n",
    "                            if len(np.unique(units_binary)) > 1:  # Need both 0s and 1s\n",
    "                                runs_stat_units, p_val_units = runstest_1samp(units_binary)\n",
    "                            else:\n",
    "                                runs_stat_units, p_val_units = np.nan, np.nan\n",
    "                            \n",
    "                            # Test Number of Prescriptions  \n",
    "                            presc_values = subset_sorted['Number of Prescriptions'].values\n",
    "                            presc_median = np.median(presc_values)\n",
    "                            presc_binary = (presc_values > presc_median).astype(int)\n",
    "                            \n",
    "                            if len(np.unique(presc_binary)) > 1:\n",
    "                                runs_stat_presc, p_val_presc = runstest_1samp(presc_binary)\n",
    "                            else:\n",
    "                                runs_stat_presc, p_val_presc = np.nan, np.nan\n",
    "                            \n",
    "                            all_results['state_atc2'].append({\n",
    "                                'Year': year,\n",
    "                                'State': state,\n",
    "                                'ATC2_Class': atc2,\n",
    "                                'ATC2_Name': atc2_name,\n",
    "                                'N_Records': len(subset),\n",
    "                                'Units_Runs_Stat': runs_stat_units,\n",
    "                                'Units_P_Value': p_val_units,\n",
    "                                'Presc_Runs_Stat': runs_stat_presc,\n",
    "                                'Presc_P_Value': p_val_presc\n",
    "                            })\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing {state}-{atc2}: {e}\")\n",
    "        \n",
    "        # Convert to DataFrame and display results\n",
    "        state_atc2_df = pd.DataFrame(all_results['state_atc2'])\n",
    "        if not state_atc2_df.empty:\n",
    "            print(f\"\\nState x ATC2 Results: {len(state_atc2_df)} combinations analyzed\")\n",
    "            \n",
    "            # Summary statistics\n",
    "            units_significant = (state_atc2_df['Units_P_Value'] < 0.05).sum()\n",
    "            presc_significant = (state_atc2_df['Presc_P_Value'] < 0.05).sum() \n",
    "            total_valid = state_atc2_df['Units_P_Value'].notna().sum()\n",
    "            \n",
    "            print(f\"Units Reimbursed - Significant non-randomness: {units_significant}/{total_valid} ({100*units_significant/total_valid if total_valid > 0 else 0:.1f}%)\")\n",
    "            print(f\"Number of Prescriptions - Significant non-randomness: {presc_significant}/{total_valid} ({100*presc_significant/total_valid if total_valid > 0 else 0:.1f}%)\")\n",
    "            \n",
    "            # Top significant results\n",
    "            print(f\"\\nTop 10 Most Significant Units Reimbursed Results (State x ATC2):\")\n",
    "            top_units = state_atc2_df.nsmallest(10, 'Units_P_Value')[['State', 'ATC2_Class', 'ATC2_Name', 'Units_Runs_Stat', 'Units_P_Value', 'N_Records']].copy()\n",
    "            top_units['Units_P_Value'] = top_units['Units_P_Value'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(top_units.to_string(index=False))\n",
    "            \n",
    "            print(f\"\\nTop 10 Most Significant Number of Prescriptions Results (State x ATC2):\")\n",
    "            top_presc = state_atc2_df.nsmallest(10, 'Presc_P_Value')[['State', 'ATC2_Class', 'ATC2_Name', 'Presc_Runs_Stat', 'Presc_P_Value', 'N_Records']].copy()\n",
    "            top_presc['Presc_P_Value'] = top_presc['Presc_P_Value'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(top_presc.to_string(index=False))\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PANEL 2: NATIONAL x ATC2 CLASSES RUNS TEST\") \n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Panel 2: National x ATC2 classes (aggregate all states per year)\n",
    "        for year in years_list:\n",
    "            if year not in all_data:\n",
    "                continue\n",
    "                \n",
    "            records = all_data[year]\n",
    "            \n",
    "            for atc2 in records['ATC2 Class'].unique():\n",
    "                subset = records[records['ATC2 Class'] == atc2]\n",
    "                \n",
    "                if len(subset) >= min_records:\n",
    "                    try:\n",
    "                        atc2_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns else ''\n",
    "                        \n",
    "                        # Sort by Year and Quarter to create a systematic sequence\n",
    "                        subset_sorted = subset.sort_values(['Year', 'Quarter'])\n",
    "                        \n",
    "                        # Test Units Reimbursed\n",
    "                        units_values = subset_sorted['Units Reimbursed'].values\n",
    "                        units_median = np.median(units_values)\n",
    "                        units_binary = (units_values > units_median).astype(int)\n",
    "                        \n",
    "                        if len(np.unique(units_binary)) > 1:\n",
    "                            runs_stat_units, p_val_units = runstest_1samp(units_binary)\n",
    "                        else:\n",
    "                            runs_stat_units, p_val_units = np.nan, np.nan\n",
    "                        \n",
    "                        # Test Number of Prescriptions\n",
    "                        presc_values = subset_sorted['Number of Prescriptions'].values\n",
    "                        presc_median = np.median(presc_values)\n",
    "                        presc_binary = (presc_values > presc_median).astype(int)\n",
    "                        \n",
    "                        if len(np.unique(presc_binary)) > 1:\n",
    "                            runs_stat_presc, p_val_presc = runstest_1samp(presc_binary)\n",
    "                        else:\n",
    "                            runs_stat_presc, p_val_presc = np.nan, np.nan\n",
    "                        \n",
    "                        all_results['national_atc2'].append({\n",
    "                            'Year': year,\n",
    "                            'ATC2_Class': atc2,\n",
    "                            'ATC2_Name': atc2_name,\n",
    "                            'N_Records': len(subset),\n",
    "                            'N_States': subset['State'].nunique(),\n",
    "                            'Units_Runs_Stat': runs_stat_units,\n",
    "                            'Units_P_Value': p_val_units,\n",
    "                            'Presc_Runs_Stat': runs_stat_presc,\n",
    "                            'Presc_P_Value': p_val_presc\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing National-{atc2}: {e}\")\n",
    "        \n",
    "        # Display National x ATC2 results\n",
    "        national_atc2_df = pd.DataFrame(all_results['national_atc2'])\n",
    "        if not national_atc2_df.empty:\n",
    "            print(f\"\\nNational x ATC2 Results: {len(national_atc2_df)} combinations analyzed\")\n",
    "            \n",
    "            units_significant = (national_atc2_df['Units_P_Value'] < 0.05).sum()\n",
    "            presc_significant = (national_atc2_df['Presc_P_Value'] < 0.05).sum()\n",
    "            total_valid = national_atc2_df['Units_P_Value'].notna().sum()\n",
    "            \n",
    "            print(f\"Units Reimbursed - Significant non-randomness: {units_significant}/{total_valid} ({100*units_significant/total_valid if total_valid > 0 else 0:.1f}%)\")\n",
    "            print(f\"Number of Prescriptions - Significant non-randomness: {presc_significant}/{total_valid} ({100*presc_significant/total_valid if total_valid > 0 else 0:.1f}%)\")\n",
    "            \n",
    "            print(f\"\\nTop 10 Most Significant Units Reimbursed Results (National x ATC2):\")\n",
    "            top_units_nat = national_atc2_df.nsmallest(10, 'Units_P_Value')[['Year', 'ATC2_Class', 'ATC2_Name', 'Units_Runs_Stat', 'Units_P_Value', 'N_Records']].copy()\n",
    "            top_units_nat['Units_P_Value'] = top_units_nat['Units_P_Value'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(top_units_nat.to_string(index=False))\n",
    "            \n",
    "            print(f\"\\nTop 10 Most Significant Number of Prescriptions Results (National x ATC2):\")\n",
    "            top_presc_nat = national_atc2_df.nsmallest(10, 'Presc_P_Value')[['Year', 'ATC2_Class', 'ATC2_Name', 'Presc_Runs_Stat', 'Presc_P_Value', 'N_Records']].copy()\n",
    "            top_presc_nat['Presc_P_Value'] = top_presc_nat['Presc_P_Value'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(top_presc_nat.to_string(index=False))\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PANEL 3: ATC2 CLASSES x QUARTER RUNS TEST\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Panel 3: ATC2 classes x Quarter (aggregate all years)\n",
    "        # Combine all years first\n",
    "        all_combined = []\n",
    "        for year, records in all_data.items():\n",
    "            records_copy = records.copy()\n",
    "            records_copy['Year'] = year\n",
    "            all_combined.append(records_copy)\n",
    "        \n",
    "        if all_combined:\n",
    "            combined_df = pd.concat(all_combined, ignore_index=True)\n",
    "            \n",
    "            for atc2 in combined_df['ATC2 Class'].unique():\n",
    "                atc2_data = combined_df[combined_df['ATC2 Class'] == atc2]\n",
    "                \n",
    "                for quarter in sorted(atc2_data['Quarter'].unique()):\n",
    "                    subset = atc2_data[atc2_data['Quarter'] == quarter]\n",
    "                    \n",
    "                    if len(subset) >= min_records:\n",
    "                        try:\n",
    "                            atc2_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns else ''\n",
    "                            \n",
    "                            # Sort by Year and Quarter\n",
    "                            subset_sorted = subset.sort_values(['Year', 'Quarter'])\n",
    "                            \n",
    "                            # Test Units Reimbursed\n",
    "                            units_values = subset_sorted['Units Reimbursed'].values\n",
    "                            units_median = np.median(units_values)\n",
    "                            units_binary = (units_values > units_median).astype(int)\n",
    "                            \n",
    "                            if len(np.unique(units_binary)) > 1:\n",
    "                                runs_stat_units, p_val_units = runstest_1samp(units_binary)\n",
    "                            else:\n",
    "                                runs_stat_units, p_val_units = np.nan, np.nan\n",
    "                            \n",
    "                            # Test Number of Prescriptions\n",
    "                            presc_values = subset_sorted['Number of Prescriptions'].values\n",
    "                            presc_median = np.median(presc_values)\n",
    "                            presc_binary = (presc_values > presc_median).astype(int)\n",
    "                            \n",
    "                            if len(np.unique(presc_binary)) > 1:\n",
    "                                runs_stat_presc, p_val_presc = runstest_1samp(presc_binary)\n",
    "                            else:\n",
    "                                runs_stat_presc, p_val_presc = np.nan, np.nan\n",
    "                            \n",
    "                            all_results['atc2_quarter'].append({\n",
    "                                'ATC2_Class': atc2,\n",
    "                                'ATC2_Name': atc2_name,\n",
    "                                'Quarter': quarter,\n",
    "                                'N_Records': len(subset),\n",
    "                                'N_Years': subset['Year'].nunique(),\n",
    "                                'N_States': subset['State'].nunique(),\n",
    "                                'Units_Runs_Stat': runs_stat_units,\n",
    "                                'Units_P_Value': p_val_units,\n",
    "                                'Presc_Runs_Stat': runs_stat_presc,\n",
    "                                'Presc_P_Value': p_val_presc\n",
    "                            })\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing {atc2}-Q{quarter}: {e}\")\n",
    "        \n",
    "        # Display ATC2 x Quarter results\n",
    "        atc2_quarter_df = pd.DataFrame(all_results['atc2_quarter'])\n",
    "        if not atc2_quarter_df.empty:\n",
    "            print(f\"\\nATC2 x Quarter Results: {len(atc2_quarter_df)} combinations analyzed\")\n",
    "            \n",
    "            units_significant = (atc2_quarter_df['Units_P_Value'] < 0.05).sum()\n",
    "            presc_significant = (atc2_quarter_df['Presc_P_Value'] < 0.05).sum()\n",
    "            total_valid = atc2_quarter_df['Units_P_Value'].notna().sum()\n",
    "            \n",
    "            print(f\"Units Reimbursed - Significant non-randomness: {units_significant}/{total_valid} ({100*units_significant/total_valid if total_valid > 0 else 0:.1f}%)\")\n",
    "            print(f\"Number of Prescriptions - Significant non-randomness: {presc_significant}/{total_valid} ({100*presc_significant/total_valid if total_valid > 0 else 0:.1f}%)\")\n",
    "            \n",
    "            print(f\"\\nTop 10 Most Significant Units Reimbursed Results (ATC2 x Quarter):\")\n",
    "            top_units_quarter = atc2_quarter_df.nsmallest(10, 'Units_P_Value')[['ATC2_Class', 'ATC2_Name', 'Quarter', 'Units_Runs_Stat', 'Units_P_Value', 'N_Records']].copy()\n",
    "            top_units_quarter['Units_P_Value'] = top_units_quarter['Units_P_Value'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(top_units_quarter.to_string(index=False))\n",
    "            \n",
    "            print(f\"\\nTop 10 Most Significant Number of Prescriptions Results (ATC2 x Quarter):\")\n",
    "            top_presc_quarter = atc2_quarter_df.nsmallest(10, 'Presc_P_Value')[['ATC2_Class', 'ATC2_Name', 'Quarter', 'Presc_Runs_Stat', 'Presc_P_Value', 'N_Records']].copy()\n",
    "            top_presc_quarter['Presc_P_Value'] = top_presc_quarter['Presc_P_Value'].apply(lambda x: f\"{x:.6f}\" if pd.notna(x) else 'NaN')\n",
    "            print(top_presc_quarter.to_string(index=False))\n",
    "        \n",
    "        # SUMMARY SECTION\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"RUNS TEST SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        summary_data = []\n",
    "        \n",
    "        if not state_atc2_df.empty:\n",
    "            state_units_sig = (state_atc2_df['Units_P_Value'] < 0.05).sum()\n",
    "            state_presc_sig = (state_atc2_df['Presc_P_Value'] < 0.05).sum()\n",
    "            state_total = state_atc2_df['Units_P_Value'].notna().sum()\n",
    "            summary_data.append(['State x ATC2', state_total, state_units_sig, state_presc_sig])\n",
    "        \n",
    "        if not national_atc2_df.empty:\n",
    "            nat_units_sig = (national_atc2_df['Units_P_Value'] < 0.05).sum()\n",
    "            nat_presc_sig = (national_atc2_df['Presc_P_Value'] < 0.05).sum()\n",
    "            nat_total = national_atc2_df['Units_P_Value'].notna().sum()\n",
    "            summary_data.append(['National x ATC2', nat_total, nat_units_sig, nat_presc_sig])\n",
    "        \n",
    "        if not atc2_quarter_df.empty:\n",
    "            quarter_units_sig = (atc2_quarter_df['Units_P_Value'] < 0.05).sum()\n",
    "            quarter_presc_sig = (atc2_quarter_df['Presc_P_Value'] < 0.05).sum()\n",
    "            quarter_total = atc2_quarter_df['Units_P_Value'].notna().sum()\n",
    "            summary_data.append(['ATC2 x Quarter', quarter_total, quarter_units_sig, quarter_presc_sig])\n",
    "        \n",
    "        if summary_data:\n",
    "            summary_df = pd.DataFrame(summary_data, columns=['Panel', 'Total Tests', 'Units Sig (p<0.05)', 'Prescriptions Sig (p<0.05)'])\n",
    "            summary_df['Units % Sig'] = (summary_df['Units Sig (p<0.05)'] / summary_df['Total Tests'] * 100).round(1)\n",
    "            summary_df['Prescriptions % Sig'] = (summary_df['Prescriptions Sig (p<0.05)'] / summary_df['Total Tests'] * 100).round(1)\n",
    "            print(summary_df.to_string(index=False))\n",
    "        \n",
    "        print(f\"\\nNote: Runs test examines whether data points occur in a random sequence.\")\n",
    "        print(f\"Significant p-values (< 0.05) indicate non-random patterns in the data.\")\n",
    "        print(f\"Lower runs statistics suggest more clustering; higher values suggest more alternation.\")\n",
    "        \n",
    "        return {\n",
    "            'state_atc2': state_atc2_df,\n",
    "            'national_atc2': national_atc2_df, \n",
    "            'atc2_quarter': atc2_quarter_df\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#covarience_results = NDCATC_ind.covariance_look(years_list)\n",
    "\n",
    "# Example 1: Indiana only\n",
    "## correlation_results_IN = NDCATC_ind.correlation_look(years_list, state_filter='IN', include_national=False)\n",
    "\n",
    "# Example 2: Both Indiana and National\n",
    "## correlation_results_both = NDCATC_ind.correlation_look(years_list, state_filter='IN', include_national=True)\n",
    "\n",
    "# Example 3: National only\n",
    "## correlation_results_national = NDCATC_ind.correlation_look(years_list, state_filter=None, include_national=True)\n",
    "\n",
    "# NEW: Normality testing examples\n",
    "## normality_results_IN = NDCATC_ind.normality_tests(years_to_analyze, state_filter='IN', include_log_transform=True)\n",
    "## normality_results_national = NDCATC_ind.normality_tests(years_to_analyze, state_filter=None, include_log_transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4877afee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Runs Test Analysis...\n",
      "This will test for randomness in Units Reimbursed and Number of Prescriptions\n",
      "across three different panels: State x ATC2, National x ATC2, and ATC2 x Quarter\n",
      "RUNS TEST ANALYSIS: Testing for Randomness in Units Reimbursed and Number of Prescriptions\n",
      "================================================================================\n",
      "Loading 2017 data... ✓ (1,931,088 records)\n",
      "Loading 2018 data... ✓ (2,022,672 records)\n",
      "Loading 2019 data... ✓ (2,109,684 records)\n",
      "Loading 2020 data... ✓ (2,173,775 records)\n",
      "Loading 2021 data... ✓ (2,287,508 records)\n",
      "Loading 2022 data... ✓ (2,353,181 records)\n",
      "Loading 2023 data... ✓ (2,385,896 records)\n",
      "Loading 2024 data... ✓ (2,338,667 records)\n",
      "\n",
      "================================================================================\n",
      "PANEL 1: STATE x ATC2 CLASSES RUNS TEST\n",
      "================================================================================\n",
      "\n",
      "State x ATC2 Results: 29248 combinations analyzed\n",
      "Units Reimbursed - Significant non-randomness: 16250/29248 (55.6%)\n",
      "Number of Prescriptions - Significant non-randomness: 11540/29248 (39.5%)\n",
      "\n",
      "Top 10 Most Significant Units Reimbursed Results (State x ATC2):\n",
      "State ATC2_Class                       ATC2_Name  Units_Runs_Stat Units_P_Value  N_Records\n",
      "   CA        J01 ANTIBACTERIALS FOR SYSTEMIC USE       -37.762811      0.000000       5653\n",
      "   MA        N06                PSYCHOANALEPTICS       -37.993428      0.000000       6336\n",
      "   MA        N05                   PSYCHOLEPTICS       -39.936970      0.000000       5742\n",
      "   CA        N06                PSYCHOANALEPTICS       -37.857331      0.000000       9358\n",
      "   CA        J01 ANTIBACTERIALS FOR SYSTEMIC USE       -39.664825      0.000000       6727\n",
      "   MA        N06                PSYCHOANALEPTICS       -40.530355      0.000000       6258\n",
      "   MA        N05                   PSYCHOLEPTICS       -38.066386      0.000000       5729\n",
      "   CA        N06                PSYCHOANALEPTICS       -38.082867      0.000000      10010\n",
      "   MA        N06                PSYCHOANALEPTICS       -40.737439      0.000000       6575\n",
      "   MA        N05                   PSYCHOLEPTICS       -38.170441      0.000000       6043\n",
      "\n",
      "Top 10 Most Significant Number of Prescriptions Results (State x ATC2):\n",
      "State ATC2_Class        ATC2_Name  Presc_Runs_Stat Presc_P_Value  N_Records\n",
      "   CA        N06 PSYCHOANALEPTICS       -38.039535      0.000000       8627\n",
      "   CA        N06 PSYCHOANALEPTICS       -38.783609      0.000000       9009\n",
      "   MA        N06 PSYCHOANALEPTICS       -38.395212      0.000000       6336\n",
      "   CA        N06 PSYCHOANALEPTICS       -38.890446      0.000000       9358\n",
      "   MA        N06 PSYCHOANALEPTICS       -40.934763      0.000000       6258\n",
      "   CA        N06 PSYCHOANALEPTICS       -38.282688      0.000000      10010\n",
      "   MA        N06 PSYCHOANALEPTICS       -42.365306      0.000000       6575\n",
      "   MA        N06 PSYCHOANALEPTICS       -40.913566      0.000000       6538\n",
      "   MA        N06 PSYCHOANALEPTICS       -39.801288      0.000000       6461\n",
      "   NC        N06 PSYCHOANALEPTICS       -41.375695      0.000000       7651\n",
      "\n",
      "================================================================================\n",
      "PANEL 2: NATIONAL x ATC2 CLASSES RUNS TEST\n",
      "================================================================================\n",
      "\n",
      "National x ATC2 Results: 695 combinations analyzed\n",
      "Units Reimbursed - Significant non-randomness: 618/695 (88.9%)\n",
      "Number of Prescriptions - Significant non-randomness: 607/695 (87.3%)\n",
      "\n",
      "Top 10 Most Significant Units Reimbursed Results (National x ATC2):\n",
      " Year ATC2_Class                                         ATC2_Name  Units_Runs_Stat Units_P_Value  N_Records\n",
      " 2017        N06                                  PSYCHOANALEPTICS       -91.748409      0.000000     196989\n",
      " 2017        S01                                 OPHTHALMOLOGICALS       -63.043016      0.000000      71205\n",
      " 2017        B01                             ANTITHROMBOTIC AGENTS       -38.808759      0.000000      32991\n",
      " 2017        L01                             ANTINEOPLASTIC AGENTS       -41.949693      0.000000      29383\n",
      " 2017        L04                                IMMUNOSUPPRESSANTS       -45.011284      0.000000      20733\n",
      " 2017        N05                                     PSYCHOLEPTICS      -117.385644      0.000000     177548\n",
      " 2017        J01                   ANTIBACTERIALS FOR SYSTEMIC USE       -89.816259      0.000000     107069\n",
      " 2017        C07                              BETA BLOCKING AGENTS       -39.655424      0.000000      53684\n",
      " 2017        D10                            ANTI-ACNE PREPARATIONS       -41.914188      0.000000      14695\n",
      " 2017        G03 SEX HORMONES AND MODULATORS OF THE GENITAL SYSTEM       -49.818220      0.000000      45589\n",
      "\n",
      "Top 10 Most Significant Number of Prescriptions Results (National x ATC2):\n",
      " Year ATC2_Class                                     ATC2_Name  Presc_Runs_Stat Presc_P_Value  N_Records\n",
      " 2017        N06                              PSYCHOANALEPTICS       -95.936432      0.000000     196989\n",
      " 2017        N05                                 PSYCHOLEPTICS       -88.965599      0.000000     177548\n",
      " 2017        N03                                ANTIEPILEPTICS       -77.934729      0.000000      96587\n",
      " 2017        N02                                    ANALGESICS       -51.594356      0.000000     108616\n",
      " 2017        C09 AGENTS ACTING ON THE RENIN-ANGIOTENSIN SYSTEM       -58.019856      0.000000      64870\n",
      " 2017        C10                        LIPID MODIFYING AGENTS       -42.523499      0.000000      57550\n",
      " 2018        N06                              PSYCHOANALEPTICS      -104.882241      0.000000     204891\n",
      " 2018        N05                                 PSYCHOLEPTICS       -95.269057      0.000000     186184\n",
      " 2018        N03                                ANTIEPILEPTICS       -79.455358      0.000000      98604\n",
      " 2018        N02                                    ANALGESICS       -48.139249      0.000000     114047\n",
      "\n",
      "================================================================================\n",
      "PANEL 3: ATC2 CLASSES x QUARTER RUNS TEST\n",
      "================================================================================\n",
      "\n",
      "ATC2 x Quarter Results: 352 combinations analyzed\n",
      "Units Reimbursed - Significant non-randomness: 330/352 (93.8%)\n",
      "Number of Prescriptions - Significant non-randomness: 319/352 (90.6%)\n",
      "\n",
      "Top 10 Most Significant Units Reimbursed Results (ATC2 x Quarter):\n",
      "ATC2_Class                                    ATC2_Name  Quarter  Units_Runs_Stat Units_P_Value  N_Records\n",
      "       A10                       DRUGS USED IN DIABETES        1       -60.416521      0.000000     136667\n",
      "       A10                       DRUGS USED IN DIABETES        2       -62.946958      0.000000     137650\n",
      "       A10                       DRUGS USED IN DIABETES        3       -63.676687      0.000000     139219\n",
      "       A10                       DRUGS USED IN DIABETES        4       -63.611149      0.000000     138077\n",
      "       N06                             PSYCHOANALEPTICS        1      -152.746029      0.000000     447062\n",
      "       N06                             PSYCHOANALEPTICS        2      -156.082469      0.000000     446875\n",
      "       N06                             PSYCHOANALEPTICS        3      -156.991199      0.000000     452809\n",
      "       N06                             PSYCHOANALEPTICS        4      -157.897228      0.000000     457178\n",
      "       D07 CORTICOSTEROIDS, DERMATOLOGICAL PREPARATIONS        1       -49.665148      0.000000      71002\n",
      "       D07 CORTICOSTEROIDS, DERMATOLOGICAL PREPARATIONS        2       -48.796746      0.000000      71282\n",
      "\n",
      "Top 10 Most Significant Number of Prescriptions Results (ATC2 x Quarter):\n",
      "ATC2_Class              ATC2_Name  Quarter  Presc_Runs_Stat Presc_P_Value  N_Records\n",
      "       A10 DRUGS USED IN DIABETES        1       -46.571538      0.000000     136667\n",
      "       A10 DRUGS USED IN DIABETES        2       -50.324092      0.000000     137650\n",
      "       A10 DRUGS USED IN DIABETES        3       -50.961257      0.000000     139219\n",
      "       A10 DRUGS USED IN DIABETES        4       -49.466332      0.000000     138077\n",
      "       N06       PSYCHOANALEPTICS        1      -157.557110      0.000000     447062\n",
      "       N06       PSYCHOANALEPTICS        2      -159.197515      0.000000     446875\n",
      "       N06       PSYCHOANALEPTICS        3      -160.906228      0.000000     452809\n",
      "       N06       PSYCHOANALEPTICS        4      -162.341316      0.000000     457178\n",
      "       B01  ANTITHROMBOTIC AGENTS        1       -39.021392      0.000000      65280\n",
      "       B01  ANTITHROMBOTIC AGENTS        2       -38.018967      0.000000      64941\n",
      "\n",
      "================================================================================\n",
      "RUNS TEST SUMMARY\n",
      "================================================================================\n",
      "          Panel  Total Tests  Units Sig (p<0.05)  Prescriptions Sig (p<0.05)  Units % Sig  Prescriptions % Sig\n",
      "   State x ATC2        29248               16250                       11540         55.6                 39.5\n",
      "National x ATC2          695                 618                         607         88.9                 87.3\n",
      " ATC2 x Quarter          352                 330                         319         93.8                 90.6\n",
      "\n",
      "Note: Runs test examines whether data points occur in a random sequence.\n",
      "Significant p-values (< 0.05) indicate non-random patterns in the data.\n",
      "Lower runs statistics suggest more clustering; higher values suggest more alternation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform runs test analysis across all three panels\n",
    "print(\"Starting Runs Test Analysis...\")\n",
    "print(\"This will test for randomness in Units Reimbursed and Number of Prescriptions\")\n",
    "print(\"across three different panels: State x ATC2, National x ATC2, and ATC2 x Quarter\")\n",
    "\n",
    "# Uncomment the line below to run the analysis\n",
    "runs_results = NDCATC_ind.runs_test_analysis(years_list, min_records=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
