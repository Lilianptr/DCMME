{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b3e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import shelve\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56f7f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "user=\"Lilian\"\n",
    "#user in personal pc <- \"asus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aebaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New changes to the class\n",
    "class NDCATCAnalyzer:\n",
    "\n",
    "    def __init__(self, year, base_path=None):\n",
    "\n",
    "        self.year = year\n",
    "        if base_path is None:\n",
    "            #Lookup the user's base path\n",
    "            self.base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        else:\n",
    "            self.base_path = base_path\n",
    "            \n",
    "        self.df_cleaned = None\n",
    "        self.df_merged = None\n",
    "        self.atc_mapping = None\n",
    "        self.df_faf = None\n",
    "        \n",
    "    def clean_sdud_data(self):\n",
    "        csv_file = os.path.join(self.base_path, f\"SDUD\\\\SDUD{self.year}.csv\")\n",
    "        print(f\"Reading CSV file: {csv_file}\")\n",
    "        \n",
    "        # Read with NDC as string to preserve leading zeros\n",
    "        df = pd.read_csv(csv_file, dtype={'NDC': 'object'})\n",
    "        \n",
    "        print(f\"Total rows in {self.year} before filtering: {len(df)}\")\n",
    "        \n",
    "        # Remove NA values\n",
    "        df_filtered = df.dropna(subset=['Units Reimbursed', 'Number of Prescriptions'])\n",
    "        print(f\"Rows after removing NA: {len(df_filtered)}\")\n",
    "        \n",
    "        # Filter out State='XX'\n",
    "        df_filtered = df_filtered[df_filtered['State'] != 'XX']\n",
    "        print(f\"Rows after filtering State='XX': {len(df_filtered)}\")\n",
    "        print(f\"Unique NDCs: {df_filtered['NDC'].nunique()}\")\n",
    "        \n",
    "        self.df_cleaned = df_filtered\n",
    "        return self.df_cleaned\n",
    "    \n",
    "    #NEW\n",
    "    def adding_key(self):\n",
    "        \"\"\"Add record_id column to cleaned dataframe.\"\"\"\n",
    "        if self.df_cleaned is None:\n",
    "            raise ValueError(\"Must run clean_sdud_data() first\")\n",
    "        \n",
    "        print(\"Adding record_id column...\")\n",
    "        \n",
    "        # Create record_id column\n",
    "        self.df_cleaned['record_id'] = (\n",
    "            self.df_cleaned['State'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['Year'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['Quarter'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['Utilization Type'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['NDC'].astype(str)\n",
    "        )\n",
    "        \n",
    "        print(f\"Created {len(self.df_cleaned)} record IDs\")\n",
    "        print(f\"Sample record_id: {self.df_cleaned['record_id'].iloc[0]}\")\n",
    "        \n",
    "        return self.df_cleaned\n",
    "    \n",
    "    def generate_ndc_txt(self, output_filename=None):\n",
    "        \"\"\"Step 2: Generate text file with unique NDC values and their record_id keys.\"\"\"\n",
    "        if self.df_cleaned is None:\n",
    "            raise ValueError(\"Must run clean_sdud_data() first\")\n",
    "        \n",
    "        if 'record_id' not in self.df_cleaned.columns:\n",
    "            raise ValueError(\"Must run adding_key() first to create record_id column\")\n",
    "            \n",
    "        if output_filename is None:\n",
    "            output_filename = f\"NDCNEW_{self.year}.txt\"\n",
    "        \n",
    "        output_path = os.path.join(self.base_path, f\"ATC\\\\text_files\\\\{output_filename}\")\n",
    "        \n",
    "        # Get unique combinations of NDC and record_id\n",
    "        unique_pairs = self.df_cleaned[['NDC', 'record_id']].drop_duplicates()\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            # Write header\n",
    "            f.write(\"NDC\\trecord_id\\n\")\n",
    "            # Write each unique pair\n",
    "            for _, row in unique_pairs.iterrows():\n",
    "                f.write(f\"{row['NDC']}\\t{row['record_id']}\\n\")\n",
    "        \n",
    "        print(f\"Exported to: {output_path}\")\n",
    "        print(f\"Unique record_id values: {unique_pairs['record_id'].nunique()}\")\n",
    "        return output_path\n",
    "    \n",
    "    def analyze_atc4_mapping(self):\n",
    "        \"\"\"Step 3: Analyze ATC4 mapping results and identify missing NDCs.\"\"\"\n",
    "        atc4_path = os.path.join(self.base_path, f\"ATC\\\\ATC4_classes\\\\NDCNEW_{self.year}_ATC4_classes.csv\")\n",
    "        \n",
    "        # Read ATC4 mapping\n",
    "        df_atc4 = pd.read_csv(atc4_path, dtype={'NDC': 'object', 'record_id': 'string'})\n",
    "        df_atc4['NDC'] = df_atc4['NDC'].str.zfill(11)\n",
    "        \n",
    "        # Ensure consistent data types before merge\n",
    "        self.df_cleaned['record_id'] = self.df_cleaned['record_id'].astype('string')\n",
    "        self.df_cleaned['NDC'] = self.df_cleaned['NDC'].astype('object')  \n",
    "        df_atc4['record_id'] = df_atc4['record_id'].astype('string')\n",
    "        df_atc4['NDC'] = df_atc4['NDC'].astype('object')  \n",
    "\n",
    "        # Merge ATC4 mapping with cleaned data using record_id\n",
    "        if self.df_cleaned is None:\n",
    "            raise ValueError(\"Must run clean_sdud_data() and adding_key() first\")\n",
    "        \n",
    "        if 'record_id' not in self.df_cleaned.columns:\n",
    "            raise ValueError(\"Must run adding_key() first to create record_id column\")\n",
    "        \n",
    "        print(f\"Merging ATC4 mapping with cleaned data using record_id and NDC...\")\n",
    "        \n",
    "        # Merge on BOTH record_id AND NDC\n",
    "        self.atc_mapping = pd.merge(\n",
    "            self.df_cleaned,\n",
    "            df_atc4[['record_id', 'NDC', 'ATC4 Class']],  # Include NDC in the selection\n",
    "            on=['record_id', 'NDC'],  # Merge on both columns\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Print rows of the merged dataframe\n",
    "        print(f\"Merged dataframe rows: {len(self.atc_mapping)}\")\n",
    "        print(self.atc_mapping.head())\n",
    "        total_records = len(self.atc_mapping)\n",
    "        mapped_records = self.atc_mapping['ATC4 Class'].notna().sum()\n",
    "        print(f\"Records with ATC4 mapping: {mapped_records} ({mapped_records/total_records*100:.1f}%)\")\n",
    "        \n",
    "        # Identify missing mappings\n",
    "        missing_records = self.atc_mapping[self.atc_mapping['ATC4 Class'].isna()]\n",
    "        if len(missing_records) > 0:\n",
    "            print(f\"\\nRecords without ATC4 mapping: {len(missing_records)}\")\n",
    "            print(f\"Unique NDCs without mapping: {missing_records['NDC'].nunique()}\")\n",
    "        \n",
    "        # Export the atc_mapping\n",
    "        output_path = os.path.join(self.base_path, f\"ATC\\\\merged_data\\\\atc_mapping_{self.year}.csv\")\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        self.atc_mapping.to_csv(output_path, index=False)\n",
    "        print(f\"\\nExported atc_mapping to: {output_path}\")\n",
    "        print(f\"Total rows exported: {len(self.atc_mapping)}\")\n",
    "        \n",
    "        return self.atc_mapping\n",
    "        \n",
    "    def analyze_atc4_distribution(self):\n",
    "\n",
    "        \"\"\"Analyze distribution of ATC4 classes per record_id.\"\"\"\n",
    "        if self.atc_mapping is None:\n",
    "            raise ValueError(\"Must run analyze_atc4_mapping() first\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ATC4 CLASSES PER RECORD_ID DISTRIBUTION\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Count ATC4 classes per record_id (only valid mappings)\n",
    "        records_with_mapping = self.atc_mapping[self.atc_mapping['ATC4 Class'].notna()].copy()\n",
    "        \n",
    "        if len(records_with_mapping) == 0:\n",
    "            print(\"No records with valid ATC4 mappings found.\")\n",
    "            return None\n",
    "        \n",
    "        # Group by record_id and count unique ATC4 classes\n",
    "        atc4_per_record = records_with_mapping.groupby('record_id').agg({\n",
    "            'ATC4 Class': 'nunique',\n",
    "            'NDC': 'first',  # Get the NDC for reference\n",
    "            'State': 'first',  # Get the state for reference\n",
    "            'Year': 'first'    # Get the year for reference\n",
    "        }).reset_index()\n",
    "        \n",
    "        atc4_per_record.columns = ['record_id', 'num_atc4_classes', 'NDC', 'State', 'Year']\n",
    "        \n",
    "        # Distribution analysis\n",
    "        distribution = atc4_per_record['num_atc4_classes'].value_counts().sort_index()\n",
    "        \n",
    "        print(\"Distribution of ATC4 classes per record_id:\")\n",
    "        for classes, count in distribution.items():\n",
    "            pct = (count / len(atc4_per_record)) * 100\n",
    "            print(f\"  {classes} class(es): {count:,} record_ids ({pct:.1f}%)\")\n",
    "        \n",
    "        # Show examples of multi-class records\n",
    "        multi_class = atc4_per_record[atc4_per_record['num_atc4_classes'] > 1].sort_values('num_atc4_classes', ascending=False)\n",
    "        \n",
    "        if len(multi_class) > 0:\n",
    "            print(f\"\\nTop 10 record_ids with most ATC4 classes:\")\n",
    "            for _, row in multi_class.head(10).iterrows():\n",
    "                record_classes = records_with_mapping[records_with_mapping['record_id'] == row['record_id']]['ATC4 Class'].unique()\n",
    "                print(f\"  {row['record_id']}: {row['num_atc4_classes']} classes\")\n",
    "                print(f\"    NDC: {row['NDC']}, State: {row['State']}, Year: {row['Year']}\")\n",
    "                print(f\"    Classes: {list(record_classes)}\")\n",
    "                print()\n",
    "        \n",
    "        return atc4_per_record\n",
    "\n",
    "    def fetch_atc_names(self, cache_path=None):\n",
    "        \"\"\"Fetch ATC class names (ATC4, ATC3, ATC2) from RxNav API.\"\"\"\n",
    "        if self.atc_mapping is None:\n",
    "            raise ValueError(\"Must run analyze_atc4_mapping() first\")\n",
    "        \n",
    "        if cache_path is None:\n",
    "            cache_path = os.path.join(self.base_path, \"ATC\\\\cache_files\\\\atc_names_cache\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"FETCHING ATC CLASS NAMES\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Using cache: {cache_path}\")\n",
    "        \n",
    "        # Get only records with valid ATC4 mappings\n",
    "        df_with_atc = self.atc_mapping[self.atc_mapping['ATC4 Class'].notna()].copy()\n",
    "        \n",
    "        # Create ATC3 and ATC2 columns from ATC4\n",
    "        print(\"\\nCreating ATC3 and ATC2 columns from ATC4...\")\n",
    "        df_with_atc['ATC3 Class'] = df_with_atc['ATC4 Class'].str[:4]\n",
    "        df_with_atc['ATC2 Class'] = df_with_atc['ATC4 Class'].str[:3]\n",
    "        \n",
    "        # Get unique codes for each level\n",
    "        unique_atc4 = df_with_atc['ATC4 Class'].dropna().unique()\n",
    "        unique_atc3 = df_with_atc['ATC3 Class'].dropna().unique()\n",
    "        unique_atc2 = df_with_atc['ATC2 Class'].dropna().unique()\n",
    "        \n",
    "        # Filter out invalid codes\n",
    "        unique_atc4 = [c for c in unique_atc4 if c not in ['No ATC Mapping Found', 'No RxCUI Found', '']]\n",
    "        unique_atc3 = [c for c in unique_atc3 if c not in ['No ATC Mapping Found', 'No RxCUI Found', '', 'No ', 'No']]\n",
    "        unique_atc2 = [c for c in unique_atc2 if c not in ['No ATC Mapping Found', 'No RxCUI Found', '', 'No ', 'No']]\n",
    "        \n",
    "        print(f\"\\nUnique codes to fetch:\")\n",
    "        print(f\"  ATC4: {len(unique_atc4)}\")\n",
    "        print(f\"  ATC3: {len(unique_atc3)}\")\n",
    "        print(f\"  ATC2: {len(unique_atc2)}\")\n",
    "        \n",
    "        # Build mappings\n",
    "        atc4_names = {}\n",
    "        atc3_names = {}\n",
    "        atc2_names = {}\n",
    "        \n",
    "        with shelve.open(cache_path) as cache:\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            print(\"\\nFetching ATC4 names...\")\n",
    "            for code in unique_atc4:\n",
    "                atc4_names[code] = self._get_atc_name(code, cache)\n",
    "            \n",
    "            print(\"Fetching ATC3 names...\")\n",
    "            for code in unique_atc3:\n",
    "                atc3_names[code] = self._get_atc_name(code, cache)\n",
    "            \n",
    "            print(\"Fetching ATC2 names...\")\n",
    "            for code in unique_atc2:\n",
    "                atc2_names[code] = self._get_atc_name(code, cache)\n",
    "            \n",
    "            print(f\"\\nTotal processing time: {(datetime.now() - start_time).total_seconds()/60:.1f} minutes\")\n",
    "        \n",
    "        # Apply names to all records in atc_mapping\n",
    "        print(\"\\nApplying names to dataframe...\")\n",
    "        self.atc_mapping['ATC3 Class'] = self.atc_mapping['ATC4 Class'].str[:4]\n",
    "        self.atc_mapping['ATC2 Class'] = self.atc_mapping['ATC4 Class'].str[:3]\n",
    "        \n",
    "        self.atc_mapping['ATC4_Name'] = self.atc_mapping['ATC4 Class'].map(atc4_names).fillna('')\n",
    "        self.atc_mapping['ATC3_Name'] = self.atc_mapping['ATC3 Class'].map(atc3_names).fillna('')\n",
    "        self.atc_mapping['ATC2_Name'] = self.atc_mapping['ATC2 Class'].map(atc2_names).fillna('')\n",
    "        \n",
    "        print(f\"\\nATC names added successfully!\")\n",
    "        print(\"\\nSample output:\")\n",
    "        sample = self.atc_mapping[self.atc_mapping['ATC4 Class'].notna()][['NDC', 'record_id', 'ATC4 Class', 'ATC4_Name', 'ATC3 Class', 'ATC3_Name', 'ATC2 Class', 'ATC2_Name']].head(5)\n",
    "        print(sample.to_string())\n",
    "        \n",
    "        return self.atc_mapping\n",
    "    \n",
    "    def prepare_final_dataframe(self):\n",
    "        \"\"\"Prepare final dataframe with scaled metrics for export.\"\"\"\n",
    "        if self.atc_mapping is None:\n",
    "            raise ValueError(\"Must run fetch_atc_names() first\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"PREPARING FINAL DATAFRAME\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Create a copy for final output\n",
    "        self.df_merged = self.atc_mapping.copy()\n",
    "        \n",
    "        # Scale units\n",
    "        print(\"\\nScaling units...\")\n",
    "        self.df_merged['Units Reimbursed'] = self.df_merged['Units Reimbursed'] / 1e9\n",
    "        self.df_merged['Number of Prescriptions'] = self.df_merged['Number of Prescriptions'] / 1e6\n",
    "        \n",
    "        # Report final statistics\n",
    "        total_records = len(self.df_merged)\n",
    "        mapped_records = self.df_merged['ATC4 Class'].notna().sum()\n",
    "        \n",
    "        print(f\"\\nFinal statistics:\")\n",
    "        print(f\"Total records: {total_records:,}\")\n",
    "        print(f\"Records with ATC4 mapping: {mapped_records:,} ({mapped_records/total_records*100:.1f}%)\")\n",
    "        print(f\"Total Units Reimbursed: {self.df_merged['Units Reimbursed'].sum():.2f} Billion\")\n",
    "        print(f\"Total Prescriptions: {self.df_merged['Number of Prescriptions'].sum():.2f} Million\")\n",
    "        \n",
    "        return self.df_merged\n",
    "    \n",
    "    def _get_atc_name(self, atc_code, cache):\n",
    "        \"\"\"Get ATC class name from code, using cache.\"\"\"\n",
    "        cache_key = f\"atc_name:{atc_code}\"\n",
    "        if cache_key in cache:\n",
    "            return cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            url = f\"https://rxnav.nlm.nih.gov/REST/rxclass/class/byId.json?classId={atc_code}\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            # Get class name\n",
    "            if 'rxclassMinConceptList' in data and 'rxclassMinConcept' in data['rxclassMinConceptList']:\n",
    "                concepts = data['rxclassMinConceptList']['rxclassMinConcept']\n",
    "                if concepts:\n",
    "                    name = concepts[0].get('className', '')\n",
    "                    cache[cache_key] = name\n",
    "                    return name\n",
    "            \n",
    "            cache[cache_key] = ''\n",
    "            return ''\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving name for {atc_code}: {e}\")\n",
    "            cache[cache_key] = ''\n",
    "            return ''\n",
    "    \n",
    "    def export_merged_data(self, output_filename=None):\n",
    "        \"\"\"Export the final merged dataframe to CSV.\"\"\"\n",
    "        if self.df_merged is None:\n",
    "            raise ValueError(\"Must run prepare_final_dataframe() first to create merged dataframe\")\n",
    "            \n",
    "        if output_filename is None:\n",
    "            output_filename = f\"merged_NEWdata_{self.year}.csv\"\n",
    "\n",
    "        output_path = os.path.join(self.base_path, f\"ATC\\\\merged_data\\\\{output_filename}\")\n",
    "\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        self.df_merged.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"DATA EXPORT COMPLETE\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Exported to: {output_path}\")\n",
    "        print(f\"Total rows exported: {len(self.df_merged):,}\")\n",
    "        print(f\"Columns: {', '.join(self.df_merged.columns.tolist())}\")\n",
    "        \n",
    "        return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "workflow_execution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file: c:\\Users\\Lilian\\OneDrive - purdue.edu\\VS code\\Data\\SDUD\\SDUD2023.csv\n",
      "Total rows in 2023 before filtering: 5277298\n",
      "Total rows in 2023 before filtering: 5277298\n",
      "Rows after removing NA: 2651527\n",
      "Rows after filtering State='XX': 2413521\n",
      "Rows after removing NA: 2651527\n",
      "Rows after filtering State='XX': 2413521\n",
      "Unique NDCs: 34439\n",
      "Adding record_id column...\n",
      "Unique NDCs: 34439\n",
      "Adding record_id column...\n",
      "Created 2413521 record IDs\n",
      "Sample record_id: AK_2023_4_FFSU_00002143380\n",
      "Created 2413521 record IDs\n",
      "Sample record_id: AK_2023_4_FFSU_00002143380\n",
      "Exported to: c:\\Users\\Lilian\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\text_files\\NDCNEW_2023.txt\n",
      "Exported to: c:\\Users\\Lilian\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\text_files\\NDCNEW_2023.txt\n",
      "Unique record_id values: 2413521\n",
      "Unique record_id values: 2413521\n",
      "Merging ATC4 mapping with cleaned data using record_id and NDC...\n",
      "Merging ATC4 mapping with cleaned data using record_id and NDC...\n",
      "Merged dataframe rows: 4454193\n",
      "  Utilization Type State          NDC  Labeler Code  Product Code  \\\n",
      "0             FFSU    AK  00002143380             2          1433   \n",
      "1             FFSU    AK  00002143480             2          1434   \n",
      "2             FFSU    AK  00002143611             2          1436   \n",
      "3             FFSU    AK  00002144511             2          1445   \n",
      "4             FFSU    AK  00002145780             2          1457   \n",
      "\n",
      "   Package Size  Year  Quarter  Suppression Used Product Name  \\\n",
      "0            80  2023        4             False   TRULICITY    \n",
      "1            80  2023        4             False   TRULICITY    \n",
      "2            11  2023        4             False   EMGALITY P   \n",
      "3            11  2023        4             False   TALTZ AUTO   \n",
      "4            80  2023        4             False   MOUNJARO     \n",
      "\n",
      "   Units Reimbursed  Number of Prescriptions  Total Amount Reimbursed  \\\n",
      "0             242.0                    121.0                108458.54   \n",
      "1             386.0                    192.0                173430.95   \n",
      "2              28.0                     28.0                 18655.66   \n",
      "3              36.0                     32.0                239510.10   \n",
      "4              84.0                     42.0                 41557.22   \n",
      "\n",
      "   Medicaid Amount Reimbursed  Non Medicaid Amount Reimbursed  \\\n",
      "0                   106175.55                         2282.99   \n",
      "1                   164814.54                         8616.41   \n",
      "2                    18655.66                            0.00   \n",
      "3                   239510.10                            0.00   \n",
      "4                    39763.70                         1793.52   \n",
      "\n",
      "                    record_id ATC4 Class  \n",
      "0  AK_2023_4_FFSU_00002143380      A10BJ  \n",
      "1  AK_2023_4_FFSU_00002143480      A10BJ  \n",
      "2  AK_2023_4_FFSU_00002143611      N02CD  \n",
      "3  AK_2023_4_FFSU_00002144511      L04AC  \n",
      "4  AK_2023_4_FFSU_00002145780      A10BX  \n",
      "Records with ATC4 mapping: 4426568 (99.4%)\n",
      "Merged dataframe rows: 4454193\n",
      "  Utilization Type State          NDC  Labeler Code  Product Code  \\\n",
      "0             FFSU    AK  00002143380             2          1433   \n",
      "1             FFSU    AK  00002143480             2          1434   \n",
      "2             FFSU    AK  00002143611             2          1436   \n",
      "3             FFSU    AK  00002144511             2          1445   \n",
      "4             FFSU    AK  00002145780             2          1457   \n",
      "\n",
      "   Package Size  Year  Quarter  Suppression Used Product Name  \\\n",
      "0            80  2023        4             False   TRULICITY    \n",
      "1            80  2023        4             False   TRULICITY    \n",
      "2            11  2023        4             False   EMGALITY P   \n",
      "3            11  2023        4             False   TALTZ AUTO   \n",
      "4            80  2023        4             False   MOUNJARO     \n",
      "\n",
      "   Units Reimbursed  Number of Prescriptions  Total Amount Reimbursed  \\\n",
      "0             242.0                    121.0                108458.54   \n",
      "1             386.0                    192.0                173430.95   \n",
      "2              28.0                     28.0                 18655.66   \n",
      "3              36.0                     32.0                239510.10   \n",
      "4              84.0                     42.0                 41557.22   \n",
      "\n",
      "   Medicaid Amount Reimbursed  Non Medicaid Amount Reimbursed  \\\n",
      "0                   106175.55                         2282.99   \n",
      "1                   164814.54                         8616.41   \n",
      "2                    18655.66                            0.00   \n",
      "3                   239510.10                            0.00   \n",
      "4                    39763.70                         1793.52   \n",
      "\n",
      "                    record_id ATC4 Class  \n",
      "0  AK_2023_4_FFSU_00002143380      A10BJ  \n",
      "1  AK_2023_4_FFSU_00002143480      A10BJ  \n",
      "2  AK_2023_4_FFSU_00002143611      N02CD  \n",
      "3  AK_2023_4_FFSU_00002144511      L04AC  \n",
      "4  AK_2023_4_FFSU_00002145780      A10BX  \n",
      "Records with ATC4 mapping: 4426568 (99.4%)\n",
      "\n",
      "Records without ATC4 mapping: 27625\n",
      "Unique NDCs without mapping: 1940\n",
      "\n",
      "Records without ATC4 mapping: 27625\n",
      "Unique NDCs without mapping: 1940\n",
      "\n",
      "Exported atc_mapping to: c:\\Users\\Lilian\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\merged_data\\atc_mapping_2023.csv\n",
      "Total rows exported: 4454193\n",
      "\n",
      "============================================================\n",
      "ATC4 CLASSES PER RECORD_ID DISTRIBUTION\n",
      "============================================================\n",
      "\n",
      "Exported atc_mapping to: c:\\Users\\Lilian\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\merged_data\\atc_mapping_2023.csv\n",
      "Total rows exported: 4454193\n",
      "\n",
      "============================================================\n",
      "ATC4 CLASSES PER RECORD_ID DISTRIBUTION\n",
      "============================================================\n",
      "Distribution of ATC4 classes per record_id:\n",
      "  1 class(es): 1,647,252 record_ids (69.0%)\n",
      "  2 class(es): 291,830 record_ids (12.2%)\n",
      "  3 class(es): 220,312 record_ids (9.2%)\n",
      "  4 class(es): 61,169 record_ids (2.6%)\n",
      "  5 class(es): 50,917 record_ids (2.1%)\n",
      "  6 class(es): 14,322 record_ids (0.6%)\n",
      "  7 class(es): 25,149 record_ids (1.1%)\n",
      "  8 class(es): 23,502 record_ids (1.0%)\n",
      "  9 class(es): 15,709 record_ids (0.7%)\n",
      "  10 class(es): 614 record_ids (0.0%)\n",
      "  11 class(es): 23,376 record_ids (1.0%)\n",
      "  12 class(es): 2,540 record_ids (0.1%)\n",
      "  13 class(es): 649 record_ids (0.0%)\n",
      "  14 class(es): 3,887 record_ids (0.2%)\n",
      "  15 class(es): 482 record_ids (0.0%)\n",
      "  16 class(es): 1,097 record_ids (0.0%)\n",
      "  17 class(es): 452 record_ids (0.0%)\n",
      "  20 class(es): 1,475 record_ids (0.1%)\n",
      "  21 class(es): 54 record_ids (0.0%)\n",
      "  22 class(es): 1,108 record_ids (0.0%)\n",
      "\n",
      "Top 10 record_ids with most ATC4 classes:\n",
      "Distribution of ATC4 classes per record_id:\n",
      "  1 class(es): 1,647,252 record_ids (69.0%)\n",
      "  2 class(es): 291,830 record_ids (12.2%)\n",
      "  3 class(es): 220,312 record_ids (9.2%)\n",
      "  4 class(es): 61,169 record_ids (2.6%)\n",
      "  5 class(es): 50,917 record_ids (2.1%)\n",
      "  6 class(es): 14,322 record_ids (0.6%)\n",
      "  7 class(es): 25,149 record_ids (1.1%)\n",
      "  8 class(es): 23,502 record_ids (1.0%)\n",
      "  9 class(es): 15,709 record_ids (0.7%)\n",
      "  10 class(es): 614 record_ids (0.0%)\n",
      "  11 class(es): 23,376 record_ids (1.0%)\n",
      "  12 class(es): 2,540 record_ids (0.1%)\n",
      "  13 class(es): 649 record_ids (0.0%)\n",
      "  14 class(es): 3,887 record_ids (0.2%)\n",
      "  15 class(es): 482 record_ids (0.0%)\n",
      "  16 class(es): 1,097 record_ids (0.0%)\n",
      "  17 class(es): 452 record_ids (0.0%)\n",
      "  20 class(es): 1,475 record_ids (0.1%)\n",
      "  21 class(es): 54 record_ids (0.0%)\n",
      "  22 class(es): 1,108 record_ids (0.0%)\n",
      "\n",
      "Top 10 record_ids with most ATC4 classes:\n",
      "  MA_2023_1_MCOU_24208079535: 22 classes\n",
      "    NDC: 24208079535, State: MA, Year: 2023\n",
      "    Classes: ['S03BA', 'J01XB', 'C05AA', 'R02AB', 'A01AC', 'H02AB', 'R01AD', 'A01AB', 'A07AA', 'S01CB', 'S01BA', 'S02AA', 'B05CA', 'D07AB', 'D07XB', 'S02BA', 'D06AX', 'S01AA', 'S01CA', 'D10AA', 'J01GB', 'S03AA']\n",
      "\n",
      "  MA_2023_1_MCOU_24208083060: 22 classes\n",
      "    NDC: 24208083060, State: MA, Year: 2023\n",
      "    Classes: ['R02AB', 'A01AC', 'H02AB', 'R01AD', 'S03BA', 'A01AB', 'C05AA', 'A07AA', 'S01BA', 'S02AA', 'B05CA', 'S01CB', 'D07AB', 'S02BA', 'D06AX', 'S01AA', 'D07XB', 'S01CA', 'D10AA', 'J01GB', 'J01XB', 'S03AA']\n",
      "\n",
      "  MA_2023_1_MCOU_24208079535: 22 classes\n",
      "    NDC: 24208079535, State: MA, Year: 2023\n",
      "    Classes: ['S03BA', 'J01XB', 'C05AA', 'R02AB', 'A01AC', 'H02AB', 'R01AD', 'A01AB', 'A07AA', 'S01CB', 'S01BA', 'S02AA', 'B05CA', 'D07AB', 'D07XB', 'S02BA', 'D06AX', 'S01AA', 'S01CA', 'D10AA', 'J01GB', 'S03AA']\n",
      "\n",
      "  MA_2023_1_MCOU_24208083060: 22 classes\n",
      "    NDC: 24208083060, State: MA, Year: 2023\n",
      "    Classes: ['R02AB', 'A01AC', 'H02AB', 'R01AD', 'S03BA', 'A01AB', 'C05AA', 'A07AA', 'S01BA', 'S02AA', 'B05CA', 'S01CB', 'D07AB', 'S02BA', 'D06AX', 'S01AA', 'D07XB', 'S01CA', 'D10AA', 'J01GB', 'J01XB', 'S03AA']\n",
      "\n",
      "  TX_2023_4_FFSU_24208083060: 22 classes\n",
      "    NDC: 24208083060, State: TX, Year: 2023\n",
      "    Classes: ['A01AB', 'C05AA', 'A07AA', 'S01BA', 'S02AA', 'B05CA', 'S01CB', 'D07AB', 'S02BA', 'D06AX', 'S01AA', 'D07XB', 'S01CA', 'D10AA', 'J01GB', 'J01XB', 'S03AA', 'R02AB', 'A01AC', 'H02AB', 'R01AD', 'S03BA']\n",
      "\n",
      "  ME_2023_3_FFSU_24208079535: 22 classes\n",
      "    NDC: 24208079535, State: ME, Year: 2023\n",
      "    Classes: ['S01AA', 'D10AA', 'J01GB', 'J01XB', 'S03AA', 'A01AC', 'H02AB', 'R01AD', 'S03BA', 'C05AA', 'R02AB', 'A07AA', 'A01AB', 'S02AA', 'B05CA', 'S01CB', 'S01BA', 'D07AB', 'S02BA', 'D06AX', 'D07XB', 'S01CA']\n",
      "\n",
      "  TX_2023_4_FFSU_24208083060: 22 classes\n",
      "    NDC: 24208083060, State: TX, Year: 2023\n",
      "    Classes: ['A01AB', 'C05AA', 'A07AA', 'S01BA', 'S02AA', 'B05CA', 'S01CB', 'D07AB', 'S02BA', 'D06AX', 'S01AA', 'D07XB', 'S01CA', 'D10AA', 'J01GB', 'J01XB', 'S03AA', 'R02AB', 'A01AC', 'H02AB', 'R01AD', 'S03BA']\n",
      "\n",
      "  ME_2023_3_FFSU_24208079535: 22 classes\n",
      "    NDC: 24208079535, State: ME, Year: 2023\n",
      "    Classes: ['S01AA', 'D10AA', 'J01GB', 'J01XB', 'S03AA', 'A01AC', 'H02AB', 'R01AD', 'S03BA', 'C05AA', 'R02AB', 'A07AA', 'A01AB', 'S02AA', 'B05CA', 'S01CB', 'S01BA', 'D07AB', 'S02BA', 'D06AX', 'D07XB', 'S01CA']\n",
      "\n",
      "  ME_2023_3_FFSU_24208083060: 22 classes\n",
      "    NDC: 24208083060, State: ME, Year: 2023\n",
      "    Classes: ['R02AB', 'A01AC', 'H02AB', 'R01AD', 'S03BA', 'A01AB', 'C05AA', 'A07AA', 'S01BA', 'S02AA', 'B05CA', 'S01CB', 'D07AB', 'S02BA', 'D06AX', 'S01AA', 'D07XB', 'S01CA', 'D10AA', 'J01GB', 'J01XB', 'S03AA']\n",
      "\n",
      "  NE_2023_2_MCOU_24208079535: 22 classes\n",
      "    NDC: 24208079535, State: NE, Year: 2023\n",
      "    Classes: ['S03AA', 'S03BA', 'J01XB', 'C05AA', 'R02AB', 'A01AC', 'H02AB', 'R01AD', 'A01AB', 'A07AA', 'S01CB', 'S01BA', 'S02AA', 'B05CA', 'D07AB', 'D07XB', 'S02BA', 'D06AX', 'S01AA', 'S01CA', 'D10AA', 'J01GB']\n",
      "\n",
      "  ME_2023_3_FFSU_24208083060: 22 classes\n",
      "    NDC: 24208083060, State: ME, Year: 2023\n",
      "    Classes: ['R02AB', 'A01AC', 'H02AB', 'R01AD', 'S03BA', 'A01AB', 'C05AA', 'A07AA', 'S01BA', 'S02AA', 'B05CA', 'S01CB', 'D07AB', 'S02BA', 'D06AX', 'S01AA', 'D07XB', 'S01CA', 'D10AA', 'J01GB', 'J01XB', 'S03AA']\n",
      "\n",
      "  NE_2023_2_MCOU_24208079535: 22 classes\n",
      "    NDC: 24208079535, State: NE, Year: 2023\n",
      "    Classes: ['S03AA', 'S03BA', 'J01XB', 'C05AA', 'R02AB', 'A01AC', 'H02AB', 'R01AD', 'A01AB', 'A07AA', 'S01CB', 'S01BA', 'S02AA', 'B05CA', 'D07AB', 'D07XB', 'S02BA', 'D06AX', 'S01AA', 'S01CA', 'D10AA', 'J01GB']\n",
      "\n",
      "  NE_2023_2_MCOU_24208083060: 22 classes\n",
      "    NDC: 24208083060, State: NE, Year: 2023\n",
      "    Classes: ['S01BA', 'S02AA', 'B05CA', 'S01CB', 'D07AB', 'S02BA', 'D06AX', 'S01AA', 'D07XB', 'S01CA', 'D10AA', 'J01GB', 'J01XB', 'S03AA', 'R02AB', 'A01AC', 'H02AB', 'R01AD', 'S03BA', 'A01AB', 'C05AA', 'A07AA']\n",
      "\n",
      "  KS_2023_3_MCOU_24208079535: 22 classes\n",
      "    NDC: 24208079535, State: KS, Year: 2023\n",
      "    Classes: ['C05AA', 'R02AB', 'A07AA', 'A01AB', 'S01BA', 'S02AA', 'B05CA', 'S01CB', 'D07AB', 'S02BA', 'D06AX', 'D07XB', 'S01CA', 'D10AA', 'J01GB', 'S01AA', 'J01XB', 'S03AA', 'A01AC', 'H02AB', 'R01AD', 'S03BA']\n",
      "\n",
      "  NE_2023_2_MCOU_24208083060: 22 classes\n",
      "    NDC: 24208083060, State: NE, Year: 2023\n",
      "    Classes: ['S01BA', 'S02AA', 'B05CA', 'S01CB', 'D07AB', 'S02BA', 'D06AX', 'S01AA', 'D07XB', 'S01CA', 'D10AA', 'J01GB', 'J01XB', 'S03AA', 'R02AB', 'A01AC', 'H02AB', 'R01AD', 'S03BA', 'A01AB', 'C05AA', 'A07AA']\n",
      "\n",
      "  KS_2023_3_MCOU_24208079535: 22 classes\n",
      "    NDC: 24208079535, State: KS, Year: 2023\n",
      "    Classes: ['C05AA', 'R02AB', 'A07AA', 'A01AB', 'S01BA', 'S02AA', 'B05CA', 'S01CB', 'D07AB', 'S02BA', 'D06AX', 'D07XB', 'S01CA', 'D10AA', 'J01GB', 'S01AA', 'J01XB', 'S03AA', 'A01AC', 'H02AB', 'R01AD', 'S03BA']\n",
      "\n",
      "  KS_2023_3_MCOU_24208083060: 22 classes\n",
      "    NDC: 24208083060, State: KS, Year: 2023\n",
      "    Classes: ['S01CB', 'S01BA', 'D07AB', 'S02AA', 'S02BA', 'B05CA', 'D06AX', 'D07XB', 'S01CA', 'S01AA', 'D10AA', 'J01GB', 'S03AA', 'A01AC', 'H02AB', 'R01AD', 'S03BA', 'J01XB', 'C05AA', 'R02AB', 'A07AA', 'A01AB']\n",
      "\n",
      "  FL_2023_3_MCOU_61314063006: 22 classes\n",
      "    NDC: 61314063006, State: FL, Year: 2023\n",
      "    Classes: ['J01XB', 'S03AA', 'A01AC', 'H02AB', 'R01AD', 'S03BA', 'C05AA', 'R02AB', 'A07AA', 'A01AB', 'S02AA', 'B05CA', 'S01CB', 'S01BA', 'D07AB', 'S02BA', 'D06AX', 'D07XB', 'S01CA', 'D10AA', 'S01AA', 'J01GB']\n",
      "\n",
      "  KS_2023_3_MCOU_24208083060: 22 classes\n",
      "    NDC: 24208083060, State: KS, Year: 2023\n",
      "    Classes: ['S01CB', 'S01BA', 'D07AB', 'S02AA', 'S02BA', 'B05CA', 'D06AX', 'D07XB', 'S01CA', 'S01AA', 'D10AA', 'J01GB', 'S03AA', 'A01AC', 'H02AB', 'R01AD', 'S03BA', 'J01XB', 'C05AA', 'R02AB', 'A07AA', 'A01AB']\n",
      "\n",
      "  FL_2023_3_MCOU_61314063006: 22 classes\n",
      "    NDC: 61314063006, State: FL, Year: 2023\n",
      "    Classes: ['J01XB', 'S03AA', 'A01AC', 'H02AB', 'R01AD', 'S03BA', 'C05AA', 'R02AB', 'A07AA', 'A01AB', 'S02AA', 'B05CA', 'S01CB', 'S01BA', 'D07AB', 'S02BA', 'D06AX', 'D07XB', 'S01CA', 'D10AA', 'S01AA', 'J01GB']\n",
      "\n",
      "\n",
      "============================================================\n",
      "FETCHING ATC CLASS NAMES\n",
      "============================================================\n",
      "Using cache: c:\\Users\\Lilian\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\cache_files\\atc_names_cache\n",
      "\n",
      "============================================================\n",
      "FETCHING ATC CLASS NAMES\n",
      "============================================================\n",
      "Using cache: c:\\Users\\Lilian\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\cache_files\\atc_names_cache\n",
      "\n",
      "Creating ATC3 and ATC2 columns from ATC4...\n",
      "\n",
      "Creating ATC3 and ATC2 columns from ATC4...\n",
      "\n",
      "Unique codes to fetch:\n",
      "  ATC4: 611\n",
      "  ATC3: 211\n",
      "  ATC2: 89\n",
      "\n",
      "Unique codes to fetch:\n",
      "  ATC4: 611\n",
      "  ATC3: 211\n",
      "  ATC2: 89\n",
      "\n",
      "Fetching ATC4 names...\n",
      "Fetching ATC3 names...\n",
      "Fetching ATC2 names...\n",
      "\n",
      "Total processing time: 0.0 minutes\n",
      "\n",
      "Applying names to dataframe...\n",
      "\n",
      "Fetching ATC4 names...\n",
      "Fetching ATC3 names...\n",
      "Fetching ATC2 names...\n",
      "\n",
      "Total processing time: 0.0 minutes\n",
      "\n",
      "Applying names to dataframe...\n",
      "\n",
      "ATC names added successfully!\n",
      "\n",
      "Sample output:\n",
      "\n",
      "ATC names added successfully!\n",
      "\n",
      "Sample output:\n",
      "           NDC                   record_id ATC4 Class                                           ATC4_Name ATC3 Class                                     ATC3_Name ATC2 Class               ATC2_Name\n",
      "0  00002143380  AK_2023_4_FFSU_00002143380      A10BJ           Glucagon-like peptide-1 (GLP-1) analogues       A10B  BLOOD GLUCOSE LOWERING DRUGS, EXCL. INSULINS        A10  DRUGS USED IN DIABETES\n",
      "1  00002143480  AK_2023_4_FFSU_00002143480      A10BJ           Glucagon-like peptide-1 (GLP-1) analogues       A10B  BLOOD GLUCOSE LOWERING DRUGS, EXCL. INSULINS        A10  DRUGS USED IN DIABETES\n",
      "2  00002143611  AK_2023_4_FFSU_00002143611      N02CD  Calcitonin gene-related peptide (CGRP) antagonists       N02C                     ANTIMIGRAINE PREPARATIONS        N02              ANALGESICS\n",
      "3  00002144511  AK_2023_4_FFSU_00002144511      L04AC                              Interleukin inhibitors       L04A                            IMMUNOSUPPRESSANTS        L04      IMMUNOSUPPRESSANTS\n",
      "4  00002145780  AK_2023_4_FFSU_00002145780      A10BX  Other blood glucose lowering drugs, excl. insulins       A10B  BLOOD GLUCOSE LOWERING DRUGS, EXCL. INSULINS        A10  DRUGS USED IN DIABETES\n",
      "           NDC                   record_id ATC4 Class                                           ATC4_Name ATC3 Class                                     ATC3_Name ATC2 Class               ATC2_Name\n",
      "0  00002143380  AK_2023_4_FFSU_00002143380      A10BJ           Glucagon-like peptide-1 (GLP-1) analogues       A10B  BLOOD GLUCOSE LOWERING DRUGS, EXCL. INSULINS        A10  DRUGS USED IN DIABETES\n",
      "1  00002143480  AK_2023_4_FFSU_00002143480      A10BJ           Glucagon-like peptide-1 (GLP-1) analogues       A10B  BLOOD GLUCOSE LOWERING DRUGS, EXCL. INSULINS        A10  DRUGS USED IN DIABETES\n",
      "2  00002143611  AK_2023_4_FFSU_00002143611      N02CD  Calcitonin gene-related peptide (CGRP) antagonists       N02C                     ANTIMIGRAINE PREPARATIONS        N02              ANALGESICS\n",
      "3  00002144511  AK_2023_4_FFSU_00002144511      L04AC                              Interleukin inhibitors       L04A                            IMMUNOSUPPRESSANTS        L04      IMMUNOSUPPRESSANTS\n",
      "4  00002145780  AK_2023_4_FFSU_00002145780      A10BX  Other blood glucose lowering drugs, excl. insulins       A10B  BLOOD GLUCOSE LOWERING DRUGS, EXCL. INSULINS        A10  DRUGS USED IN DIABETES\n",
      "\n",
      "============================================================\n",
      "PREPARING FINAL DATAFRAME\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PREPARING FINAL DATAFRAME\n",
      "============================================================\n",
      "\n",
      "Scaling units...\n",
      "\n",
      "Scaling units...\n",
      "\n",
      "Final statistics:\n",
      "Total records: 4,454,193\n",
      "Records with ATC4 mapping: 4,426,568 (99.4%)\n",
      "Total Units Reimbursed: 112.21 Billion\n",
      "Total Prescriptions: 1525.88 Million\n",
      "\n",
      "Final statistics:\n",
      "Total records: 4,454,193\n",
      "Records with ATC4 mapping: 4,426,568 (99.4%)\n",
      "Total Units Reimbursed: 112.21 Billion\n",
      "Total Prescriptions: 1525.88 Million\n",
      "\n",
      "============================================================\n",
      "DATA EXPORT COMPLETE\n",
      "============================================================\n",
      "Exported to: c:\\Users\\Lilian\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\merged_data\\merged_NEWdata_2023.csv\n",
      "Total rows exported: 4,454,193\n",
      "Columns: Utilization Type, State, NDC, Labeler Code, Product Code, Package Size, Year, Quarter, Suppression Used, Product Name, Units Reimbursed, Number of Prescriptions, Total Amount Reimbursed, Medicaid Amount Reimbursed, Non Medicaid Amount Reimbursed, record_id, ATC4 Class, ATC3 Class, ATC2 Class, ATC4_Name, ATC3_Name, ATC2_Name\n",
      "\n",
      "============================================================\n",
      "DATA EXPORT COMPLETE\n",
      "============================================================\n",
      "Exported to: c:\\Users\\Lilian\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\merged_data\\merged_NEWdata_2023.csv\n",
      "Total rows exported: 4,454,193\n",
      "Columns: Utilization Type, State, NDC, Labeler Code, Product Code, Package Size, Year, Quarter, Suppression Used, Product Name, Units Reimbursed, Number of Prescriptions, Total Amount Reimbursed, Medicaid Amount Reimbursed, Non Medicaid Amount Reimbursed, record_id, ATC4 Class, ATC3 Class, ATC2 Class, ATC4_Name, ATC3_Name, ATC2_Name\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Lilian\\\\OneDrive - purdue.edu\\\\VS code\\\\Data\\\\ATC\\\\merged_data\\\\merged_NEWdata_2023.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = NDCATCAnalyzer(year=2023)\n",
    "analyzer.clean_sdud_data()           # Clean SDUD data\n",
    "analyzer.adding_key()                # Add record_id key\n",
    "analyzer.generate_ndc_txt()          # Generate NDC text file\n",
    "analyzer.analyze_atc4_mapping()      # Merge ATC4 by record_id & NDC\n",
    "analyzer.analyze_atc4_distribution() # Analyze distribution\n",
    "analyzer.fetch_atc_names()           # Fetch ATC4, ATC3, ATC2 names\n",
    "analyzer.prepare_final_dataframe()   # Scale units and finalize\n",
    "analyzer.export_merged_data()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8a9cdb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           NDC                   record_id ATC4 Class\n",
      "0  63323010601  WV_2023_1_MCOU_63323010601      A06AD\n",
      "1  65862016901  NC_2023_2_MCOU_65862016901      C07AB\n",
      "2  51672206902  MN_2023_1_MCOU_51672206902      S02BA\n",
      "3  65162083594  CT_2023_3_FFSU_65162083594      J05AB\n",
      "4  16714098502  IA_2023_4_MCOU_16714098502      D07AB\n",
      "5    409653311  VA_2023_1_MCOU_00409653311      A07AA\n",
      "6  65162008203  NV_2023_3_MCOU_65162008203      N05AE\n",
      "7  59651003212  MA_2023_3_FFSU_59651003212      M02AA\n",
      "8  67877025130  NY_2023_3_FFSU_67877025130      D07AB\n",
      "9  42858011830  SD_2023_3_FFSU_42858011830      C01BB\n",
      "Index(['NDC', 'record_id', 'ATC4 Class'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Looking at the new key identifier\n",
    "path=r\"C:\\Users\\lholguin\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\ATC4_classes\\NDCNEW_2023_ATC4_classes.csv\"\n",
    "df_atcnew=pd.read_csv(path)\n",
    "print(df_atcnew.head(10))\n",
    "print(df_atcnew.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_totals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check total sums\n",
    "total_units = analyzer.df_merged['Units Reimbursed'].sum()\n",
    "total_prescriptions = analyzer.df_merged['Number of Prescriptions'].sum()\n",
    "\n",
    "print(f\"\\nTotal Statistics:\")\n",
    "print(f\"Total Units Reimbursed: {total_units:.4f} Billion\")\n",
    "print(f\"Total Number of Prescriptions: {total_prescriptions:.4f} Million\")\n",
    "print(f\"Total rows in dataset: {len(analyzer.df_merged):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc8e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Year vs Top 5 ATC Classes for a Single State\n",
    "if 'analyzer' in globals() and hasattr(analyzer, 'df_merged') and analyzer.df_merged is not None:\n",
    "    # Select state to analyze (change this to any state you want)\n",
    "    target_state = 'IN'  # Indiana - change to any state code\n",
    "    \n",
    "    # Filter data for the target state\n",
    "    state_data = analyzer.df_merged[analyzer.df_merged['State'] == target_state].copy()\n",
    "    \n",
    "    if len(state_data) > 0:\n",
    "        print(f\"Analyzing state: {target_state}\")\n",
    "        print(f\"Total records for {target_state}: {len(state_data):,}\")\n",
    "        \n",
    "        # Group by Quarter (year) and ATC4 Class, sum prescriptions\n",
    "        quarterly_atc = state_data.groupby(['Quarter', 'ATC4 Class']).agg({\n",
    "            'Number of Prescriptions': 'sum',\n",
    "            'Units Reimbursed': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Find top 5 ATC classes by total prescriptions across all quarters\n",
    "        top_atc_classes = quarterly_atc.groupby('ATC4 Class')['Number of Prescriptions'].sum().sort_values(ascending=False).head(5)\n",
    "        top_5_classes = top_atc_classes.index.tolist()\n",
    "        \n",
    "        print(f\"\\nTop 5 ATC Classes in {target_state}:\")\n",
    "        for i, atc_class in enumerate(top_5_classes, 1):\n",
    "            total_prescriptions = top_atc_classes[atc_class]\n",
    "            print(f\"{i}. {atc_class}: {total_prescriptions:.2f} million prescriptions\")\n",
    "        \n",
    "        # Filter data for top 5 classes only\n",
    "        top_5_data = quarterly_atc[quarterly_atc['ATC4 Class'].isin(top_5_classes)].copy()\n",
    "        \n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot each ATC class as a separate line\n",
    "        for atc_class in top_5_classes:\n",
    "            class_data = top_5_data[top_5_data['ATC4 Class'] == atc_class]\n",
    "            plt.plot(class_data['Quarter'], class_data['Number of Prescriptions'], \n",
    "                    marker='o', linewidth=2, label=atc_class)\n",
    "        \n",
    "        plt.title(f'Top 5 ATC Classes by Quarter in {target_state} ({analyzer.year})', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Quarter', fontsize=12)\n",
    "        plt.ylabel('Number of Prescriptions (Millions)', fontsize=12)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Show summary table\n",
    "        print(f\"\\nSummary table for {target_state}:\")\n",
    "        summary_pivot = top_5_data.pivot(index='Quarter', columns='ATC4 Class', values='Number of Prescriptions').fillna(0)\n",
    "        print(summary_pivot.round(2))\n",
    "        \n",
    "    else:\n",
    "        print(f\"No data found for state: {target_state}\")\n",
    "        print(f\"Available states: {sorted(analyzer.df_merged['State'].unique())}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Please run the NDC-ATC workflow first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058adea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Plot with Filters using Plotly\n",
    "if 'analyzer' in globals() and hasattr(analyzer, 'df_merged') and analyzer.df_merged is not None:\n",
    "    \n",
    "    # Get unique values for filters\n",
    "    states = ['All States']+sorted(analyzer.df_merged['State'].unique())\n",
    "    years = sorted(analyzer.df_merged['Year'].unique()) if 'Year' in analyzer.df_merged.columns else [analyzer.year]\n",
    "    quarters = ['All'] + sorted([str(q) for q in analyzer.df_merged['Quarter'].unique()])\n",
    "    \n",
    "    # Create dropdown widgets\n",
    "    state_dropdown = widgets.Dropdown(\n",
    "        options=states,\n",
    "        value=states[0],\n",
    "        description='State:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    \n",
    "    year_dropdown = widgets.Dropdown(\n",
    "        options=years,\n",
    "        value=years[0],\n",
    "        description='Year:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    \n",
    "    quarter_dropdown = widgets.Dropdown(\n",
    "        options=quarters,\n",
    "        value='All',\n",
    "        description='Quarter:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    \n",
    "    top_n_slider = widgets.IntSlider(\n",
    "        value=10,\n",
    "        min=5,\n",
    "        max=20,\n",
    "        step=5,\n",
    "        description='Top N:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    \n",
    "    # Create output widget\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def update_plot(change=None):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            state = state_dropdown.value\n",
    "            year = year_dropdown.value\n",
    "            quarter = quarter_dropdown.value\n",
    "            top_n = top_n_slider.value\n",
    "            #Filtering\n",
    "            if state=='All States':\n",
    "                filtered_data=analyzer.df_merged.copy()\n",
    "            else:\n",
    "                filtered_data = analyzer.df_merged[analyzer.df_merged['State'] == state].copy()\n",
    "\n",
    "            # Filter data by state and year\n",
    "            filtered_data = analyzer.df_merged[\n",
    "                analyzer.df_merged['State'] == state\n",
    "            ].copy()\n",
    "            \n",
    "            if 'Year' in analyzer.df_merged.columns:\n",
    "                filtered_data = filtered_data[filtered_data['Year'] == year]\n",
    "            \n",
    "            # Filter by quarter if not 'All'\n",
    "            if quarter != 'All':\n",
    "                filtered_data = filtered_data[filtered_data['Quarter'] == int(quarter)]\n",
    "            \n",
    "            quarter_text = f\"Q{quarter}\" if quarter != 'All' else \"All Quarters\"\n",
    "            \n",
    "            if len(filtered_data) == 0:\n",
    "                print(f\"No data available for State: {state}, Year: {year}, Quarter: {quarter_text}\")\n",
    "                return\n",
    "            \n",
    "            # Group by ATC3 Class and sum metrics\n",
    "            atc3_summary = filtered_data.groupby(['ATC3 Class', 'ATC3_Name']).agg({\n",
    "                'Units Reimbursed': 'sum',\n",
    "                'Number of Prescriptions': 'sum'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Create labels combining code and name\n",
    "            atc3_summary['Label'] = atc3_summary['ATC3 Class'] + ': ' + atc3_summary['ATC3_Name']\n",
    "            \n",
    "            # Get top N for each metric\n",
    "            top_units = atc3_summary.nlargest(top_n, 'Units Reimbursed').sort_values('Units Reimbursed', ascending=True)\n",
    "            top_prescriptions = atc3_summary.nlargest(top_n, 'Number of Prescriptions').sort_values('Number of Prescriptions', ascending=True)\n",
    "            \n",
    "            # Create subplots\n",
    "            fig = make_subplots(\n",
    "                rows=2, cols=1,\n",
    "                subplot_titles=(\n",
    "                    f'Top {top_n} ATC3 Classes by Units Reimbursed<br>{state} - {year} - {quarter_text}',\n",
    "                    f'Top {top_n} ATC3 Classes by Number of Prescriptions<br>{state} - {year} - {quarter_text}'\n",
    "                ),\n",
    "                vertical_spacing=0.15\n",
    "            )\n",
    "            \n",
    "            # Add first bar chart (Units Reimbursed)\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    y=top_units['Label'],\n",
    "                    x=top_units['Units Reimbursed'],\n",
    "                    orientation='h',\n",
    "                    marker=dict(\n",
    "                        color=top_units['Units Reimbursed'],\n",
    "                        colorscale='YlGnBu',\n",
    "                        showscale=False\n",
    "                    ),\n",
    "                    text=[f'{val:.2f}B' for val in top_units['Units Reimbursed']],\n",
    "                    textposition='outside',\n",
    "                    hovertemplate='<b>%{y}</b><br>Units: %{x:.2f}B<extra></extra>'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # Add second bar chart (Number of Prescriptions)\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    y=top_prescriptions['Label'],\n",
    "                    x=top_prescriptions['Number of Prescriptions'],\n",
    "                    orientation='h',\n",
    "                    marker=dict(\n",
    "                        color=top_prescriptions['Number of Prescriptions'],\n",
    "                        colorscale='OrRd',\n",
    "                        showscale=False\n",
    "                    ),\n",
    "                    text=[f'{val:.2f}M' for val in top_prescriptions['Number of Prescriptions']],\n",
    "                    textposition='outside',\n",
    "                    hovertemplate='<b>%{y}</b><br>Prescriptions: %{x:.2f}M<extra></extra>'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            # Update layout\n",
    "            fig.update_xaxes(title_text='Units Reimbursed (Billions)', row=1, col=1, gridcolor='lightgray')\n",
    "            fig.update_xaxes(title_text='Number of Prescriptions (Millions)', row=2, col=1, gridcolor='lightgray')\n",
    "            \n",
    "            fig.update_layout(\n",
    "                height=800,\n",
    "                showlegend=False,\n",
    "                plot_bgcolor='white',\n",
    "                font=dict(size=10)\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "            # Print summary statistics\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Summary for {state} - {year} - {quarter_text}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"\\nTotal ATC3 Classes: {len(atc3_summary)}\")\n",
    "            print(f\"Total Units Reimbursed: {filtered_data['Units Reimbursed'].sum():.2f} Billion\")\n",
    "            print(f\"Total Prescriptions: {filtered_data['Number of Prescriptions'].sum():.2f} Million\")\n",
    "    \n",
    "    # Attach the update function to dropdown changes\n",
    "    state_dropdown.observe(update_plot, names='value')\n",
    "    year_dropdown.observe(update_plot, names='value')\n",
    "    quarter_dropdown.observe(update_plot, names='value')\n",
    "    top_n_slider.observe(update_plot, names='value')\n",
    "    \n",
    "    # Display widgets and output\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Interactive ATC3 Class Analysis\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    display(widgets.VBox([state_dropdown, year_dropdown, quarter_dropdown, top_n_slider, output]))\n",
    "    \n",
    "    # Generate initial plot\n",
    "    update_plot()\n",
    "    \n",
    "else:\n",
    "    print(\"Please run the NDC-ATC workflow first!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
