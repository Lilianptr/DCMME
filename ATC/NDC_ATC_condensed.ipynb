{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b3e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import shelve\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f7f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "user=\"lholguin\"\n",
    "#user in personal pc1 <- \"asus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73aebaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDCATCProcessor:\n",
    "\n",
    "    def __init__(self, year, base_path=None):\n",
    "        self.year = year\n",
    "        self.base_path = base_path or rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        self.df_cleaned = None\n",
    "        self.df_merged = None\n",
    "        self.atc_mapping = None\n",
    "        \n",
    "    def clean_sdud_data(self):\n",
    "\n",
    "        csv_file = os.path.join(self.base_path, f\"SDUD\\\\SDUD{self.year}.csv\")\n",
    "        print(f\"Reading CSV: {csv_file}\")\n",
    "        \n",
    "        df = pd.read_csv(csv_file, dtype={'NDC': 'object'})\n",
    "        print(f\"Initial rows: {len(df):,}\")\n",
    "        \n",
    "        # Filter data\n",
    "        df_filtered = df.dropna(subset=['Units Reimbursed', 'Number of Prescriptions']) #dropping na values\n",
    "        df_filtered = df_filtered[df_filtered['State'] != 'XX']\n",
    "        \n",
    "        print(f\"After cleaning: {len(df_filtered):,} rows, {df_filtered['NDC'].nunique():,} unique NDCs\")\n",
    "        \n",
    "        self.df_cleaned = df_filtered\n",
    "        return self.df_cleaned\n",
    "    \n",
    "    def adding_key(self):\n",
    "        if self.df_cleaned is None:\n",
    "            raise ValueError(\"Run clean_sdud_data() first\")\n",
    "        \n",
    "        self.df_cleaned['record_id'] = (\n",
    "            self.df_cleaned['State'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['Year'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['Quarter'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['Utilization Type'].astype(str) + \"_\" +\n",
    "            self.df_cleaned['NDC'].astype(str)\n",
    "        )\n",
    "        \n",
    "        print(f\"Created {len(self.df_cleaned):,} record IDs\")\n",
    "        return self.df_cleaned\n",
    "    \n",
    "    def generate_ndc_txt(self, output_filename=None):\n",
    "        if 'record_id' not in self.df_cleaned.columns:\n",
    "            raise ValueError(\"Run adding_key() first\")\n",
    "            \n",
    "        output_filename = output_filename or f\"NDCNEW_{self.year}.txt\"\n",
    "        output_path = os.path.join(self.base_path, f\"ATC\\\\text_files\\\\{output_filename}\")\n",
    "        \n",
    "        unique_pairs = self.df_cleaned[['NDC', 'record_id']].drop_duplicates()\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(\"NDC\\trecord_id\\n\")\n",
    "            for _, row in unique_pairs.iterrows():\n",
    "                f.write(f\"{row['NDC']}\\t{row['record_id']}\\n\")\n",
    "        \n",
    "        print(f\"Exported {unique_pairs['record_id'].nunique():,} unique records to {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def analyze_atc4_mapping(self):\n",
    "\n",
    "        if 'record_id' not in self.df_cleaned.columns:\n",
    "            raise ValueError(\"Run adding_key() first\")\n",
    "            \n",
    "        atc4_path = os.path.join(self.base_path, f\"ATC\\\\ATC4_classes\\\\NDCNEW_{self.year}_ATC4_classes.csv\")\n",
    "        \n",
    "        # Load ATC4 mapping\n",
    "        df_atc4 = pd.read_csv(atc4_path, dtype={'NDC': 'object', 'record_id': 'string'})\n",
    "        df_atc4['NDC'] = df_atc4['NDC'].str.zfill(11)\n",
    "        \n",
    "        print(f\"ATC4 file: {len(df_atc4):,} rows, {df_atc4['NDC'].nunique():,} unique NDCs\")\n",
    "        \n",
    "        # Ensure consistent types\n",
    "        self.df_cleaned['record_id'] = self.df_cleaned['record_id'].astype('string')\n",
    "        self.df_cleaned['NDC'] = self.df_cleaned['NDC'].astype('object')\n",
    "        \n",
    "        # Merge on both record_id and NDC\n",
    "        self.atc_mapping = pd.merge(\n",
    "            self.df_cleaned,\n",
    "            df_atc4[['record_id', 'NDC', 'ATC4 Class']],\n",
    "            on=['record_id', 'NDC'],\n",
    "            how='left'\n",
    "        )\n",
    "        #deduplication after merge by record_id\n",
    "        before_count=len(self.atc_mapping) \n",
    "        self.atc_mapping=self.atc_mapping.drop_duplicates(subset='record_id', keep='first')\n",
    "        \n",
    "        total = len(self.atc_mapping)\n",
    "     \n",
    "        mapped = self.atc_mapping['ATC4 Class'].notna().sum()\n",
    "        print(f\"Merged: {total:,} records, {mapped:,} with ATC4 ({mapped/total*100:.1f}%)\")\n",
    "        \n",
    "        missing = total - mapped\n",
    "        if missing > 0:\n",
    "            print(f\"Missing: {missing:,} records, {self.atc_mapping[self.atc_mapping['ATC4 Class'].isna()]['NDC'].nunique():,} unique NDCs\")\n",
    "        \n",
    "        return self.atc_mapping\n",
    "    \n",
    "    def analyze_atc_distribution(self, level='ATC3'):\n",
    "\n",
    "        if self.atc_mapping is None:\n",
    "            raise ValueError(\"Run analyze_atc4_mapping() first\")\n",
    "        \n",
    "        records = self.atc_mapping[self.atc_mapping['ATC4 Class'].notna()].copy()\n",
    "        \n",
    "        if len(records) == 0:\n",
    "            print(\"No records with valid ATC4 mappings.\")\n",
    "            return None\n",
    "        \n",
    "        # Create ATC level column if needed\n",
    "        if level == 'ATC3':\n",
    "            records['ATC3 Class'] = records['ATC4 Class'].str[:4]\n",
    "            class_col = 'ATC3 Class'\n",
    "        elif level == 'ATC2':\n",
    "            records['ATC2 Class'] = records['ATC4 Class'].str[:3]\n",
    "            class_col = 'ATC2 Class'\n",
    "        else:\n",
    "            class_col = 'ATC4 Class'\n",
    "        \n",
    "        # Count classes per record_id\n",
    "        per_record = records.groupby('record_id')[class_col].nunique().reset_index()\n",
    "        per_record.columns = ['record_id', 'num_classes']\n",
    "        \n",
    "        distribution = per_record['num_classes'].value_counts().sort_index()\n",
    "        \n",
    "        print(f\"\\n{level} CLASSES PER RECORD_ID:\")\n",
    "        for n_classes, count in distribution.items():\n",
    "            pct = (count / len(per_record)) * 100\n",
    "            print(f\"  {n_classes} class(es): {count:,} records ({pct:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"  Avg {level} per record: {per_record['num_classes'].mean():.2f}\")\n",
    "        print(f\"  Max {level} per record: {per_record['num_classes'].max()}\")\n",
    "        \n",
    "        return per_record\n",
    "\n",
    "    def fetch_atc_names(self, cache_path=None):\n",
    "        \"\"\"Fetch ATC class names (ATC4, ATC3, ATC2) from RxNav API.\"\"\"\n",
    "        if self.atc_mapping is None:\n",
    "            raise ValueError(\"Must run analyze_atc4_mapping() first\")\n",
    "        \n",
    "        if cache_path is None:\n",
    "            cache_path = os.path.join(self.base_path, \"ATC\\\\cache_files\\\\atc_names_cache\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"FETCHING ATC CLASS NAMES\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Using cache: {cache_path}\")\n",
    "        \n",
    "        # Get only records with valid ATC4 mappings\n",
    "        df_with_atc = self.atc_mapping[self.atc_mapping['ATC4 Class'].notna()].copy()\n",
    "        \n",
    "        # Create ATC3 and ATC2 columns from ATC4\n",
    "        print(\"\\nCreating ATC3 and ATC2 columns from ATC4...\")\n",
    "        df_with_atc['ATC3 Class'] = df_with_atc['ATC4 Class'].str[:4]\n",
    "        df_with_atc['ATC2 Class'] = df_with_atc['ATC4 Class'].str[:3]\n",
    "        \n",
    "        # Get unique codes for each level\n",
    "        unique_atc4 = df_with_atc['ATC4 Class'].dropna().unique()\n",
    "        unique_atc3 = df_with_atc['ATC3 Class'].dropna().unique()\n",
    "        unique_atc2 = df_with_atc['ATC2 Class'].dropna().unique()\n",
    "        \n",
    "        # Filter out invalid codes\n",
    "        unique_atc4 = [c for c in unique_atc4 if c not in ['No ATC Mapping Found', 'No RxCUI Found', '']]\n",
    "        unique_atc3 = [c for c in unique_atc3 if c not in ['No ATC Mapping Found', 'No RxCUI Found', '', 'No ', 'No']]\n",
    "        unique_atc2 = [c for c in unique_atc2 if c not in ['No ATC Mapping Found', 'No RxCUI Found', '', 'No ', 'No']]\n",
    "        \n",
    "        print(f\"\\nUnique codes to fetch:\")\n",
    "        print(f\"  ATC4: {len(unique_atc4)}\")\n",
    "        print(f\"  ATC3: {len(unique_atc3)}\")\n",
    "        print(f\"  ATC2: {len(unique_atc2)}\")\n",
    "        \n",
    "        # Build mappings\n",
    "        atc4_names = {}\n",
    "        atc3_names = {}\n",
    "        atc2_names = {}\n",
    "        \n",
    "        with shelve.open(cache_path) as cache:\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            print(\"\\nFetching ATC4 names...\")\n",
    "            for code in unique_atc4:\n",
    "                atc4_names[code] = self._get_atc_name(code, cache)\n",
    "            \n",
    "            print(\"Fetching ATC3 names...\")\n",
    "            for code in unique_atc3:\n",
    "                atc3_names[code] = self._get_atc_name(code, cache)\n",
    "            \n",
    "            print(\"Fetching ATC2 names...\")\n",
    "            for code in unique_atc2:\n",
    "                atc2_names[code] = self._get_atc_name(code, cache)\n",
    "            \n",
    "            print(f\"\\nTotal processing time: {(datetime.now() - start_time).total_seconds()/60:.1f} minutes\")\n",
    "        \n",
    "        # Apply names to all records in atc_mapping\n",
    "        print(\"\\nApplying names to dataframe...\")\n",
    "        self.atc_mapping['ATC3 Class'] = self.atc_mapping['ATC4 Class'].str[:4]\n",
    "        self.atc_mapping['ATC2 Class'] = self.atc_mapping['ATC4 Class'].str[:3]\n",
    "        \n",
    "        self.atc_mapping['ATC4_Name'] = self.atc_mapping['ATC4 Class'].map(atc4_names).fillna('')\n",
    "        self.atc_mapping['ATC3_Name'] = self.atc_mapping['ATC3 Class'].map(atc3_names).fillna('')\n",
    "        self.atc_mapping['ATC2_Name'] = self.atc_mapping['ATC2 Class'].map(atc2_names).fillna('')\n",
    "        \n",
    "        print(f\"\\nATC names added successfully!\")\n",
    "        print(\"\\nSample output:\")\n",
    "        sample = self.atc_mapping[self.atc_mapping['ATC4 Class'].notna()][['NDC', 'record_id', 'ATC4 Class', 'ATC4_Name', 'ATC3 Class', 'ATC3_Name', 'ATC2 Class', 'ATC2_Name']].head(5)\n",
    "        print(sample.to_string())\n",
    "        \n",
    "        return self.atc_mapping\n",
    "    \n",
    "    def prepare_final_dataframe(self):\n",
    "\n",
    "        if self.atc_mapping is None:\n",
    "            raise ValueError(\"Run fetch_atc_names() first\")\n",
    "        \n",
    "        self.df_merged = self.atc_mapping.copy()\n",
    "        \n",
    "        # Scale units\n",
    "        self.df_merged['Units Reimbursed'] = self.df_merged['Units Reimbursed'] / 1e9\n",
    "        self.df_merged['Number of Prescriptions'] = self.df_merged['Number of Prescriptions'] / 1e6\n",
    "        \n",
    "        total = len(self.df_merged)\n",
    "        mapped = self.df_merged['ATC4 Class'].notna().sum()\n",
    "        \n",
    "        print(f\"\\nFinal Statistics:\")\n",
    "        print(f\"  Records: {total:,} ({mapped:,} with ATC4, {mapped/total*100:.1f}%)\")\n",
    "        print(f\"  Units Reimbursed: {self.df_merged['Units Reimbursed'].sum():.2f} Billion\")\n",
    "        print(f\"  Prescriptions: {self.df_merged['Number of Prescriptions'].sum():.2f} Million\")\n",
    "        \n",
    "        return self.df_merged\n",
    "    \n",
    "    def _get_atc_name(self, atc_code, cache):\n",
    "        \"\"\"Helper: Fetch ATC name from RxNav API with caching.\"\"\"\n",
    "        cache_key = f\"atc_name:{atc_code}\"\n",
    "        if cache_key in cache:\n",
    "            return cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            url = f\"https://rxnav.nlm.nih.gov/REST/rxclass/class/byId.json?classId={atc_code}\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'rxclassMinConceptList' in data and 'rxclassMinConcept' in data['rxclassMinConceptList']:\n",
    "                concepts = data['rxclassMinConceptList']['rxclassMinConcept']\n",
    "                if concepts:\n",
    "                    name = concepts[0].get('className', '')\n",
    "                    cache[cache_key] = name\n",
    "                    return name\n",
    "            \n",
    "            cache[cache_key] = ''\n",
    "            return ''\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving {atc_code}: {e}\")\n",
    "            cache[cache_key] = ''\n",
    "            return ''\n",
    "\n",
    "    def export_merged_data(self, output_filename=None, show_details=True):\n",
    "\n",
    "        if self.df_merged is None:\n",
    "            raise ValueError(\"Run prepare_final_dataframe() first\")\n",
    "            \n",
    "        output_filename = output_filename or f\"merged_NEWdata_{self.year}.csv\"\n",
    "        output_path = os.path.join(self.base_path, f\"ATC\\\\merged_data\\\\{output_filename}\")\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Check duplicates\n",
    "        initial_count = len(self.df_merged)\n",
    "        duplicate_count = self.df_merged['record_id'].duplicated().sum()\n",
    "        \n",
    "        print(f\"\\nDeduplication Check:\")\n",
    "        print(f\"  Before: {initial_count:,} rows\")\n",
    "        print(f\"  Duplicates: {duplicate_count:,}\")\n",
    "        \n",
    "        # Show sample duplicates if requested\n",
    "        if show_details and duplicate_count > 0:\n",
    "            dup_records = self.df_merged[self.df_merged['record_id'].duplicated(keep=False)].sort_values('record_id')\n",
    "            sample_ids = dup_records['record_id'].unique()[:2]\n",
    "            \n",
    "            print(f\"\\nSample duplicate record_ids:\")\n",
    "            for rid in sample_ids:\n",
    "                sample = self.df_merged[self.df_merged['record_id'] == rid][\n",
    "                    ['record_id', 'NDC', 'State', 'ATC4 Class', 'ATC2 Class']\n",
    "                ]\n",
    "                print(f\"\\n{rid}:\")\n",
    "                print(sample.to_string(index=False))\n",
    "        \n",
    "        # Deduplicate and export\n",
    "        df_final = self.df_merged.drop_duplicates(subset='record_id', keep='first')\n",
    "        df_final.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\n  After: {len(df_final):,} rows\")\n",
    "        print(f\"  Removed: {initial_count - len(df_final):,}\")\n",
    "        print(f\"\\nExported to: {output_path}\")\n",
    "\n",
    "        #Showing final atc class mapping in %\n",
    "        final_count = len(df_final)\n",
    "        final_mapped_records = df_final['ATC4 Class'].notna().sum()\n",
    "        final_unmapped_records = final_count - final_mapped_records\n",
    "        final_mapped_ndcs=df_final[df_final['ATC4 Class'].notna()]['NDC'].nunique()\n",
    "        final_unmapped_ndcs=df_final['NDC'].nunique()\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        agg = df_final.groupby('record_id').agg({\n",
    "            'Units Reimbursed': 'sum',\n",
    "            'Number of Prescriptions': 'sum'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nAggregated Totals:\")\n",
    "        print(f\"  Units Reimbursed: {agg['Units Reimbursed'].sum():.3f} Billion\")\n",
    "        print(f\"  Number of Prescriptions: {agg['Number of Prescriptions'].sum():3f} Million\")\n",
    "        \n",
    "        return output_path\n",
    "    \n",
    "    def export_unscaled_data(self, output_filename=None, show_details=True):\n",
    "        \"\"\"Export merged data without scaling units.\"\"\"\n",
    "        if self.atc_mapping is None:\n",
    "            raise ValueError(\"Run fetch_atc_names() first\")\n",
    "        \n",
    "        output_filename = output_filename or f\"MUD_{self.year}.csv\"\n",
    "        output_path = os.path.join(self.base_path, f\"ATC\\\\merged_data\\\\unscaled_data\\\\{output_filename}\")\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Use atc_mapping directly (no scaling)\n",
    "        df_unscaled = self.atc_mapping.copy()\n",
    "        \n",
    "        # Check duplicates\n",
    "        initial_count = len(df_unscaled)\n",
    "        duplicate_count = df_unscaled['record_id'].duplicated().sum()\n",
    "        \n",
    "        print(f\"\\nDeduplication Check:\")\n",
    "        print(f\"  Before: {initial_count:,} rows\")\n",
    "        print(f\"  Duplicates: {duplicate_count:,}\")\n",
    "        \n",
    "        # Show sample duplicates if requested\n",
    "        if show_details and duplicate_count > 0:\n",
    "            dup_records = df_unscaled[df_unscaled['record_id'].duplicated(keep=False)].sort_values('record_id')\n",
    "            sample_ids = dup_records['record_id'].unique()[:2]\n",
    "            \n",
    "            print(f\"\\nSample duplicate record_ids:\")\n",
    "            for rid in sample_ids:\n",
    "                sample = df_unscaled[df_unscaled['record_id'] == rid][\n",
    "                    ['record_id', 'NDC', 'State', 'ATC4 Class', 'ATC2 Class']\n",
    "                ]\n",
    "                print(f\"\\n{rid}:\")\n",
    "                print(sample.to_string(index=False))\n",
    "        \n",
    "        # Deduplicate and export\n",
    "        df_final = df_unscaled.drop_duplicates(subset='record_id', keep='first')\n",
    "        df_final.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\n  After: {len(df_final):,} rows\")\n",
    "        print(f\"  Removed: {initial_count - len(df_final):,}\")\n",
    "        print(f\"\\nExported to: {output_path}\")\n",
    "        \n",
    "        # Show final ATC class mapping\n",
    "        final_count = len(df_final)\n",
    "        final_mapped_records = df_final['ATC4 Class'].notna().sum()\n",
    "        \n",
    "        # Aggregate metrics (unscaled)\n",
    "        agg = df_final.groupby('record_id').agg({\n",
    "            'Units Reimbursed': 'sum',\n",
    "            'Number of Prescriptions': 'sum'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nAggregated Totals (Unscaled):\")\n",
    "        print(f\"  Units Reimbursed: {agg['Units Reimbursed'].sum():,.0f}\")\n",
    "        print(f\"  Number of Prescriptions: {agg['Number of Prescriptions'].sum():,.0f}\")\n",
    "        print(f\"  Mapped Records: {final_mapped_records:,} ({final_mapped_records/final_count*100:.1f}%)\")\n",
    "        \n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDCATC_overview:\n",
    "\n",
    "    @staticmethod\n",
    "    def create_multi_year_distribution_analysis(years_list, base_path=None):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "            \n",
    "        print(\"Creating Multi-Year ATC Distribution Analysis...\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        results = {\n",
    "            'ATC4_1_class': {}, 'ATC4_2_classes': {}, 'ATC4_3+_classes': {},\n",
    "            'ATC3_1_class': {}, 'ATC3_2_classes': {}, 'ATC3_3+_classes': {},\n",
    "            'ATC2_1_class': {}, 'ATC2_2_classes': {}, 'ATC2_3+_classes': {}\n",
    "        }\n",
    "        \n",
    "        for year in years_list:\n",
    "            print(f\"Processing {year}...\", end=\" \")\n",
    "            try:\n",
    "                # Load the pre-processed CSV file\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                \n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                if records.empty:\n",
    "                    print(\"No ATC records\")\n",
    "                    for key in results.keys():\n",
    "                        results[key][year] = \"N/A\"\n",
    "                    continue\n",
    "                    \n",
    "                records['ATC2 Class'] = records['ATC4 Class'].str[:3]\n",
    "                records['ATC3 Class'] = records['ATC4 Class'].str[:4]\n",
    "                \n",
    "                # Calculate distributions for each level\n",
    "                for level, col in [('ATC4', 'ATC4 Class'), ('ATC3', 'ATC3 Class'), ('ATC2', 'ATC2 Class')]:\n",
    "                    per_record = records.groupby('record_id')[col].nunique()\n",
    "                    dist = per_record.value_counts().sort_index()\n",
    "                    total = len(per_record)\n",
    "                    \n",
    "                    results[f'{level}_1_class'][year] = f\"{(dist.get(1, 0) / total * 100):.1f}%\"\n",
    "                    results[f'{level}_2_classes'][year] = f\"{(dist.get(2, 0) / total * 100):.1f}%\"\n",
    "                    results[f'{level}_3+_classes'][year] = f\"{(dist[dist.index >= 3].sum() / total * 100):.1f}%\"\n",
    "                \n",
    "                print(\"✓\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"✗ File not found: {csv_path}\")\n",
    "                for key in results.keys():\n",
    "                    results[key][year] = \"N/A\"\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "                for key in results.keys():\n",
    "                    results[key][year] = \"N/A\"\n",
    "        \n",
    "        df_percentages = pd.DataFrame(results).T\n",
    "        print(f\"\\nATC DISTRIBUTION PERCENTAGES ACROSS YEARS\")\n",
    "        print(\"=\"*60)\n",
    "        print(df_percentages)\n",
    "        return df_percentages\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_general_atc_overview_by_state(years_list, base_path=None, state_filter=None):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "\n",
    "        if state_filter:\n",
    "            if isinstance(state_filter, str):\n",
    "                state_filter = [state_filter]\n",
    "            print(f\"Creating ATC2 & ATC3 Overview by State for: {', '.join(state_filter)}\")\n",
    "        else:\n",
    "            print(\"Creating ATC2 & ATC3 Overview by State (All States)\")\n",
    "        print(\"=\"*78)\n",
    "        \n",
    "        # Results will be organized by state\n",
    "        state_results = {}\n",
    "        all_states = set()\n",
    "        \n",
    "        # Collect data for all years first\n",
    "        for year in years_list:\n",
    "            print(f\"Processing {year}...\", end=\" \")\n",
    "            try:\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                \n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                if records.empty:\n",
    "                    print(\"No ATC records\")\n",
    "                    continue\n",
    "                \n",
    "                # Filter by state if specified\n",
    "                if state_filter:\n",
    "                    records = records[records['State'].isin(state_filter)]\n",
    "                    if records.empty:\n",
    "                        print(f\"No records for states {state_filter}\")\n",
    "                        continue\n",
    "                    states_to_process = state_filter\n",
    "                else:\n",
    "                    states_to_process = records['State'].unique()\n",
    "                    all_states.update(states_to_process)\n",
    "                \n",
    "                for state in states_to_process:\n",
    "                    state_records = records[records['State'] == state]\n",
    "                    if state_records.empty:\n",
    "                        continue\n",
    "                    \n",
    "                    # Initialize state data structure\n",
    "                    if state not in state_results:\n",
    "                        state_results[state] = {\n",
    "                            'atc2_year_results': {},\n",
    "                            'atc3_year_results': {}\n",
    "                        }\n",
    "                    \n",
    "                    # ATC2 summary for this state\n",
    "                    pairs2 = state_records[['record_id', 'NDC', 'ATC2 Class']].drop_duplicates()\n",
    "                    atc2_summary = pairs2.groupby('ATC2 Class').agg(\n",
    "                        Unique_NDCs=('NDC', 'nunique'),\n",
    "                        Total_Records=('record_id', 'nunique')\n",
    "                    ).sort_values('Unique_NDCs', ascending=False)\n",
    "                    atc2_summary['Percentage_of_NDCs'] = (\n",
    "                        atc2_summary['Unique_NDCs'] / pairs2['NDC'].nunique() * 100\n",
    "                    ).round(1)\n",
    "                    \n",
    "                    # ATC3 summary for this state\n",
    "                    pairs3 = state_records[['record_id', 'NDC', 'ATC3 Class']].drop_duplicates()\n",
    "                    atc3_summary = pairs3.groupby('ATC3 Class').agg(\n",
    "                        Unique_NDCs=('NDC', 'nunique'),\n",
    "                        Total_Records=('record_id', 'nunique')\n",
    "                    ).sort_values('Unique_NDCs', ascending=False)\n",
    "                    atc3_summary['Percentage_of_NDCs'] = (\n",
    "                        atc3_summary['Unique_NDCs'] / pairs3['NDC'].nunique() * 100\n",
    "                    ).round(1)\n",
    "                    \n",
    "                    state_results[state]['atc2_year_results'][year] = atc2_summary\n",
    "                    state_results[state]['atc3_year_results'][year] = atc3_summary\n",
    "                \n",
    "                processed_states = len([s for s in states_to_process if s in state_results])\n",
    "                print(f\"✓ (States: {processed_states})\")\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                print(f\"✗ File not found\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "        \n",
    "        # Process each state with the same format as original method\n",
    "        final_state_results = {}\n",
    "        \n",
    "        states_to_analyze = state_filter if state_filter else sorted(all_states)\n",
    "        \n",
    "        for state in states_to_analyze:\n",
    "            if state not in state_results:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n\" + \"=\"*60)\n",
    "            print(f\"PROCESSING STATE: {state}\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            atc2_year_results = state_results[state]['atc2_year_results']\n",
    "            atc3_year_results = state_results[state]['atc3_year_results']\n",
    "            \n",
    "            # Print summaries for this state (same format as original)\n",
    "            print(f\"\\nUNIQUE NDCs PER ATC2 CLASS BY YEAR - {state}\")\n",
    "            print(\"=\"*50)\n",
    "            for year in years_list:\n",
    "                if year in atc2_year_results and not atc2_year_results[year].empty:\n",
    "                    print(f\"\\n{year}: {len(atc2_year_results[year])} classes, \"\n",
    "                        f\"{atc2_year_results[year]['Unique_NDCs'].sum():,} total NDCs\")\n",
    "                    print(\"Top 10:\")\n",
    "                    print(atc2_year_results[year].head(10))\n",
    "            \n",
    "            print(f\"\\nUNIQUE NDCs PER ATC3 CLASS BY YEAR - {state}\")\n",
    "            print(\"=\"*50)\n",
    "            for year in years_list:\n",
    "                if year in atc3_year_results and not atc3_year_results[year].empty:\n",
    "                    print(f\"\\n{year}: {len(atc3_year_results[year])} classes, \"\n",
    "                        f\"{atc3_year_results[year]['Unique_NDCs'].sum():,} total NDCs\")\n",
    "                    print(\"Top 10:\")\n",
    "                    print(atc3_year_results[year].head(10))\n",
    "            \n",
    "            # Build comparison tables (same logic as original)\n",
    "            def build_comparison(year_tables):\n",
    "                all_classes = set()\n",
    "                for tbl in year_tables.values():\n",
    "                    if not tbl.empty:\n",
    "                        all_classes.update(tbl.index.tolist())\n",
    "                comp = {cls: {y: int(year_tables[y].loc[cls, 'Unique_NDCs']) \n",
    "                            if y in year_tables and not year_tables[y].empty and cls in year_tables[y].index else 0\n",
    "                            for y in years_list}\n",
    "                        for cls in sorted(all_classes)}\n",
    "                df = pd.DataFrame(comp).T\n",
    "                return df.loc[df.sum(axis=1).sort_values(ascending=False).index]\n",
    "            \n",
    "            atc2_comparison = build_comparison(atc2_year_results)\n",
    "            atc3_comparison = build_comparison(atc3_year_results)\n",
    "            \n",
    "            # Create cumulative frequency tables (same logic as original)\n",
    "            def create_cumulative_frequency_table(comparison_df, level_name):\n",
    "                total_ndcs = comparison_df.sum(axis=1).sort_values(ascending=False)\n",
    "                \n",
    "                freq_table = pd.DataFrame({\n",
    "                    'ATC_Class': total_ndcs.index,\n",
    "                    'Total_Unique_NDCs': total_ndcs.values,\n",
    "                    'Percentage': (total_ndcs.values / total_ndcs.sum() * 100).round(2)\n",
    "                })\n",
    "                \n",
    "                freq_table['Cumulative_NDCs'] = freq_table['Total_Unique_NDCs'].cumsum()\n",
    "                freq_table['Cumulative_Percentage'] = freq_table['Percentage'].cumsum().round(2)\n",
    "                \n",
    "                freq_table.reset_index(drop=True, inplace=True)\n",
    "                freq_table.index = freq_table.index + 1\n",
    "                \n",
    "                return freq_table\n",
    "            \n",
    "            atc2_freq_table = create_cumulative_frequency_table(atc2_comparison, 'ATC2')\n",
    "            atc3_freq_table = create_cumulative_frequency_table(atc3_comparison, 'ATC3')\n",
    "            \n",
    "            # Store results for this state\n",
    "            final_state_results[state] = {\n",
    "                'atc2_year_results': atc2_year_results,\n",
    "                'atc3_year_results': atc3_year_results,\n",
    "                'atc2_comparison': atc2_comparison,\n",
    "                'atc3_comparison': atc3_comparison,\n",
    "                'atc2_freq_table': atc2_freq_table,\n",
    "                'atc3_freq_table': atc3_freq_table\n",
    "            }\n",
    "        \n",
    "        return final_state_results\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_atc_ndc_details(year, top_n=10, base_path=None):\n",
    "    \n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "            \n",
    "        print(f\"Analyzing ATC-NDC details for {year}...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            # Load the pre-processed CSV file\n",
    "            csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "            df_merged = pd.read_csv(csv_path)\n",
    "            \n",
    "            records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "            if records.empty:\n",
    "                print(\"No records with ATC mapping\")\n",
    "                return pd.DataFrame(), pd.DataFrame()\n",
    "                \n",
    "            records['ATC2 Class'] = records['ATC4 Class'].str[:3]\n",
    "            records['ATC3 Class'] = records['ATC4 Class'].str[:4]\n",
    "            \n",
    "            # ATC2 details\n",
    "            atc2_details = records.groupby('ATC2 Class').agg(\n",
    "                Unique_NDCs=('NDC', 'nunique'),\n",
    "                Total_Records=('record_id', 'nunique')\n",
    "            ).sort_values('Unique_NDCs', ascending=False).head(top_n)\n",
    "            \n",
    "            # ATC3 details\n",
    "            atc3_details = records.groupby('ATC3 Class').agg(\n",
    "                Unique_NDCs=('NDC', 'nunique'),\n",
    "                Total_Records=('record_id', 'nunique')\n",
    "            ).sort_values('Unique_NDCs', ascending=False).head(top_n)\n",
    "            \n",
    "            print(f\"\\nTop {top_n} ATC2 Classes:\")\n",
    "            print(atc2_details)\n",
    "            print(f\"\\nTop {top_n} ATC3 Classes:\")\n",
    "            print(atc3_details)\n",
    "            \n",
    "            return atc2_details, atc3_details\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"✗ File not found: {csv_path}\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error: {e}\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    @staticmethod\n",
    "    def export_cumulative_frequency_excel(years_list, level='ATC2', base_path=None, output_filename=None, \n",
    "                                          include_ndc_counts=True, by_state=False, state_filter=None):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        # Print header\n",
    "        if by_state:\n",
    "            states_msg = ', '.join(state_filter) if state_filter else 'All States'\n",
    "            print(f\"Creating {level} Cumulative Frequency Analysis Excel for States: {states_msg}\")\n",
    "        else:\n",
    "            print(f\"Creating {level} Cumulative Frequency Analysis Excel...\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Helper function to create ATC level column\n",
    "        def create_atc_level_column(df, level):\n",
    "            if level == 'ATC2':\n",
    "                return df['ATC4 Class'].str[:3]\n",
    "            elif level == 'ATC3':\n",
    "                return df['ATC4 Class'].str[:4]\n",
    "            else:  # ATC4\n",
    "                return df['ATC4 Class']\n",
    "        \n",
    "        # Helper function to get name mapping\n",
    "        def get_name_mapping(records, level):\n",
    "            name_col_map = {'ATC2': 'ATC2_Name', 'ATC3': 'ATC3_Name', 'ATC4': 'ATC4_Name'}\n",
    "            name_col = name_col_map.get(level)\n",
    "            \n",
    "            if name_col and name_col in records.columns:\n",
    "                return records[['ATC_Level', name_col]].drop_duplicates().set_index('ATC_Level')[name_col].to_dict()\n",
    "            return {}\n",
    "        \n",
    "        # Helper function to process year data\n",
    "        def process_year_data(csv_path, level, state_filter=None):\n",
    "            try:\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                \n",
    "                if records.empty:\n",
    "                    return None, None, None, None\n",
    "                \n",
    "                # Filter by state if needed\n",
    "                if state_filter:\n",
    "                    records = records[records['State'].isin(state_filter)]\n",
    "                    if records.empty:\n",
    "                        return None, None, None, None\n",
    "                \n",
    "                records['ATC_Level'] = create_atc_level_column(records, level)\n",
    "                name_mapping = get_name_mapping(records, level)\n",
    "                \n",
    "                # Aggregate financial data\n",
    "                financial = records.groupby('ATC_Level').agg(\n",
    "                    Units_Reimbursed=('Units Reimbursed', 'sum'),\n",
    "                    Number_of_Prescriptions=('Number of Prescriptions', 'sum')\n",
    "                )\n",
    "                \n",
    "                # Count unique NDCs\n",
    "                ndc_counts = records.groupby('ATC_Level').agg(\n",
    "                    Unique_NDCs=('NDC', 'nunique')\n",
    "                )\n",
    "                \n",
    "                states_processed = records['State'].nunique() if 'State' in records.columns else 1\n",
    "                \n",
    "                return financial, ndc_counts, name_mapping, states_processed\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file: {e}\")\n",
    "                return None, None, None, None\n",
    "        \n",
    "        # Collect data for all years\n",
    "        if by_state:\n",
    "            state_data = {}  # {state: {year: (financial, ndc_counts, name_mapping)}}\n",
    "            all_states = set()\n",
    "        else:\n",
    "            year_results = {}\n",
    "            ndc_counts = {}\n",
    "            name_mapping = {}\n",
    "        \n",
    "        # Process each year\n",
    "        for year in years_list:\n",
    "            print(f\"Processing {year}...\", end=\" \")\n",
    "            csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "            \n",
    "            if by_state:\n",
    "                # Load full data first\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_path)\n",
    "                    records = df[df['ATC4 Class'].notna()].copy()\n",
    "                    \n",
    "                    if state_filter:\n",
    "                        states_to_process = state_filter if isinstance(state_filter, list) else [state_filter]\n",
    "                    else:\n",
    "                        states_to_process = records['State'].unique()\n",
    "                        all_states.update(states_to_process)\n",
    "                    \n",
    "                    for state in states_to_process:\n",
    "                        financial, ndcs, names, _ = process_year_data(csv_path, level, [state])\n",
    "                        \n",
    "                        if financial is not None:\n",
    "                            if state not in state_data:\n",
    "                                state_data[state] = {}\n",
    "                            state_data[state][year] = (financial, ndcs, names)\n",
    "                    \n",
    "                    print(f\"✓ (States: {len(states_to_process)})\")\n",
    "                except:\n",
    "                    print(\"✗\")\n",
    "            else:\n",
    "                financial, ndcs, names, _ = process_year_data(csv_path, level)\n",
    "                \n",
    "                if financial is not None:\n",
    "                    year_results[year] = financial\n",
    "                    ndc_counts[year] = ndcs\n",
    "                    name_mapping.update(names)\n",
    "                    print(f\"✓ ({len(financial)} classes)\")\n",
    "                else:\n",
    "                    print(\"✗\")\n",
    "        \n",
    "        # Helper function to build comparison dataframes\n",
    "        def build_comparison_dfs(year_financial_dict, year_ndc_dict, years_list):\n",
    "            # Collect all unique ATC classes\n",
    "            all_classes = set()\n",
    "            for df in year_financial_dict.values():\n",
    "                if df is not None and not df.empty:\n",
    "                    all_classes.update(df.index)\n",
    "            for df in year_ndc_dict.values():\n",
    "                if df is not None and not df.empty:\n",
    "                    all_classes.update(df.index)\n",
    "            \n",
    "            if not all_classes:\n",
    "                return None, None, None\n",
    "            \n",
    "            # Build comparison dictionaries\n",
    "            units_comp = {}\n",
    "            presc_comp = {}\n",
    "            ndc_comp = {}\n",
    "            \n",
    "            for cls in sorted(all_classes):\n",
    "                units_comp[cls] = {\n",
    "                    y: float(year_financial_dict[y].loc[cls, 'Units_Reimbursed']) \n",
    "                    if y in year_financial_dict and year_financial_dict[y] is not None \n",
    "                    and not year_financial_dict[y].empty and cls in year_financial_dict[y].index \n",
    "                    else 0.0 \n",
    "                    for y in years_list\n",
    "                }\n",
    "                \n",
    "                presc_comp[cls] = {\n",
    "                    y: float(year_financial_dict[y].loc[cls, 'Number_of_Prescriptions']) \n",
    "                    if y in year_financial_dict and year_financial_dict[y] is not None \n",
    "                    and not year_financial_dict[y].empty and cls in year_financial_dict[y].index \n",
    "                    else 0.0 \n",
    "                    for y in years_list\n",
    "                }\n",
    "                \n",
    "                ndc_comp[cls] = {\n",
    "                    y: int(year_ndc_dict[y].loc[cls, 'Unique_NDCs']) \n",
    "                    if y in year_ndc_dict and year_ndc_dict[y] is not None \n",
    "                    and not year_ndc_dict[y].empty and cls in year_ndc_dict[y].index \n",
    "                    else 0 \n",
    "                    for y in years_list\n",
    "                }\n",
    "            \n",
    "            # Convert to DataFrames and sort by total units\n",
    "            units_df = pd.DataFrame(units_comp).T\n",
    "            presc_df = pd.DataFrame(presc_comp).T\n",
    "            ndc_df = pd.DataFrame(ndc_comp).T\n",
    "            \n",
    "            units_total = units_df.sum(axis=1).sort_values(ascending=False)\n",
    "            units_df = units_df.loc[units_total.index]\n",
    "            presc_df = presc_df.loc[units_total.index]\n",
    "            ndc_df = ndc_df.loc[units_total.index]\n",
    "            \n",
    "            return units_df, presc_df, ndc_df\n",
    "        \n",
    "        # Helper function to create cumulative frequency DataFrame\n",
    "        def create_cumulative_df(comparison_df, metric_name, name_mapping):\n",
    "            totals = comparison_df.sum(axis=1)\n",
    "            total_sum = totals.sum()\n",
    "            \n",
    "            cumulative_total = 0\n",
    "            df_data = []\n",
    "            \n",
    "            for atc_class in comparison_df.index:\n",
    "                class_total = totals[atc_class]\n",
    "                cumulative_total += class_total\n",
    "                percentage = (class_total / total_sum * 100) if total_sum > 0 else 0\n",
    "                cumulative_pct = (cumulative_total / total_sum * 100) if total_sum > 0 else 0\n",
    "                \n",
    "                row = {'ATC_Class': atc_class}\n",
    "                \n",
    "                # Add ATC name if available\n",
    "                row['ATC_Name'] = name_mapping.get(atc_class, '')\n",
    "                \n",
    "                # Add year-by-year data\n",
    "                for year in years_list:\n",
    "                    if metric_name == 'NDCs':\n",
    "                        row[f'{metric_name}_{year}'] = int(comparison_df.loc[atc_class, year])\n",
    "                    else:\n",
    "                        row[f'{metric_name}_{year}'] = round(comparison_df.loc[atc_class, year], 3)\n",
    "                \n",
    "                # Add summary columns\n",
    "                if metric_name == 'NDCs':\n",
    "                    row[f'Total_{metric_name}'] = int(class_total)\n",
    "                    row['Percentage'] = round(percentage, 2)\n",
    "                    row[f'Cumulative_{metric_name}'] = int(cumulative_total)\n",
    "                    row['Cumulative_Percentage_NDCs'] = round(cumulative_pct, 2)\n",
    "                else:\n",
    "                    row[f'Total_{metric_name}'] = round(class_total, 3)\n",
    "                    row['Percentage'] = round(percentage, 2)\n",
    "                    row[f'Cumulative_{metric_name}'] = round(cumulative_total, 3)\n",
    "                    row['Cumulative_Percentage'] = round(cumulative_pct, 2)\n",
    "                \n",
    "                df_data.append(row)\n",
    "            \n",
    "            return pd.DataFrame(df_data)\n",
    "        \n",
    "        # Export function\n",
    "        def export_to_excel(units_df, prescriptions_df, ndc_df, output_path, include_ndc_counts):\n",
    "            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "                units_df.to_excel(writer, sheet_name='Units_Reimbursed', index=False)\n",
    "                prescriptions_df.to_excel(writer, sheet_name='Prescriptions', index=False)\n",
    "                \n",
    "                if include_ndc_counts:\n",
    "                    ndc_df.to_excel(writer, sheet_name='NDC_Counts', index=False)\n",
    "            \n",
    "            print(f\"Exported to Excel: {output_path}\")\n",
    "        \n",
    "        # Process and export\n",
    "        output_dir = os.path.join(base_path, \"ATC\\\\exported_analysis\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        if by_state:\n",
    "            states_to_export = state_filter if state_filter else sorted(all_states)\n",
    "            all_output_paths = {}\n",
    "            \n",
    "            for state in states_to_export:\n",
    "                if state not in state_data:\n",
    "                    continue\n",
    "                \n",
    "                # Extract data for this state\n",
    "                state_year_financial = {y: data[0] for y, data in state_data[state].items()}\n",
    "                state_year_ndc = {y: data[1] for y, data in state_data[state].items()}\n",
    "                state_name_mapping = {}\n",
    "                for data in state_data[state].values():\n",
    "                    state_name_mapping.update(data[2])\n",
    "                \n",
    "                # Build comparison DataFrames\n",
    "                units_comp_df, presc_comp_df, ndc_comp_df = build_comparison_dfs(\n",
    "                    state_year_financial, state_year_ndc, years_list\n",
    "                )\n",
    "                \n",
    "                if units_comp_df is None:\n",
    "                    continue\n",
    "                \n",
    "                # Create cumulative DataFrames\n",
    "                units_cum = create_cumulative_df(units_comp_df, 'Units', state_name_mapping)\n",
    "                presc_cum = create_cumulative_df(presc_comp_df, 'Prescriptions', state_name_mapping)\n",
    "                ndc_cum = create_cumulative_df(ndc_comp_df, 'NDCs', state_name_mapping)\n",
    "                \n",
    "                # Generate output filename\n",
    "                if output_filename:\n",
    "                    name_parts = output_filename.rsplit('.', 1)\n",
    "                    state_output_filename = f\"{name_parts[0]}_{state}.{name_parts[1]}\"\n",
    "                else:\n",
    "                    state_output_filename = f\"{level}_Cumulative_Analysis_{state}_with_NDC_Counts.xlsx\"\n",
    "                \n",
    "                output_path = os.path.join(output_dir, state_output_filename)\n",
    "                export_to_excel(units_cum, presc_cum, ndc_cum, output_path, include_ndc_counts)\n",
    "                all_output_paths[state] = output_path\n",
    "            \n",
    "            return all_output_paths\n",
    "        \n",
    "        else:\n",
    "            # Build comparison DataFrames\n",
    "            units_comp_df, presc_comp_df, ndc_comp_df = build_comparison_dfs(\n",
    "                year_results, ndc_counts, years_list\n",
    "            )\n",
    "            \n",
    "            if units_comp_df is None:\n",
    "                print(\"No data found!\")\n",
    "                return None\n",
    "            \n",
    "            # Create cumulative DataFrames\n",
    "            units_cum = create_cumulative_df(units_comp_df, 'Units', name_mapping)\n",
    "            presc_cum = create_cumulative_df(presc_comp_df, 'Prescriptions', name_mapping)\n",
    "            ndc_cum = create_cumulative_df(ndc_comp_df, 'NDCs', name_mapping)\n",
    "            \n",
    "            # Generate output filename\n",
    "            if not output_filename:\n",
    "                output_filename = f\"{level}_Cumulative_Analysis_with_NDC_Counts.xlsx\"\n",
    "            \n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            export_to_excel(units_cum, presc_cum, ndc_cum, output_path, include_ndc_counts)\n",
    "            \n",
    "            return output_path\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_cumulative_80_analysis(base_path=None, cumulative_threshold=80.0):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(f\"COMPARATIVE ANALYSIS: Indiana vs National ATC2 Classes at {cumulative_threshold}% Threshold\")\n",
    "        print(\"=\"*85)\n",
    "        \n",
    "        # File paths\n",
    "        indiana_file = os.path.join(base_path, \"ATC\\\\exported_analysis\\\\ATC2_Cumulative_Analysis_IN_with_NDC_Counts.xlsx\")\n",
    "        national_file = os.path.join(base_path, \"ATC\\\\exported_analysis\\\\ATC2_Cumulative_Analysis_with_NDC_Counts.xlsx\")\n",
    "        \n",
    "        # Load data\n",
    "        try:\n",
    "            indiana_units = pd.read_excel(indiana_file, sheet_name='Units_Reimbursed')\n",
    "            indiana_prescriptions = pd.read_excel(indiana_file, sheet_name='Prescriptions')\n",
    "            national_units = pd.read_excel(national_file, sheet_name='Units_Reimbursed')\n",
    "            national_prescriptions = pd.read_excel(national_file, sheet_name='Prescriptions')\n",
    "            \n",
    "            print(f\"✓ Loaded Indiana data: {len(indiana_units)} ATC2 classes\")\n",
    "            print(f\"✓ Loaded National data: {len(national_units)} ATC2 classes\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"✗ Error: Required Excel files not found. Please run export functions first.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading files: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Helper function to get classes at threshold\n",
    "        def get_classes_at_threshold(df, threshold):\n",
    "            df_sorted = df.sort_values('Cumulative_Percentage').reset_index(drop=True)\n",
    "            threshold_idx = df_sorted[df_sorted['Cumulative_Percentage'] >= threshold].index\n",
    "            \n",
    "            if len(threshold_idx) > 0:\n",
    "                return df_sorted.iloc[:threshold_idx[0] + 1]['ATC_Class'].tolist()\n",
    "            return df_sorted['ATC_Class'].tolist()\n",
    "        \n",
    "        # Get classes at threshold for each dataset\n",
    "        in_units_80 = get_classes_at_threshold(indiana_units, cumulative_threshold)\n",
    "        in_presc_80 = get_classes_at_threshold(indiana_prescriptions, cumulative_threshold)\n",
    "        nat_units_80 = get_classes_at_threshold(national_units, cumulative_threshold)\n",
    "        nat_presc_80 = get_classes_at_threshold(national_prescriptions, cumulative_threshold)\n",
    "        \n",
    "        # Calculate overlaps\n",
    "        in_overlap = set(in_units_80) & set(in_presc_80)\n",
    "        in_only_units = set(in_units_80) - set(in_presc_80)\n",
    "        in_only_presc = set(in_presc_80) - set(in_units_80)\n",
    "        \n",
    "        nat_overlap = set(nat_units_80) & set(nat_presc_80)\n",
    "        nat_only_units = set(nat_units_80) - set(nat_presc_80)\n",
    "        nat_only_presc = set(nat_presc_80) - set(nat_units_80)\n",
    "        \n",
    "        # Print Indiana Analysis\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"1. INDIANA ANALYSIS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nClasses reaching {cumulative_threshold}% cumulative:\")\n",
    "        print(f\"  Units Reimbursed: {len(in_units_80)} classes\")\n",
    "        print(f\"  Prescriptions: {len(in_presc_80)} classes\")\n",
    "        print(f\"\\nComparison:\")\n",
    "        print(f\"  Overlap (both metrics): {len(in_overlap)} classes\")\n",
    "        print(f\"  Only in Units: {len(in_only_units)} classes\")\n",
    "        print(f\"  Only in Prescriptions: {len(in_only_presc)} classes\")\n",
    "        \n",
    "        # Print National Analysis\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"2. NATIONAL ANALYSIS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nClasses reaching {cumulative_threshold}% cumulative:\")\n",
    "        print(f\"  Units Reimbursed: {len(nat_units_80)} classes\")\n",
    "        print(f\"  Prescriptions: {len(nat_presc_80)} classes\")\n",
    "        print(f\"\\nComparison:\")\n",
    "        print(f\"  Overlap (both metrics): {len(nat_overlap)} classes\")\n",
    "        print(f\"  Only in Units: {len(nat_only_units)} classes\")\n",
    "        print(f\"  Only in Prescriptions: {len(nat_only_presc)} classes\")\n",
    "        \n",
    "        # Helper function for detailed comparison\n",
    "        def print_detailed_comparison(set1, set2, metric_name):\n",
    "            overlap = set1 & set2\n",
    "            only_in = set1 - set2\n",
    "            only_nat = set2 - set1\n",
    "            \n",
    "            print(f\"\\n{metric_name.upper()} - Detailed Comparison:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Classes in BOTH Indiana and National ({len(overlap)}):\")\n",
    "            print(f\"  {sorted(overlap) if overlap else 'None'}\")\n",
    "            print(f\"\\nClasses ONLY in Indiana ({len(only_in)}):\")\n",
    "            print(f\"  {sorted(only_in) if only_in else 'None'}\")\n",
    "            print(f\"\\nClasses ONLY in National ({len(only_nat)}):\")\n",
    "            print(f\"  {sorted(only_nat) if only_nat else 'None'}\")\n",
    "        \n",
    "        # Print detailed comparisons\n",
    "        print(f\"\\n{'='*85}\")\n",
    "        print(\"3. DETAILED CLASS ANALYSIS\")\n",
    "        print(f\"{'='*85}\")\n",
    "        \n",
    "        print_detailed_comparison(set(in_units_80), set(nat_units_80), \"Units Reimbursed\")\n",
    "        print_detailed_comparison(set(in_presc_80), set(nat_presc_80), \"Prescriptions\")\n",
    "        \n",
    "        # Helper function to calculate category totals\n",
    "        def get_category_totals(df_units, df_presc, class_list):\n",
    "            if not class_list:\n",
    "                return 0.0, 0.0\n",
    "            units_total = df_units[df_units['ATC_Class'].isin(class_list)]['Total_Units'].sum()\n",
    "            presc_total = df_presc[df_presc['ATC_Class'].isin(class_list)]['Total_Prescriptions'].sum()\n",
    "            return units_total, presc_total\n",
    "        \n",
    "        # Calculate totals for all categories\n",
    "        in_units_only_u, in_units_only_p = get_category_totals(indiana_units, indiana_prescriptions, list(in_only_units))\n",
    "        in_presc_only_u, in_presc_only_p = get_category_totals(indiana_units, indiana_prescriptions, list(in_only_presc))\n",
    "        in_overlap_u, in_overlap_p = get_category_totals(indiana_units, indiana_prescriptions, list(in_overlap))\n",
    "        \n",
    "        nat_units_only_u, nat_units_only_p = get_category_totals(national_units, national_prescriptions, list(nat_only_units))\n",
    "        nat_presc_only_u, nat_presc_only_p = get_category_totals(national_units, national_prescriptions, list(nat_only_presc))\n",
    "        nat_overlap_u, nat_overlap_p = get_category_totals(national_units, national_prescriptions, list(nat_overlap))\n",
    "        \n",
    "        # Create summary DataFrame\n",
    "        totals_summary = pd.DataFrame({\n",
    "            'Geography': ['Indiana', 'Indiana', 'Indiana', 'National', 'National', 'National'],\n",
    "            'Category': ['Only in Units', 'Only in Prescriptions', 'In Both (Overlap)'] * 2,\n",
    "            'Num_Classes': [\n",
    "                len(in_only_units), len(in_only_presc), len(in_overlap),\n",
    "                len(nat_only_units), len(nat_only_presc), len(nat_overlap)\n",
    "            ],\n",
    "            'Total_Units': [\n",
    "                in_units_only_u, in_presc_only_u, in_overlap_u,\n",
    "                nat_units_only_u, nat_presc_only_u, nat_overlap_u\n",
    "            ],\n",
    "            'Total_Prescriptions': [\n",
    "                in_units_only_p, in_presc_only_p, in_overlap_p,\n",
    "                nat_units_only_p, nat_presc_only_p, nat_overlap_p\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{'='*85}\")\n",
    "        print(\"4. SUMMARY TOTALS\")\n",
    "        print(f\"{'='*85}\")\n",
    "        print(totals_summary.to_string(index=False))\n",
    "        \n",
    "        # Return comprehensive results\n",
    "        return {\n",
    "            'totals_summary': totals_summary,\n",
    "            'indiana': {\n",
    "                'units_80': in_units_80,\n",
    "                'prescriptions_80': in_presc_80,\n",
    "                'overlap': list(in_overlap),\n",
    "                'only_units': list(in_only_units),\n",
    "                'only_prescriptions': list(in_only_presc)\n",
    "            },\n",
    "            'national': {\n",
    "                'units_80': nat_units_80,\n",
    "                'prescriptions_80': nat_presc_80,\n",
    "                'overlap': list(nat_overlap),\n",
    "                'only_units': list(nat_only_units),\n",
    "                'only_prescriptions': list(nat_only_presc)\n",
    "            }\n",
    "        }\n",
    "    @staticmethod\n",
    "    def create_pareto_charts(base_path=None, top_n=30):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(\"CREATING PARETO CHARTS: Indiana vs National\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # File paths\n",
    "        indiana_file = os.path.join(base_path, \"ATC\\\\exported_analysis\\\\ATC2_Cumulative_Analysis_IN_with_NDC_Counts.xlsx\")\n",
    "        national_file = os.path.join(base_path, \"ATC\\\\exported_analysis\\\\ATC2_Cumulative_Analysis_with_NDC_Counts.xlsx\")\n",
    "        \n",
    "        # Load data\n",
    "        try:\n",
    "            indiana_units = pd.read_excel(indiana_file, sheet_name='Units_Reimbursed')\n",
    "            indiana_prescriptions = pd.read_excel(indiana_file, sheet_name='Prescriptions')\n",
    "            national_units = pd.read_excel(national_file, sheet_name='Units_Reimbursed')\n",
    "            national_prescriptions = pd.read_excel(national_file, sheet_name='Prescriptions')\n",
    "            print(\"✓ Data loaded successfully\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"✗ Error: Required Excel files not found. Please run export functions first.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading files: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Create figure with 2x2 subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(24, 16))\n",
    "        \n",
    "        def create_single_pareto(ax, df, metric_name, geography, top_n):\n",
    "            \"\"\"Create a single Pareto chart\"\"\"\n",
    "            df_top = df.head(top_n).copy()\n",
    "            \n",
    "            # Prepare data\n",
    "            classes = df_top['ATC_Class']\n",
    "            if metric_name == 'Units':\n",
    "                values = df_top['Total_Units']\n",
    "                y_label = 'Total Units Reimbursed (Billions)'\n",
    "            else:\n",
    "                values = df_top['Total_Prescriptions']\n",
    "                y_label = 'Total Prescriptions (Millions)'\n",
    "            \n",
    "            cumulative_pct = df_top['Cumulative_Percentage']\n",
    "            \n",
    "            # Create bar chart\n",
    "            bars = ax.bar(range(len(classes)), values, alpha=0.7, color='steelblue')\n",
    "            ax.set_xlabel('ATC2 Classes', fontsize=12)\n",
    "            ax.set_ylabel(y_label, fontsize=12, color='steelblue')\n",
    "            ax.tick_params(axis='y', labelcolor='steelblue')\n",
    "            ax.set_xticks(range(len(classes)))\n",
    "            ax.set_xticklabels(classes, rotation=90, ha='center', fontsize=8)\n",
    "            \n",
    "            # Add value labels on top 3 bars\n",
    "            for i in range(min(3, len(bars))):\n",
    "                height = bars[i].get_height()\n",
    "                ax.text(bars[i].get_x() + bars[i].get_width()/2., height,\n",
    "                       f'{height:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "            \n",
    "            # Create cumulative percentage line\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.plot(range(len(classes)), cumulative_pct, color='red', marker='o', linewidth=2, markersize=3)\n",
    "            ax2.set_ylabel('Cumulative Percentage (%)', fontsize=12, color='red')\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            ax2.set_ylim(0, 100)\n",
    "            \n",
    "            # Add 80% threshold line\n",
    "            ax2.axhline(y=80, color='red', linestyle='--', alpha=0.7, linewidth=1)\n",
    "            ax2.text(len(classes)*0.7, 82, '80%', color='red', fontweight='bold')\n",
    "            \n",
    "            # Title\n",
    "            ax.set_title(f'{geography} - {metric_name}\\n(Top {top_n} ATC2 Classes)', \n",
    "                        fontsize=12, fontweight='bold', pad=15)\n",
    "        \n",
    "        # Create all four Pareto charts\n",
    "        create_single_pareto(axes[0,0], indiana_units, 'Units', 'INDIANA', top_n)\n",
    "        create_single_pareto(axes[0,1], national_units, 'Units', 'NATIONAL', top_n)\n",
    "        create_single_pareto(axes[1,0], indiana_prescriptions, 'Prescriptions', 'INDIANA', top_n)\n",
    "        create_single_pareto(axes[1,1], national_prescriptions, 'Prescriptions', 'NATIONAL', top_n)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Helper function to print Pareto summary\n",
    "        def print_pareto_summary(df, metric_name, geography):\n",
    "            df_sorted = df.sort_values('Cumulative_Percentage').reset_index(drop=True)\n",
    "            threshold_idx = df_sorted[df_sorted['Cumulative_Percentage'] >= 80].index\n",
    "            \n",
    "            if len(threshold_idx) > 0:\n",
    "                classes_80 = df_sorted.iloc[:threshold_idx[0] + 1]\n",
    "            else:\n",
    "                classes_80 = df_sorted\n",
    "            \n",
    "            total_classes = len(df)\n",
    "            classes_for_80 = len(classes_80)\n",
    "            pct_classes_for_80 = (classes_for_80 / total_classes) * 100\n",
    "            \n",
    "            if metric_name == 'Units':\n",
    "                total_value = df['Total_Units'].sum()\n",
    "                value_80 = classes_80['Total_Units'].sum()\n",
    "            else:\n",
    "                total_value = df['Total_Prescriptions'].sum()\n",
    "                value_80 = classes_80['Total_Prescriptions'].sum()\n",
    "            \n",
    "            actual_pct_covered = (value_80 / total_value) * 100\n",
    "            \n",
    "            print(f\"\\n{geography} - {metric_name}:\")\n",
    "            print(f\"  Total ATC2 classes: {total_classes}\")\n",
    "            print(f\"  Classes needed for ~80%: {classes_for_80} ({pct_classes_for_80:.1f}% of classes)\")\n",
    "            print(f\"  Actual coverage: {actual_pct_covered:.1f}%\")\n",
    "            print(f\"  Total {metric_name.lower()}: {total_value:.3f}\")\n",
    "            print(f\"  Value in top classes: {value_80:.3f}\")\n",
    "            print(f\"  Top 5 classes:\")\n",
    "            for i, (_, row) in enumerate(df.head(5).iterrows(), 1):\n",
    "                value = row['Total_Units'] if metric_name == 'Units' else row['Total_Prescriptions']\n",
    "                print(f\"    {i}. {row['ATC_Class']}: {value:.3f} ({row['Percentage']:.1f}%)\")\n",
    "        \n",
    "        # Print summaries\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PARETO ANALYSIS SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print_pareto_summary(indiana_units, 'Units', 'INDIANA')\n",
    "        print_pareto_summary(national_units, 'Units', 'NATIONAL')\n",
    "        print_pareto_summary(indiana_prescriptions, 'Prescriptions', 'INDIANA')\n",
    "        print_pareto_summary(national_prescriptions, 'Prescriptions', 'NATIONAL')\n",
    "        \n",
    "        # Compare top classes\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"KEY INSIGHTS - Top 10 Classes Comparison\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        in_top_units = set(indiana_units.head(10)['ATC_Class'])\n",
    "        nat_top_units = set(national_units.head(10)['ATC_Class'])\n",
    "        in_top_presc = set(indiana_prescriptions.head(10)['ATC_Class'])\n",
    "        nat_top_presc = set(national_prescriptions.head(10)['ATC_Class'])\n",
    "        \n",
    "        units_overlap = in_top_units & nat_top_units\n",
    "        presc_overlap = in_top_presc & nat_top_presc\n",
    "        \n",
    "        print(f\"\\nUnits Reimbursed - Common classes: {len(units_overlap)}/10\")\n",
    "        print(f\"  Shared: {sorted(units_overlap)}\")\n",
    "        print(f\"  Only Indiana: {sorted(in_top_units - nat_top_units)}\")\n",
    "        print(f\"  Only National: {sorted(nat_top_units - in_top_units)}\")\n",
    "        \n",
    "        print(f\"\\nPrescriptions - Common classes: {len(presc_overlap)}/10\")\n",
    "        print(f\"  Shared: {sorted(presc_overlap)}\")\n",
    "        print(f\"  Only Indiana: {sorted(in_top_presc - nat_top_presc)}\")\n",
    "        print(f\"  Only National: {sorted(nat_top_presc - in_top_presc)}\")\n",
    "        \n",
    "        return {\n",
    "            'indiana_units': indiana_units,\n",
    "            'indiana_prescriptions': indiana_prescriptions,\n",
    "            'national_units': national_units,\n",
    "            'national_prescriptions': national_prescriptions\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "workflow_execution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV: c:\\Users\\lholguin\\OneDrive - purdue.edu\\VS code\\Data\\SDUD\\SDUD2016.csv\n",
      "Initial rows: 4,702,214\n",
      "After cleaning: 2,291,493 rows, 28,875 unique NDCs\n",
      "Created 2,291,493 record IDs\n",
      "ATC4 file: 3,431,883 rows, 19,956 unique NDCs\n",
      "Merged: 2,291,493 records, 1,818,174 with ATC4 (79.3%)\n",
      "Missing: 473,319 records, 8,919 unique NDCs\n",
      "\n",
      "============================================================\n",
      "FETCHING ATC CLASS NAMES\n",
      "============================================================\n",
      "Using cache: c:\\Users\\lholguin\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\cache_files\\atc_names_cache\n",
      "\n",
      "Creating ATC3 and ATC2 columns from ATC4...\n",
      "\n",
      "Unique codes to fetch:\n",
      "  ATC4: 589\n",
      "  ATC3: 208\n",
      "  ATC2: 89\n",
      "\n",
      "Fetching ATC4 names...\n",
      "Fetching ATC3 names...\n",
      "Fetching ATC2 names...\n",
      "\n",
      "Total processing time: 0.0 minutes\n",
      "\n",
      "Applying names to dataframe...\n",
      "\n",
      "ATC names added successfully!\n",
      "\n",
      "Sample output:\n",
      "           NDC                   record_id ATC4 Class                                  ATC4_Name ATC3 Class                                              ATC3_Name ATC2 Class               ATC2_Name\n",
      "0  00002143380  AK_2016_4_FFSU_00002143380      A10BJ  Glucagon-like peptide-1 (GLP-1) analogues       A10B           BLOOD GLUCOSE LOWERING DRUGS, EXCL. INSULINS        A10  DRUGS USED IN DIABETES\n",
      "1  00002143480  AK_2016_4_FFSU_00002143480      A10BJ  Glucagon-like peptide-1 (GLP-1) analogues       A10B           BLOOD GLUCOSE LOWERING DRUGS, EXCL. INSULINS        A10  DRUGS USED IN DIABETES\n",
      "2  00002322730  AK_2016_4_FFSU_00002322730      N06BA          Centrally acting sympathomimetics       N06B  PSYCHOSTIMULANTS, AGENTS USED FOR ADHD AND NOOTROPICS        N06        PSYCHOANALEPTICS\n",
      "3  00002322830  AK_2016_4_FFSU_00002322830      N06BA          Centrally acting sympathomimetics       N06B  PSYCHOSTIMULANTS, AGENTS USED FOR ADHD AND NOOTROPICS        N06        PSYCHOANALEPTICS\n",
      "4  00002322930  AK_2016_4_FFSU_00002322930      N06BA          Centrally acting sympathomimetics       N06B  PSYCHOSTIMULANTS, AGENTS USED FOR ADHD AND NOOTROPICS        N06        PSYCHOANALEPTICS\n",
      "\n",
      "Final Statistics:\n",
      "  Records: 2,291,493 (1,818,174 with ATC4, 79.3%)\n",
      "  Units Reimbursed: 46.00 Billion\n",
      "  Prescriptions: 738.53 Million\n",
      "\n",
      "Deduplication Check:\n",
      "  Before: 2,291,493 rows\n",
      "  Duplicates: 0\n",
      "\n",
      "  After: 2,291,493 rows\n",
      "  Removed: 0\n",
      "\n",
      "Exported to: c:\\Users\\lholguin\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\merged_data\\merged_NEWdata_2016.csv\n",
      "\n",
      "Aggregated Totals:\n",
      "  Units Reimbursed: 46.003 Billion\n",
      "  Number of Prescriptions: 738.534166 Million\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\lholguin\\\\OneDrive - purdue.edu\\\\VS code\\\\Data\\\\ATC\\\\merged_data\\\\merged_NEWdata_2016.csv'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = NDCATCProcessor(year=2016)\n",
    "processor.clean_sdud_data()           # Clean SDUD data\n",
    "processor.adding_key()                # Add record_id key\n",
    "#analyzer.generate_ndc_txt()          # Generate NDC text file\n",
    "processor.analyze_atc4_mapping() \n",
    "processor.fetch_atc_names() \n",
    "#processor.export_unscaled_data()\n",
    "\n",
    "processor.prepare_final_dataframe()   \n",
    "processor.export_merged_data()  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just checking overlap between files with and without key\n",
    "nokey_path=rf'C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\ATC4_classes\\Classes_notgood\\NDCf_2020_ATC4_classes.csv'\n",
    "keyed_path=rf'C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\ATC4_classes\\NDCNEW_2020_ATC4_classes.csv'\n",
    "# Load them\n",
    "keyed = pd.read_csv(keyed_path, dtype=str)\n",
    "nokey = pd.read_csv(nokey_path, dtype=str)\n",
    "\n",
    "# Normalize NDCs (remove hyphens, pad to 11 digits)\n",
    "for df in [keyed, nokey]:\n",
    "    df[\"NDC\"] = df[\"NDC\"].str.replace(\"-\", \"\", regex=False).str.zfill(11)\n",
    "\n",
    "# --- Summary stats ---\n",
    "summary = {\n",
    "    \"File\": [\"With key (NDCNEW_2024_ATC4_classes)\", \"Without key (NDCf_2024_ATC4_classes)\"],\n",
    "    \"Total rows\": [len(keyed), len(nokey)],\n",
    "    \"Unique NDCs\": [keyed[\"NDC\"].nunique(), nokey[\"NDC\"].nunique()],\n",
    "    \"Mapped NDCs (non-null ATC)\": [\n",
    "        keyed[\"ATC4 Class\"].notna().sum(),\n",
    "        nokey[\"ATC4 Class\"].notna().sum(),\n",
    "    ],\n",
    "}\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# --- Compare overlap of unique NDCs ---\n",
    "ndc_keyed = set(keyed[\"NDC\"].unique())\n",
    "ndc_nokey = set(nokey[\"NDC\"].unique())\n",
    "\n",
    "overlap_ndcs = len(ndc_keyed & ndc_nokey)\n",
    "only_in_nokey = len(ndc_nokey - ndc_keyed)\n",
    "only_in_keyed = len(ndc_keyed - ndc_nokey)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Metric\": [\"Overlap NDCs\", \"Only in without-key file\", \"Only in with-key file\", \"Percent overlap\"],\n",
    "    \"Value\": [overlap_ndcs, only_in_nokey, only_in_keyed, overlap_ndcs / len(ndc_nokey) * 100]\n",
    "})\n",
    "\n",
    "print(\"\\n=== Summary of Each File ===\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== NDC Overlap Comparison ===\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c36c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indiana only\n",
    "years_list = [2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
    "\n",
    "#Generating only one excel file for all the states\n",
    "#output_path = NDCATC_overview.export_cumulative_frequency_excel(years_list, by_state=False)  # This combines all states into one analysis\n",
    "#NDCATC_overview.export_cumulative_frequency_excel(years_list,  level='ATC2','ATC4'by_state=True,state_filter=['IN'])\n",
    "\n",
    "print(\"\\nComparing Indiana vs National...\")\n",
    "ind_vs_national = NDCATC_overview.compare_cumulative_80_analysis()\n",
    "\n",
    "ndc_pareto_results = NDCATC_overview.create_pareto_charts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e502d111",
   "metadata": {},
   "source": [
    "Doing the statistical analysis.\n",
    "The unscaled data is analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a33987ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDCATC_ind:  #Creating a new class to analyze correlations\n",
    "    \n",
    "    @staticmethod\n",
    "    def covariance_look(years_list, base_path=None, min_records=25):\n",
    "        \n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(\"COVARIANCE ANALYSIS: Units Reimbursed vs Number of Prescriptions\")\n",
    "        print(\"NATIONAL ANALYSIS - By ATC Class\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        all_within_class_cov = []\n",
    "        all_data_for_between = {}\n",
    "        \n",
    "        # Load and process all data\n",
    "        for year in years_list:\n",
    "            print(f\"Processing {year}...\", end=\" \")\n",
    "            try:\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\unscaled_data\\\\MUD_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                \n",
    "                records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "                if records.empty:\n",
    "                    print(\"No valid records\")\n",
    "                    continue\n",
    "                \n",
    "                # Scale both variables by 1e6\n",
    "                records['Units Reimbursed'] = records['Units Reimbursed'] / 1e3\n",
    "                records['Number of Prescriptions'] = records['Number of Prescriptions'] / 1e3        \n",
    "\n",
    "                print(f\"✓ ({len(records):,} records)\")\n",
    "                all_data_for_between[year] = records\n",
    "                \n",
    "                # Calculate within-class covariances for this year\n",
    "                for atc2 in records['ATC2 Class'].unique():\n",
    "                    subset = records[records['ATC2 Class'] == atc2]\n",
    "                    if len(subset) >= min_records:\n",
    "                        try:\n",
    "                            # Calculate covariance\n",
    "                            cov_matrix = np.cov(subset['Units Reimbursed'], subset['Number of Prescriptions'])\n",
    "                            covariance = cov_matrix[0, 1]\n",
    "                            \n",
    "                            atc2_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns else ''\n",
    "                            \n",
    "                            all_within_class_cov.append({\n",
    "                                'Year': year,\n",
    "                                'ATC2_Class': atc2,\n",
    "                                'ATC2_Name': atc2_name,\n",
    "                                'N_Records': len(subset),\n",
    "                                'Within_Class_Covariance': covariance,\n",
    "                                'Mean_Units': subset['Units Reimbursed'].mean(),\n",
    "                                'Mean_Prescriptions': subset['Number of Prescriptions'].mean()\n",
    "                            })\n",
    "                        except Exception as e:\n",
    "                            print(f\"\\nError processing {atc2}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "        \n",
    "        within_class_df = pd.DataFrame(all_within_class_cov)\n",
    "        \n",
    "        if within_class_df.empty:\n",
    "            print(\"No within-class covariance data collected!\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "        \n",
    "        # ==================== WITHIN-CLASS ANALYSIS ====================\n",
    "        print(f\"\\n{'='*70}\\n1. WITHIN-CLASS COVARIANCE ANALYSIS\\n{'='*70}\")\n",
    "        print(f\"Total class-year combinations: {len(within_class_df):,}\")\n",
    "        print(f\"Years analyzed: {sorted(within_class_df['Year'].unique())}\")\n",
    "        print(f\"Unique ATC2 classes: {len(within_class_df['ATC2_Class'].unique())}\")\n",
    "        \n",
    "        print(f\"\\nOverall Within-Class Covariance Statistics:\")\n",
    "        print(f\"  Mean: {within_class_df['Within_Class_Covariance'].mean():,.10f}\")\n",
    "        print(f\"  Median: {within_class_df['Within_Class_Covariance'].median():,.10f}\")\n",
    "        print(f\"  Std Dev: {within_class_df['Within_Class_Covariance'].std():,.10f}\")\n",
    "        print(f\"  Range: {within_class_df['Within_Class_Covariance'].min():,.10f} to {within_class_df['Within_Class_Covariance'].max():,.10f}\")\n",
    "        \n",
    "        # Aggregate by ATC class across all years\n",
    "        atc_within_summary = within_class_df.groupby(['ATC2_Class', 'ATC2_Name']).agg({\n",
    "            'Within_Class_Covariance': ['mean', 'std'],\n",
    "            'N_Records': 'sum'\n",
    "        }).round(2)\n",
    "        atc_within_summary.columns = ['Avg_Within_Cov', 'Std_Within_Cov', 'Total_Records']\n",
    "        atc_within_summary = atc_within_summary.sort_values('Avg_Within_Cov', ascending=False)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\\nWITHIN-CLASS COVARIANCE BY ATC2 CLASS (Averaged across years)\\n{'='*70}\")\n",
    "        print(f\"{'ATC2':<5} {'Name':<30} {'Avg Cov':>15} {'Std':>12} {'Total N':>10}\")\n",
    "        print(\"-\" * 80)\n",
    "        for (atc_class, atc_name), row in atc_within_summary.head(15).iterrows():\n",
    "            name_short = atc_name[:28] if atc_name else atc_class\n",
    "            print(f\"{atc_class:<5} {name_short:<30} {row['Avg_Within_Cov']:>15,.10f} {row['Std_Within_Cov']:>12,.10f} {row['Total_Records']:>10,.0f}\")\n",
    "        \n",
    "        # ==================== BETWEEN-CLASS ANALYSIS ====================\n",
    "        print(f\"\\n{'='*70}\\n2. BETWEEN-CLASS COVARIANCE ANALYSIS\\n{'='*70}\")\n",
    "        \n",
    "        between_class_results = []\n",
    "        \n",
    "        for year in sorted(all_data_for_between.keys()):\n",
    "            year_data = all_data_for_between[year]\n",
    "            \n",
    "            # Get class means for this year\n",
    "            class_means = year_data.groupby('ATC2 Class').agg({\n",
    "                'Units Reimbursed': 'mean',\n",
    "                'Number of Prescriptions': 'mean',\n",
    "                'ATC2_Name': 'first'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Filter classes with sufficient records\n",
    "            class_counts = year_data['ATC2 Class'].value_counts()\n",
    "            valid_classes = class_counts[class_counts >= min_records].index\n",
    "            class_means = class_means[class_means['ATC2 Class'].isin(valid_classes)]\n",
    "            \n",
    "            if len(class_means) < 2:\n",
    "                print(f\"{year}: Not enough classes for between-class analysis\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate between-class covariance\n",
    "            between_cov = np.cov(\n",
    "                class_means['Units Reimbursed'],\n",
    "                class_means['Number of Prescriptions']\n",
    "            )[0, 1]\n",
    "            \n",
    "            between_class_results.append({\n",
    "                'Year': year,\n",
    "                'Between_Class_Covariance': between_cov,\n",
    "                'N_Classes': len(class_means),\n",
    "                'Total_Records': len(year_data)\n",
    "            })\n",
    "            \n",
    "            print(f\"Year {year}: Between-Class Cov = {between_cov:>15,.10f} (across {len(class_means)} classes, {len(year_data):,} records)\")\n",
    "        \n",
    "        between_class_df = pd.DataFrame(between_class_results)\n",
    "        \n",
    "        if not between_class_df.empty:\n",
    "            print(f\"\\n{'='*70}\\nBETWEEN-CLASS COVARIANCE SUMMARY\\n{'='*70}\")\n",
    "            print(f\"Average across years: {between_class_df['Between_Class_Covariance'].mean():,.10f}\")\n",
    "            print(f\"Median: {between_class_df['Between_Class_Covariance'].median():,.10f}\")\n",
    "            print(f\"Std Dev: {between_class_df['Between_Class_Covariance'].std():,.10f}\")\n",
    "            print(f\"Range: {between_class_df['Between_Class_Covariance'].min():,.10f} to {between_class_df['Between_Class_Covariance'].max():,.10f}\")\n",
    "            \n",
    "            # Overall covariance (all data pooled across all classes and years)\n",
    "            print(f\"\\n{'='*70}\\n3. OVERALL COVARIANCE (All classes and years combined)\\n{'='*70}\")\n",
    "            all_combined = pd.concat(all_data_for_between.values(), ignore_index=True)\n",
    "            overall_cov = np.cov(\n",
    "                all_combined['Units Reimbursed'],\n",
    "                all_combined['Number of Prescriptions']\n",
    "            )[0, 1]\n",
    "            print(f\"Overall Covariance: {overall_cov:,.10f} ({len(all_combined):,} total records)\")\n",
    "            \n",
    "            # Comparison\n",
    "            print(f\"\\n{'='*70}\\nCOMPARISON\\n{'='*70}\")\n",
    "            avg_within = within_class_df['Within_Class_Covariance'].mean()\n",
    "            avg_between = between_class_df['Between_Class_Covariance'].mean()\n",
    "            print(f\"Average Within-Class Covariance:  {avg_within:>15,.10f}\")\n",
    "            print(f\"Average Between-Class Covariance: {avg_between:>15,.10f}\")\n",
    "            print(f\"Overall Covariance:               {overall_cov:>15,.10f}\")\n",
    "            print(f\"\\nRatio (Between/Within):           {avg_between/avg_within if avg_within != 0 else 'undefined':>15.4f}\")\n",
    "        \n",
    "        return within_class_df, between_class_df\n",
    "\n",
    "    @staticmethod\n",
    "    def analyze_correlation_by_state_atc(years_list, base_path=None, min_records=25):\n",
    "        \n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(\"CORRELATION ANALYSIS: Units Reimbursed vs Number of Prescriptions\")\n",
    "        print(\"INDIANA ONLY - By ATC Class\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        all_correlations = []\n",
    "        \n",
    "        for year in years_list:\n",
    "            print(f\"Processing {year}...\", end=\" \")\n",
    "            try:\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                \n",
    "                records = df_merged[(df_merged['ATC4 Class'].notna()) & (df_merged['State'] == 'IN')].copy()\n",
    "                if records.empty:\n",
    "                    print(\"No valid Indiana records\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"✓ ({len(records):,} records)\")\n",
    "                \n",
    "                for atc2 in records['ATC2 Class'].unique():\n",
    "                    subset = records[records['ATC2 Class'] == atc2]\n",
    "                    if len(subset) >= min_records:\n",
    "                        try:\n",
    "                            pearson_r, pearson_p = pearsonr(subset['Units Reimbursed'], subset['Number of Prescriptions'])\n",
    "                            spearman_r, spearman_p = spearmanr(subset['Units Reimbursed'], subset['Number of Prescriptions'])\n",
    "                            atc2_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns else ''\n",
    "                            \n",
    "                            all_correlations.append({\n",
    "                                'Year': year, 'State': 'IN', 'ATC2_Class': atc2, 'ATC2_Name': atc2_name,\n",
    "                                'N_Records': len(subset), 'Pearson_r': pearson_r, 'Pearson_p': pearson_p,\n",
    "                                'Spearman_r': spearman_r, 'Spearman_p': spearman_p\n",
    "                            })\n",
    "                        except Exception as e:\n",
    "                            print(f\"\\nError processing IN-{atc2}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "        \n",
    "        correlations_df = pd.DataFrame(all_correlations)\n",
    "        if correlations_df.empty:\n",
    "            print(\"No correlation data collected!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # SUMMARY STATISTICS\n",
    "        print(f\"\\n{'='*70}\\nSUMMARY STATISTICS FOR INDIANA\\n{'='*70}\")\n",
    "        print(f\"Total combinations: {len(correlations_df):,} | Years: {sorted(correlations_df['Year'].unique())} | ATC classes: {len(correlations_df['ATC2_Class'].unique())}\")\n",
    "        \n",
    "        # PEARSON CORRELATION RESULTS\n",
    "        print(f\"\\n{'='*70}\\nPEARSON CORRELATION RESULTS\\n{'='*70}\")\n",
    "        print(f\"Average: {correlations_df['Pearson_r'].mean():.4f} | Range: {correlations_df['Pearson_r'].min():.4f} to {correlations_df['Pearson_r'].max():.4f} | Std Dev: {correlations_df['Pearson_r'].std():.4f}\")\n",
    "        \n",
    "        atc_pearson = correlations_df.groupby(['ATC2_Class', 'ATC2_Name']).agg({\n",
    "            'Pearson_r': ['mean', 'std'], 'Pearson_p': 'mean', 'N_Records': 'sum'\n",
    "        }).round(4)\n",
    "        atc_pearson.columns = ['Avg_Pearson', 'Std_Pearson', 'Avg_P_Value', 'Total_Records']\n",
    "        atc_pearson = atc_pearson.sort_values('Avg_Pearson', ascending=False)\n",
    "        \n",
    "        print(f\"\\nPEARSON BY ATC CLASS (Average across years):\")\n",
    "        print(f\"{'ATC2':<5} {'Name':<25} {'Avg Pearson':<12} {'Std':<8} {'p-val':<8} {'Total N':<8}\\n{'-' * 75}\")\n",
    "        for (atc_class, atc_name), row in atc_pearson.iterrows():\n",
    "            name_short = atc_name[:23] if atc_name else atc_class\n",
    "            print(f\"{atc_class:<5} {name_short:<25} {row['Avg_Pearson']:<12.4f} {row['Std_Pearson']:<8.4f} {row['Avg_P_Value']:<8.4f} {row['Total_Records']:<8.0f}\")\n",
    "        \n",
    "        # SPEARMAN CORRELATION RESULTS\n",
    "        print(f\"\\n{'='*70}\\nSPEARMAN RANK CORRELATION RESULTS\\n{'='*70}\")\n",
    "        print(f\"Average: {correlations_df['Spearman_r'].mean():.4f} | Range: {correlations_df['Spearman_r'].min():.4f} to {correlations_df['Spearman_r'].max():.4f} | Std Dev: {correlations_df['Spearman_r'].std():.4f}\")\n",
    "        \n",
    "        atc_spearman = correlations_df.groupby(['ATC2_Class', 'ATC2_Name']).agg({\n",
    "            'Spearman_r': ['mean', 'std'], 'Spearman_p': 'mean', 'N_Records': 'sum'\n",
    "        }).round(4)\n",
    "        atc_spearman.columns = ['Avg_Spearman', 'Std_Spearman', 'Avg_P_Value', 'Total_Records']\n",
    "        atc_spearman = atc_spearman.sort_values('Avg_Spearman', ascending=False)\n",
    "        \n",
    "        print(f\"\\nSPEARMAN BY ATC CLASS (Average across years):\")\n",
    "        print(f\"{'ATC2':<5} {'Name':<25} {'Avg Spearman':<12} {'Std':<8} {'p-val':<8} {'Total N':<8}\\n{'-' * 75}\")\n",
    "        for (atc_class, atc_name), row in atc_spearman.iterrows():\n",
    "            name_short = atc_name[:23] if atc_name else atc_class\n",
    "            print(f\"{atc_class:<5} {name_short:<25} {row['Avg_Spearman']:<12.4f} {row['Std_Spearman']:<8.4f} {row['Avg_P_Value']:<8.4f} {row['Total_Records']:<8.0f}\")\n",
    "        # YEAR-OVER-YEAR TRENDS\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"YEAR-OVER-YEAR TRENDS (Top 3 by Pearson)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        top_classes = atc_pearson.head(3).index.get_level_values(0).tolist()\n",
    "        for atc_class in top_classes:\n",
    "            atc_data = correlations_df[correlations_df['ATC2_Class'] == atc_class].sort_values('Year')\n",
    "            atc_name = atc_data['ATC2_Name'].iloc[0] if not atc_data.empty else atc_class\n",
    "            \n",
    "            print(f\"\\n{atc_class} - {atc_name[:30]}:\")\n",
    "            print(f\"{'Year':<6} {'Pearson':<8} {'Spearman':<9} {'N':<6}\")\n",
    "            print(\"-\" * 35)\n",
    "            for _, row in atc_data.iterrows():\n",
    "                print(f\"{row['Year']:<6} {row['Pearson_r']:<8.4f} {row['Spearman_r']:<9.4f} {row['N_Records']:<6}\")\n",
    "        \n",
    "        return correlations_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_units_vs_prescriptions_by_atc(years_list, base_path=None, min_records=25, include_negative=True):\n",
    "\n",
    "        if base_path is None:\n",
    "            base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "        \n",
    "        print(\"Creating plots for Indiana ATC classes...\")\n",
    "        \n",
    "        # Combine all years of data\n",
    "        all_data = []\n",
    "        for year in years_list:\n",
    "            try:\n",
    "                csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "                df_merged = pd.read_csv(csv_path)\n",
    "                records = df_merged[(df_merged['ATC4 Class'].notna()) & (df_merged['State'] == 'IN')].copy()\n",
    "                records['Year'] = year\n",
    "                all_data.append(records)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {year}: {e}\")\n",
    "        \n",
    "        if not all_data:\n",
    "            print(\"No data loaded!\")\n",
    "            return\n",
    "        \n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # Get ATC classes with sufficient data and calculate correlations\n",
    "        atc_counts = combined_df['ATC2 Class'].value_counts()\n",
    "        sufficient_data_classes = atc_counts[atc_counts >= min_records].index\n",
    "        \n",
    "        # Calculate correlations for all classes with sufficient data\n",
    "        class_correlations = {}\n",
    "        for atc_class in sufficient_data_classes:\n",
    "            subset = combined_df[combined_df['ATC2 Class'] == atc_class]\n",
    "            if len(subset) > 1:\n",
    "                corr = subset['Number of Prescriptions'].corr(subset['Units Reimbursed'])\n",
    "                class_correlations[atc_class] = corr\n",
    "        \n",
    "        # Select classes to plot\n",
    "        if include_negative:\n",
    "            # Get top positive correlations and all negative correlations\n",
    "            positive_corrs = {k: v for k, v in class_correlations.items() if v >= 0}\n",
    "            negative_corrs = {k: v for k, v in class_correlations.items() if v < 0}\n",
    "            \n",
    "            # Sort positive by correlation (descending) and negative by correlation (ascending, most negative first)\n",
    "            positive_sorted = sorted(positive_corrs.items(), key=lambda x: x[1], reverse=True)\n",
    "            negative_sorted = sorted(negative_corrs.items(), key=lambda x: x[1])\n",
    "            \n",
    "            # Take top 8 positive and all negative (up to 4 more)\n",
    "            selected_positive = [x[0] for x in positive_sorted[:8]]\n",
    "            selected_negative = [x[0] for x in negative_sorted[:4]]\n",
    "            \n",
    "            valid_atc_classes = selected_positive + selected_negative\n",
    "            \n",
    "            print(f\"\\nSelected classes: {len(selected_positive)} positive correlations + {len(selected_negative)} negative correlations\")\n",
    "            if selected_negative:\n",
    "                print(f\"Negative correlation classes: {selected_negative}\")\n",
    "        else:\n",
    "            # Original behavior - top classes by count\n",
    "            valid_atc_classes = sufficient_data_classes[:12]\n",
    "        \n",
    "        # Determine grid size based on number of classes\n",
    "        n_classes = len(valid_atc_classes)\n",
    "        if n_classes <= 6:\n",
    "            rows, cols = 2, 3\n",
    "        elif n_classes <= 9:\n",
    "            rows, cols = 3, 3\n",
    "        elif n_classes <= 12:\n",
    "            rows, cols = 3, 4\n",
    "        else:\n",
    "            rows, cols = 4, 4\n",
    "            valid_atc_classes = valid_atc_classes[:16]  # Limit to 16 for display\n",
    "        \n",
    "        # Set up the plot grid\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols*5, rows*4))\n",
    "        if rows == 1 or cols == 1:\n",
    "            axes = axes.flatten() if hasattr(axes, 'flatten') else [axes]\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(valid_atc_classes)))\n",
    "        \n",
    "        for i, atc_class in enumerate(valid_atc_classes):\n",
    "            subset = combined_df[combined_df['ATC2 Class'] == atc_class]\n",
    "            atc_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns and not subset['ATC2_Name'].isna().all() else atc_class\n",
    "            \n",
    "            # Create scatter plot\n",
    "            axes[i].scatter(subset['Number of Prescriptions'], \n",
    "                           subset['Units Reimbursed'], \n",
    "                           alpha=0.6, color=colors[i], s=20)\n",
    "            \n",
    "            # Add trend line\n",
    "            if len(subset) > 1:\n",
    "                z = np.polyfit(subset['Number of Prescriptions'], subset['Units Reimbursed'], 1)\n",
    "                p = np.poly1d(z)\n",
    "                axes[i].plot(subset['Number of Prescriptions'], p(subset['Number of Prescriptions']), \n",
    "                            \"r--\", alpha=0.8, linewidth=1)\n",
    "            \n",
    "            # Format axes\n",
    "            axes[i].set_xlabel('Number of Prescriptions')\n",
    "            axes[i].set_ylabel('Units Reimbursed')\n",
    "            axes[i].set_title(f'{atc_class}\\n{atc_name[:30]}', fontsize=10)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add correlation coefficient with color coding\n",
    "            if len(subset) > 1:\n",
    "                corr = subset['Number of Prescriptions'].corr(subset['Units Reimbursed'])\n",
    "                color = 'red' if corr < 0 else 'blue'\n",
    "                axes[i].text(0.05, 0.95, f'r = {corr:.3f}', \n",
    "                            transform=axes[i].transAxes, \n",
    "                            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8, edgecolor=color),\n",
    "                            fontsize=9, color=color)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for j in range(len(valid_atc_classes), len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        title_suffix = \" (Including Negative Correlations)\" if include_negative else \"\"\n",
    "        plt.suptitle(f'Indiana: Units Reimbursed vs Number of Prescriptions by ATC2 Class{title_suffix}\\n(All Years Combined)', \n",
    "                     fontsize=16, y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary table\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"PLOT SUMMARY - INDIANA ATC CLASSES\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"{'ATC2':<5} {'Name':<30} {'Records':<8} {'Correlation':<12} {'Type':<8}\")\n",
    "        print(\"-\" * 90)\n",
    "        \n",
    "        for atc_class in valid_atc_classes:\n",
    "            subset = combined_df[combined_df['ATC2 Class'] == atc_class]\n",
    "            atc_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns and not subset['ATC2_Name'].isna().all() else atc_class\n",
    "            corr = subset['Number of Prescriptions'].corr(subset['Units Reimbursed'])\n",
    "            corr_type = \"Negative\" if corr < 0 else \"Positive\"\n",
    "            \n",
    "            print(f\"{atc_class:<5} {atc_name[:28]:<30} {len(subset):<8} {corr:<12.4f} {corr_type:<8}\")\n",
    "        \n",
    "        return combined_df[combined_df['ATC2 Class'].isin(valid_atc_classes)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f230401",
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def correlation_picture(years_list, base_path=None, min_records=25):\n",
    "    \n",
    "    if base_path is None:\n",
    "        base_path = rf\"c:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\"\n",
    "    \n",
    "    print(\"CORRELATION ANALYSIS: Units Reimbursed vs Number of Prescriptions\")\n",
    "    print(\"NATIONAL ANALYSIS - By ATC Class\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    all_within_class_corr = []\n",
    "    all_data_for_between = {}\n",
    "    \n",
    "    # Load and process all data\n",
    "    for year in years_list:\n",
    "        print(f\"Processing {year}...\", end=\" \")\n",
    "        try:\n",
    "            csv_path = os.path.join(base_path, f\"ATC\\\\merged_data\\\\merged_NEWdata_{year}.csv\")\n",
    "            df_merged = pd.read_csv(csv_path)\n",
    "            \n",
    "            records = df_merged[df_merged['ATC4 Class'].notna()].copy()\n",
    "            if records.empty:\n",
    "                print(\"No valid records\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"✓ ({len(records):,} records)\")\n",
    "            all_data_for_between[year] = records\n",
    "            \n",
    "            # Calculate within-class correlations for this year\n",
    "            for atc2 in records['ATC2 Class'].unique():\n",
    "                subset = records[records['ATC2 Class'] == atc2]\n",
    "                if len(subset) >= min_records:\n",
    "                    try:\n",
    "                        # Calculate correlation\n",
    "                        correlation = subset['Units Reimbursed'].corr(subset['Number of Prescriptions'])\n",
    "                        \n",
    "                        atc2_name = subset['ATC2_Name'].iloc[0] if 'ATC2_Name' in subset.columns else ''\n",
    "                        \n",
    "                        all_within_class_corr.append({\n",
    "                            'Year': year,\n",
    "                            'ATC2_Class': atc2,\n",
    "                            'ATC2_Name': atc2_name,\n",
    "                            'N_Records': len(subset),\n",
    "                            'Within_Class_Correlation': correlation,\n",
    "                            'Mean_Units': subset['Units Reimbursed'].mean(),\n",
    "                            'Mean_Prescriptions': subset['Number of Prescriptions'].mean()\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\nError processing {atc2}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error: {e}\")\n",
    "    \n",
    "    within_class_df = pd.DataFrame(all_within_class_corr)\n",
    "    \n",
    "    if within_class_df.empty:\n",
    "        print(\"No within-class correlation data collected!\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    # ==================== WITHIN-CLASS ANALYSIS ====================\n",
    "    print(f\"\\n{'='*70}\\n1. WITHIN-CLASS CORRELATION ANALYSIS\\n{'='*70}\")\n",
    "    print(f\"Total class-year combinations: {len(within_class_df):,}\")\n",
    "    print(f\"Years analyzed: {sorted(within_class_df['Year'].unique())}\")\n",
    "    print(f\"Unique ATC2 classes: {len(within_class_df['ATC2_Class'].unique())}\")\n",
    "    \n",
    "    print(f\"\\nOverall Within-Class Correlation Statistics:\")\n",
    "    print(f\"  Mean: {within_class_df['Within_Class_Correlation'].mean():.4f}\")\n",
    "    print(f\"  Median: {within_class_df['Within_Class_Correlation'].median():.4f}\")\n",
    "    print(f\"  Std Dev: {within_class_df['Within_Class_Correlation'].std():.4f}\")\n",
    "    print(f\"  Range: {within_class_df['Within_Class_Correlation'].min():.4f} to {within_class_df['Within_Class_Correlation'].max():.4f}\")\n",
    "    \n",
    "    # Aggregate by ATC class across all years\n",
    "    atc_within_summary = within_class_df.groupby(['ATC2_Class', 'ATC2_Name']).agg({\n",
    "        'Within_Class_Correlation': ['mean', 'std'],\n",
    "        'N_Records': 'sum'\n",
    "    }).round(4)\n",
    "    atc_within_summary.columns = ['Avg_Within_Corr', 'Std_Within_Corr', 'Total_Records']\n",
    "    atc_within_summary = atc_within_summary.sort_values('Avg_Within_Corr', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\nWITHIN-CLASS CORRELATION BY ATC2 CLASS (Averaged across years)\\n{'='*70}\")\n",
    "    print(f\"{'ATC2':<5} {'Name':<30} {'Avg Corr':>12} {'Std':>10} {'Total N':>10}\")\n",
    "    print(\"-\" * 75)\n",
    "    for (atc_class, atc_name), row in atc_within_summary.head(15).iterrows():\n",
    "        name_short = atc_name[:28] if atc_name else atc_class\n",
    "        print(f\"{atc_class:<5} {name_short:<30} {row['Avg_Within_Corr']:>12.4f} {row['Std_Within_Corr']:>10.4f} {row['Total_Records']:>10,.0f}\")\n",
    "    \n",
    "    # ==================== BETWEEN-CLASS ANALYSIS ====================\n",
    "    print(f\"\\n{'='*70}\\n2. BETWEEN-CLASS CORRELATION ANALYSIS\\n{'='*70}\")\n",
    "    \n",
    "    between_class_results = []\n",
    "    \n",
    "    for year in sorted(all_data_for_between.keys()):\n",
    "        year_data = all_data_for_between[year]\n",
    "        \n",
    "        # Get class means for this year\n",
    "        class_means = year_data.groupby('ATC2 Class').agg({\n",
    "            'Units Reimbursed': 'mean',\n",
    "            'Number of Prescriptions': 'mean',\n",
    "            'ATC2_Name': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Filter classes with sufficient records\n",
    "        class_counts = year_data['ATC2 Class'].value_counts()\n",
    "        valid_classes = class_counts[class_counts >= min_records].index\n",
    "        class_means = class_means[class_means['ATC2 Class'].isin(valid_classes)]\n",
    "        \n",
    "        if len(class_means) < 2:\n",
    "            print(f\"{year}: Not enough classes for between-class analysis\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate between-class correlation\n",
    "        between_corr = class_means['Units Reimbursed'].corr(class_means['Number of Prescriptions'])\n",
    "        \n",
    "        between_class_results.append({\n",
    "            'Year': year,\n",
    "            'Between_Class_Correlation': between_corr,\n",
    "            'N_Classes': len(class_means),\n",
    "            'Total_Records': len(year_data)\n",
    "        })\n",
    "        \n",
    "        print(f\"Year {year}: Between-Class Corr = {between_corr:.4f} (across {len(class_means)} classes, {len(year_data):,} records)\")\n",
    "    \n",
    "    between_class_df = pd.DataFrame(between_class_results)\n",
    "    \n",
    "    if not between_class_df.empty:\n",
    "        print(f\"\\n{'='*70}\\nBETWEEN-CLASS CORRELATION SUMMARY\\n{'='*70}\")\n",
    "        print(f\"Average across years: {between_class_df['Between_Class_Correlation'].mean():.4f}\")\n",
    "        print(f\"Median: {between_class_df['Between_Class_Correlation'].median():.4f}\")\n",
    "        print(f\"Std Dev: {between_class_df['Between_Class_Correlation'].std():.4f}\")\n",
    "        print(f\"Range: {between_class_df['Between_Class_Correlation'].min():.4f} to {between_class_df['Between_Class_Correlation'].max():.4f}\")\n",
    "        \n",
    "        # Overall correlation (all data pooled across all classes and years)\n",
    "        print(f\"\\n{'='*70}\\n3. OVERALL CORRELATION (All classes and years combined)\\n{'='*70}\")\n",
    "        all_combined = pd.concat(all_data_for_between.values(), ignore_index=True)\n",
    "        overall_corr = all_combined['Units Reimbursed'].corr(all_combined['Number of Prescriptions'])\n",
    "        print(f\"Overall Correlation: {overall_corr:.4f} ({len(all_combined):,} total records)\")\n",
    "        \n",
    "        # Comparison\n",
    "        print(f\"\\n{'='*70}\\nCOMPARISON\\n{'='*70}\")\n",
    "        avg_within = within_class_df['Within_Class_Correlation'].mean()\n",
    "        avg_between = between_class_df['Between_Class_Correlation'].mean()\n",
    "        print(f\"Average Within-Class Correlation:  {avg_within:>12.4f}\")\n",
    "        print(f\"Average Between-Class Correlation: {avg_between:>12.4f}\")\n",
    "        print(f\"Overall Correlation:               {overall_corr:>12.4f}\")\n",
    "    \n",
    "    return within_class_df, between_class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e41c5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVARIANCE ANALYSIS: Units Reimbursed vs Number of Prescriptions\n",
      "NATIONAL ANALYSIS - By ATC Class\n",
      "======================================================================\n",
      "Processing 2016... ✓ (1,818,174 records)\n",
      "Processing 2017... ✓ (1,931,088 records)\n",
      "Processing 2018... ✓ (2,022,672 records)\n",
      "Processing 2019... ✓ (2,109,684 records)\n",
      "Processing 2020... ✓ (2,173,775 records)\n",
      "Processing 2021... ✓ (2,287,508 records)\n",
      "Processing 2022... ✓ (2,353,181 records)\n",
      "Processing 2023... ✓ (2,385,896 records)\n",
      "Processing 2024... ✓ (2,338,667 records)\n",
      "\n",
      "======================================================================\n",
      "1. WITHIN-CLASS COVARIANCE ANALYSIS\n",
      "======================================================================\n",
      "Total class-year combinations: 782\n",
      "Years analyzed: [np.int64(2016), np.int64(2017), np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023), np.int64(2024)]\n",
      "Unique ATC2 classes: 88\n",
      "\n",
      "Overall Within-Class Covariance Statistics:\n",
      "  Mean: 220.1820741807\n",
      "  Median: 33.8981366064\n",
      "  Std Dev: 1,308.0082293363\n",
      "  Range: -4.2830359326 to 32,557.2624805437\n",
      "\n",
      "======================================================================\n",
      "WITHIN-CLASS COVARIANCE BY ATC2 CLASS (Averaged across years)\n",
      "======================================================================\n",
      "ATC2  Name                                   Avg Cov          Std    Total N\n",
      "--------------------------------------------------------------------------------\n",
      "D09   MEDICATED DRESSINGS            8,409.7400000000 10,139.3300000000        539\n",
      "D08   ANTISEPTICS AND DISINFECTANT   1,663.0100000000 1,658.5600000000      4,535\n",
      "R02   THROAT PREPARATIONS             851.5200000000 281.3000000000     57,835\n",
      "R03   DRUGS FOR OBSTRUCTIVE AIRWAY    844.7000000000 275.9800000000    383,756\n",
      "B05   BLOOD SUBSTITUTES AND PERFUS    662.9600000000 223.6800000000    173,236\n",
      "R06   ANTIHISTAMINES FOR SYSTEMIC     556.2800000000 188.6000000000    282,557\n",
      "M02   TOPICAL PRODUCTS FOR JOINT A    525.5200000000 263.1400000000     94,434\n",
      "G02   OTHER GYNECOLOGICALS            473.7500000000 230.3000000000     65,096\n",
      "A06   DRUGS FOR CONSTIPATION          453.8700000000 224.4400000000    216,109\n",
      "A11   VITAMINS                        386.6600000000 188.1700000000     47,101\n",
      "A12   MINERAL SUPPLEMENTS             338.6800000000 332.3200000000    149,356\n",
      "S03   OPHTHALMOLOGICAL AND OTOLOGI    294.2300000000 301.2100000000     48,653\n",
      "R01   NASAL PREPARATIONS              277.0400000000 87.4300000000    163,206\n",
      "R05   COUGH AND COLD PREPARATIONS     261.6100000000 169.0200000000    160,573\n",
      "N07   OTHER NERVOUS SYSTEM DRUGS      247.5100000000 138.2700000000    210,065\n",
      "\n",
      "======================================================================\n",
      "2. BETWEEN-CLASS COVARIANCE ANALYSIS\n",
      "======================================================================\n",
      "Year 2016: Between-Class Cov =   16.6014730133 (across 87 classes, 1,818,174 records)\n",
      "Year 2017: Between-Class Cov =   12.0200106456 (across 87 classes, 1,931,088 records)\n",
      "Year 2018: Between-Class Cov =   28.9408518642 (across 87 classes, 2,022,672 records)\n",
      "Year 2019: Between-Class Cov =    2.2103563105 (across 87 classes, 2,109,684 records)\n",
      "Year 2020: Between-Class Cov =   13.9258036636 (across 87 classes, 2,173,775 records)\n",
      "Year 2021: Between-Class Cov =    4.5462543001 (across 87 classes, 2,287,508 records)\n",
      "Year 2022: Between-Class Cov =    0.5515093533 (across 85 classes, 2,353,181 records)\n",
      "Year 2023: Between-Class Cov =   24.2404933662 (across 88 classes, 2,385,896 records)\n",
      "Year 2024: Between-Class Cov =    5.3549799508 (across 87 classes, 2,338,667 records)\n",
      "\n",
      "======================================================================\n",
      "BETWEEN-CLASS COVARIANCE SUMMARY\n",
      "======================================================================\n",
      "Average across years: 12.0435258297\n",
      "Median: 12.0200106456\n",
      "Std Dev: 9.9266525160\n",
      "Range: 0.5515093533 to 28.9408518642\n",
      "\n",
      "======================================================================\n",
      "3. OVERALL COVARIANCE (All classes and years combined)\n",
      "======================================================================\n",
      "Overall Covariance: 125.6502979210 (19,420,645 total records)\n",
      "\n",
      "======================================================================\n",
      "COMPARISON\n",
      "======================================================================\n",
      "Average Within-Class Covariance:   220.1820741807\n",
      "Average Between-Class Covariance:   12.0435258297\n",
      "Overall Covariance:                125.6502979210\n",
      "\n",
      "Ratio (Between/Within):                    0.0547\n"
     ]
    }
   ],
   "source": [
    "years_to_analyze = [2016,2017,2018,2019, 2020, 2021, 2022, 2023, 2024]\n",
    "covarience_results = NDCATC_ind.covariance_look(years_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dd55e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(corr_results))\n",
    "print(type(covarience_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1630906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#covarience_results = NDCATC_ind.analyze_covariance_by_state_atc(years_to_analyze)\n",
    "\n",
    "#plot_data = NDCATC_ind.plot_units_vs_prescriptions_by_atc(years_to_analyze)\n",
    "correlation_results = NDCATC_ind.analyze_correlation_by_state_atc(years_to_analyze)\n",
    "#plots= NDCATC_ind.plot_units_vs_prescriptions_by_atc(years_to_analyze, include_negative=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
