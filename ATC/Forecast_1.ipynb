{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de25d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "from utilsforecast.plotting import plot_series\n",
    "from utilsforecast.evaluation import evaluate\n",
    "from utilsforecast.losses import *\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import (\n",
    "    Naive,WindowAverage, ARIMA, \n",
    "    AutoARIMA,SeasonalNaive,HoltWinters,\n",
    "    CrostonClassic as Croston, HistoricAverage,DynamicOptimizedTheta as DOT,\n",
    "    SeasonalNaive\n",
    ")\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "from itertools import product\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # To ignore warnings from pandas/numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "user=\"Lilian\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1baf5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New data class created to handle configuration parameters\n",
    "@dataclass\n",
    "class ForecastConfig:\n",
    "    \n",
    "    # Forecast parameters\n",
    "    h: int = 8                          \n",
    "    season_length: int = 4              \n",
    "    \n",
    "    # Cross-validation parameters\n",
    "    n_windows: int = 2                  \n",
    "    step_size: Optional[int] = None     \n",
    "    \n",
    "    # Train-test split parameters\n",
    "    train_size: Optional[int] = None    # Use all available data except test\n",
    "    test_size: Optional[int] = None     # Auto-set to h in __post_init__\n",
    "    \n",
    "    # Plotting parameters\n",
    "    n_samples: int = 4                  # Plot 4 random samples\n",
    "    models_to_plot: Optional[List[str]] = None  # \n",
    "    \n",
    "    # Other settings\n",
    "    confidence_level: int = 95          # 95% confidence intervals\n",
    "    n_jobs: int = -1                    \n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \n",
    "        if self.step_size is None:\n",
    "            self.step_size = self.h\n",
    "        if self.test_size is None:\n",
    "            self.test_size = self.h\n",
    "        if self.models_to_plot is None:\n",
    "            self.models_to_plot = ['Naive', 'ARIMA_manual', 'SARIMA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ba352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_models(config):\n",
    "    \n",
    "    models = [Naive(), HistoricAverage(), WindowAverage(window_size=4),\n",
    "        SeasonalNaive(season_length=4), ARIMA(order=(1, 1, 1), alias=\"ARIMA_manual\"),\n",
    "        AutoARIMA(seasonal=True, season_length=4, alias=\"SARIMA\"),\n",
    "    ]\n",
    "    return models\n",
    "\n",
    "def train_and_forecast(df,target_name, config):\n",
    "    \n",
    "    sf = StatsForecast(\n",
    "        models=get_models(config),\n",
    "        freq='QS',\n",
    "        n_jobs=config.n_jobs,\n",
    "        fallback_model=SeasonalNaive(season_length=config.season_length)\n",
    "    )\n",
    "    \n",
    "    forecasts_df = sf.forecast(df=df, h=config.h, level=[config.confidence_level])\n",
    "    return forecasts_df, sf\n",
    "\n",
    "def evaluate_train_test(df, target_name, config):\n",
    "\n",
    "    if config.train_size is None:\n",
    "        train = df.groupby('unique_id').apply(\n",
    "            lambda x: x.iloc[:-config.test_size]\n",
    "        ).reset_index(drop=True)\n",
    "    else:\n",
    "        train = df.groupby('unique_id').apply(\n",
    "            lambda x: x.iloc[-(config.train_size + config.test_size):-config.test_size]\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    test = df.groupby('unique_id').apply(\n",
    "        lambda x: x.iloc[-config.test_size:]\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    sf = StatsForecast(\n",
    "        models=get_models(config),\n",
    "        freq='QS',\n",
    "        n_jobs=config.n_jobs,\n",
    "        fallback_model=SeasonalNaive(season_length=config.season_length)\n",
    "    )\n",
    "    sf.fit(df=train)\n",
    "    preds = sf.predict(h=config.h)\n",
    "\n",
    "    preds_df = pd.merge(test, preds.reset_index(), on=['ds', 'unique_id'], how='left')\n",
    "    models = [col for col in preds.columns if col not in ['unique_id', 'ds']]\n",
    "    \n",
    "    eval_df = evaluate(preds_df, metrics=[mae, mse, rmse], models=models)\n",
    "    \n",
    "    mae_df = eval_df[eval_df['metric'] == 'mae'].copy()\n",
    "    mae_df['best_model'] = mae_df[models].idxmin(axis=1)\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Best Models (Train-Test Split - based on MAE):\")\n",
    "    print(mae_df['best_model'].value_counts())\n",
    "    \n",
    "    return eval_df, preds_df, train, test  \n",
    "\n",
    "def evaluate_model_cross(df, target_name, config):\n",
    "    \n",
    "    sf = StatsForecast(\n",
    "        models=get_models(config),\n",
    "        freq='QS',\n",
    "        n_jobs=config.n_jobs,\n",
    "        fallback_model=SeasonalNaive(season_length=config.season_length)\n",
    "    )\n",
    "\n",
    "    print(f\"   Running cross-validation...\")\n",
    "    cv_df = sf.cross_validation(\n",
    "        df=df,\n",
    "        h=config.h,\n",
    "        n_windows=config.n_windows,\n",
    "        step_size=config.step_size\n",
    "    )\n",
    "    \n",
    "    # Define model columns\n",
    "    exclude_cols = ['unique_id', 'ds', 'y', 'cutoff', 'metric']\n",
    "    model_cols = [col for col in cv_df.columns if col not in exclude_cols]\n",
    "\n",
    "    # Evaluate per cutoff window\n",
    "    all_results = []\n",
    "    \n",
    "    for cutoff in cv_df['cutoff'].unique():\n",
    "        cutoff_data = cv_df[cv_df['cutoff'] == cutoff]\n",
    "        \n",
    "        # Evaluate all metrics for this cutoff\n",
    "        cutoff_eval = evaluate(cutoff_data, metrics=[mae, mse, rmse], models=model_cols)\n",
    "        cutoff_eval['cutoff'] = cutoff\n",
    "        \n",
    "        # Add best_model column (lowest value per row, regardless of metric)\n",
    "        cutoff_eval['best_model'] = cutoff_eval[model_cols].idxmin(axis=1)\n",
    "        cutoff_eval['best_value'] = cutoff_eval[model_cols].min(axis=1)\n",
    "        \n",
    "        all_results.append(cutoff_eval)\n",
    "        \n",
    "        # Print MAE summary for this cutoff\n",
    "        cutoff_mae = cutoff_eval[cutoff_eval['metric'] == 'mae']\n",
    "        print(f\"\\n   Cutoff {cutoff.strftime('%Y-%m-%d')} (MAE best models):\")\n",
    "        print(f\"   {cutoff_mae['best_model'].value_counts().to_dict()}\")\n",
    "\n",
    "    # Combine all cutoff results into single dataframe\n",
    "    eval_df = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # Print overall summary\n",
    "    mae_overall = eval_df[eval_df['metric'] == 'mae']\n",
    "    print(f\"\\nðŸ“ˆ Overall Best Models (all cutoffs - based on MAE):\")\n",
    "    print(mae_overall['best_model'].value_counts())\n",
    "    \n",
    "    return eval_df, cv_df\n",
    "\n",
    "def get_best_model_forecast(forecasts_df,evaluation_df_summary):\n",
    "\n",
    "    if 'unique_id' not in forecasts_df.columns:\n",
    "        forecasts_df = forecasts_df.reset_index()\n",
    "    \n",
    "    # Get best model info\n",
    "    best_info = evaluation_df_summary[['unique_id', 'best_model']].drop_duplicates()\n",
    "    \n",
    "    result_rows = []\n",
    "    \n",
    "    for uid in forecasts_df['unique_id'].unique():\n",
    "        # Get forecasts for this unique_id\n",
    "        uid_forecasts = forecasts_df[forecasts_df['unique_id'] == uid].copy()\n",
    "        best_model_row = best_info[best_info['unique_id'] == uid]\n",
    "        \n",
    "        if len(best_model_row) == 0:\n",
    "            print(f\"Warning: No best model found for {uid}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        best_model = best_model_row['best_model'].values[0]\n",
    "        \n",
    "        # For each forecast period\n",
    "        for _, row in uid_forecasts.iterrows():\n",
    "            result_row = {\n",
    "                'unique_id': uid,\n",
    "                'ds': row['ds'],\n",
    "                'best_model': best_model,\n",
    "                'best_forecast': row.get(best_model, np.nan)\n",
    "            }\n",
    "            \n",
    "            for level in [95]:\n",
    "                lo_col_source = f'{best_model}-lo-{level}'\n",
    "                hi_col_source = f'{best_model}-hi-{level}'\n",
    "                \n",
    "                result_row[f'best_forecast-lo-{level}'] = row.get(lo_col_source, np.nan)\n",
    "                result_row[f'best_forecast-hi-{level}'] = row.get(hi_col_source, np.nan)\n",
    "            \n",
    "            result_rows.append(result_row)\n",
    "    \n",
    "    result = pd.DataFrame(result_rows)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def add_best_model_column(eval_traintest_df):\n",
    "\n",
    "    eval_with_best = eval_traintest_df.copy()\n",
    "    model_cols=[col for col in eval_with_best.columns if col not in ['unique_id', 'metric']]\n",
    "    eval_with_best['best_model'] = eval_with_best[model_cols].idxmin(axis=1)\n",
    "\n",
    "    return eval_with_best\n",
    "\n",
    "def save_results_to_excel(forecasts_df, best_forecasts_df, evaluation_cv_df, evaluation_traintest_df, target_name, save_path=None):\n",
    "    \n",
    "    if save_path is None or save_path == \"\":\n",
    "        save_path = os.getcwd()\n",
    "    \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    prefix = target_name.lower().replace(' ', '_')\n",
    "    excel_filename=os.path.join(save_path, f'{prefix}_forecast_results.xlsx')\n",
    "\n",
    "    eval_traintest_with_best = add_best_model_column(evaluation_traintest_df)\n",
    "    \n",
    "    with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "\n",
    "        eval_traintest_with_best.to_excel(writer, sheet_name='Evaluation_TrainTest', index=False)\n",
    "        forecasts_df.reset_index().to_excel(writer, sheet_name='Forecast_models', index=False)\n",
    "        best_forecasts_df.to_excel(writer, sheet_name='Best_model_forecasts', index=False)\n",
    "        evaluation_cv_df.to_excel(writer, sheet_name='Cross-Validation', index=False)\n",
    "\n",
    "    print(f\"âœ“ Results saved to: {excel_filename}\")\n",
    "    \n",
    "def plot_sample_forecasts(df, forecasts_df, target_name, config):\n",
    " \n",
    "    unique_ids = df['unique_id'].unique()\n",
    "    sample_ids = np.random.choice(unique_ids, min(config.n_samples, len(unique_ids)), replace=False)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    forecast_cols = forecasts_df.columns.tolist()\n",
    "    available_models = [col for col in forecast_cols if col not in ['unique_id', 'ds'] and not col.endswith(('-lo-80', '-hi-80', '-lo-95', '-hi-95'))]\n",
    "    \n",
    "    # If models_to_plot is specified, filter to only those models\n",
    "    if config.models_to_plot is not None:  \n",
    "        available_models = [m for m in available_models if m in config.models_to_plot]\n",
    "    \n",
    "    palette = plt.cm.get_cmap('viridis', len(available_models))\n",
    "    model_styles = {\n",
    "        model: {'color': palette(idx), 'marker': '.', 'linestyle': '-'}\n",
    "        for idx, model in enumerate(available_models)\n",
    "    }\n",
    "    \n",
    "    for idx, uid in enumerate(sample_ids):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Historical data\n",
    "        hist_data = df[df['unique_id'] == uid]\n",
    "        ax.plot(hist_data['ds'], hist_data['y'], 'o-', \n",
    "                label='Historical', linewidth=1.5, color='grey', markersize=3)\n",
    "        \n",
    "        # Forecast data\n",
    "        forecast_data = forecasts_df.reset_index()\n",
    "        forecast_data = forecast_data[forecast_data['unique_id'] == uid]\n",
    "        \n",
    "        # Plot each available model\n",
    "        for model in available_models:\n",
    "            if model in forecast_data.columns:\n",
    "                style = model_styles.get(model, {'color': 'gray', 'marker': 'o', 'linestyle': '-'})\n",
    "                ax.plot(forecast_data['ds'], forecast_data[model], \n",
    "                       marker=style['marker'], \n",
    "                       linestyle=style['linestyle'],\n",
    "                       color=style['color'],\n",
    "                       label=model, \n",
    "                       linewidth=1.5,\n",
    "                       markersize=3)\n",
    "        \n",
    "        ax.set_title(f'{uid}', fontsize=10, fontweight='bold')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel(target_name)\n",
    "        ax.legend(loc='best', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'{target_name.lower().replace(\" \", \"_\")}_sample_forecasts.png'\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_train_test_forecasts(df, train_df, test_df, preds_df, target_name, config, ids_to_plot=None):\n",
    "\n",
    "    \n",
    "    if ids_to_plot is None:\n",
    "        unique_ids = df['unique_id'].unique()\n",
    "        ids_to_plot = np.random.choice(\n",
    "            unique_ids, \n",
    "            min(config.n_samples, len(unique_ids)),  # â† Use config.n_samples\n",
    "            replace=False\n",
    "        )\n",
    "    \n",
    "    n_plots = len(ids_to_plot); n_cols = 2; n_rows = (n_plots + 1) // 2\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 5 * n_rows))\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    # Get available models from preds_df\n",
    "    all_model_cols = [col for col in preds_df.columns \n",
    "                      if col not in ['unique_id', 'ds', 'y', 'cutoff']]\n",
    "    \n",
    "    # Print available models for debugging\n",
    "    print(f\"\\nðŸ” Available models in predictions: {all_model_cols}\")\n",
    "    \n",
    "    # Filter to selected models if specified\n",
    "    if config.models_to_plot is not None:  # â† Use config.models_to_plot\n",
    "        # Try exact match first\n",
    "        model_cols = [m for m in all_model_cols if m in config.models_to_plot]\n",
    "        \n",
    "        # If no exact matches, try case-insensitive partial matching\n",
    "        if len(model_cols) == 0:\n",
    "            model_cols = []\n",
    "            for requested in config.models_to_plot:  # â† Use config.models_to_plot\n",
    "                for available in all_model_cols:\n",
    "                    if requested.lower() in available.lower() or available.lower() in requested.lower():\n",
    "                        model_cols.append(available)\n",
    "            model_cols = list(set(model_cols))  # Remove duplicates\n",
    "        \n",
    "        if len(model_cols) == 0:\n",
    "            print(f\"âš ï¸  Warning: None of the requested models {config.models_to_plot} found.\")\n",
    "            print(f\"   Available models: {all_model_cols}\")\n",
    "            print(f\"   Plotting all available models instead.\")\n",
    "            model_cols = all_model_cols\n",
    "        else:\n",
    "            print(f\"âœ“ Plotting models: {model_cols}\")\n",
    "    else:\n",
    "        model_cols = all_model_cols\n",
    "        print(f\"âœ“ Plotting all {len(model_cols)} models\")\n",
    "    \n",
    "    for idx, uid in enumerate(ids_to_plot):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Get data for this unique_id\n",
    "        hist_data = df[df['unique_id'] == uid].sort_values('ds')\n",
    "        train_data = train_df[train_df['unique_id'] == uid].sort_values('ds')\n",
    "        test_data = test_df[test_df['unique_id'] == uid].sort_values('ds')\n",
    "        pred_data = preds_df[preds_df['unique_id'] == uid].sort_values('ds')\n",
    "        \n",
    "        # Plot training data (historical)\n",
    "        ax.plot(train_data['ds'], train_data['y'], \n",
    "                'o-', color='Grey', linewidth=2, markersize=5, \n",
    "                label='Training Data', zorder=3)\n",
    "        \n",
    "        # Plot ACTUAL test data\n",
    "        ax.plot(test_data['ds'], test_data['y'], \n",
    "                'o-', color='darkgreen', linewidth=2.5, markersize=7, \n",
    "                label='Actual Test Values', zorder=5)\n",
    "\n",
    "            # Plot predictions for each selected model\n",
    "        palette = plt.cm.get_cmap('viridis', len(model_cols))\n",
    "        for model_idx, model in enumerate(model_cols):\n",
    "            if model in pred_data.columns:\n",
    "                ax.plot(pred_data['ds'], pred_data[model], \n",
    "                       's--', color=palette(model_idx), linewidth=2, \n",
    "                       markersize=6, label=f'{model}', \n",
    "                       alpha=0.8, zorder=4)\n",
    "        \n",
    "        # Add vertical line to separate train/test\n",
    "        if len(train_data) > 0:\n",
    "            train_end = train_data['ds'].max()\n",
    "            ax.axvline(x=train_end, color='red', linestyle=':', linewidth=2.5, \n",
    "                      label='Train/Test Split', alpha=0.7, zorder=2)\n",
    "        \n",
    "        # Add shaded region for test period\n",
    "        if len(test_data) > 0:\n",
    "            test_start = test_data['ds'].min()\n",
    "            test_end = test_data['ds'].max()\n",
    "            ax.axvspan(test_start, test_end, alpha=0.1, color='yellow', \n",
    "                      label='Test Period', zorder=1)\n",
    "        \n",
    "        ax.set_title(f'{uid}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Date', fontsize=10)\n",
    "        ax.set_ylabel(target_name, fontsize=10)\n",
    "        ax.legend(loc='best', fontsize=8, framealpha=0.9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hide extra subplots if n_plots is odd\n",
    "    for idx in range(n_plots, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'{target_name.lower().replace(\" \", \"_\")}_train_test_forecasts.png'\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a8e9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_analysis(df, target_name, param_grid=None, save_path=None):\n",
    "    \n",
    "    # Default parameter grid if not provided\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'h': [4, 8],\n",
    "            'test_size': [4, 8],\n",
    "            'train_size': [None, 20, 28],  # None = use all available\n",
    "            'n_windows': [2, 3, 4],\n",
    "            'step_size': [None, 2, 4],  # None = defaults to h\n",
    "        }\n",
    "    \n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    combinations = list(product(*param_values))\n",
    "    \n",
    "    print(f\"   Total configurations to test: {len(combinations)}\")\n",
    "    print(f\"   Parameters: {param_names}\")\n",
    "    \n",
    "    all_results = []\n",
    "    detailed_results = []\n",
    "    \n",
    "    for i, combo in enumerate(combinations):\n",
    "        params = dict(zip(param_names, combo))\n",
    "        \n",
    "        if params.get('test_size') and params.get('h'):\n",
    "            if params['test_size'] < params['h']:\n",
    "                continue\n",
    "        \n",
    "        # Check if we have enough data for this configuration\n",
    "        min_data_needed = (params.get('train_size') or 0) + (params.get('test_size') or params['h'])\n",
    "        min_series_length = df.groupby('unique_id').size().min()\n",
    "        \n",
    "        if params.get('train_size') and params.get('n_windows'):\n",
    "            cv_data_needed = params['train_size'] + params['h'] * params['n_windows']\n",
    "            if cv_data_needed > min_series_length:\n",
    "                print(f\"   âš ï¸  Skipping config {i+1}: insufficient data (need {cv_data_needed}, have {min_series_length})\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Configuration {i+1}/{len(combinations)}: {params}\")\n",
    "        try:\n",
    "            # Create config object\n",
    "            config = ForecastConfig(\n",
    "                h=params.get('h', 8),\n",
    "                train_size=params.get('train_size'),\n",
    "                test_size=params.get('test_size'),\n",
    "                n_windows=params.get('n_windows', 2),\n",
    "                step_size=params.get('step_size'),\n",
    "                season_length=4,\n",
    "                n_samples=4,\n",
    "                confidence_level=95,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Run train-test evaluation\n",
    "            eval_traintest, preds_traintest, train_df, test_df = evaluate_train_test(\n",
    "                df, target_name, config\n",
    "            )\n",
    "            \n",
    "            # Run cross-validation\n",
    "            eval_cv, cv_df = evaluate_model_cross(df, target_name, config)\n",
    "            \n",
    "            # Extract MAE results\n",
    "            mae_cv = eval_cv[eval_cv['metric'] == 'mae'].copy()\n",
    "            mae_traintest = eval_traintest[eval_traintest['metric'] == 'mae'].copy()\n",
    "            \n",
    "            # Get model columns\n",
    "            exclude_cols = ['unique_id', 'ds', 'y', 'cutoff', 'metric', 'best_model', 'best_value']\n",
    "            model_cols = [col for col in mae_cv.columns if col not in exclude_cols]\n",
    "            \n",
    "            # Add best_model to train-test if not present\n",
    "            if 'best_model' not in mae_traintest.columns:\n",
    "                mae_traintest['best_model'] = mae_traintest[model_cols].idxmin(axis=1)\n",
    "            \n",
    "            # Summarize results for this configuration\n",
    "            for uid in df['unique_id'].unique():\n",
    "                # CV results\n",
    "                uid_cv = mae_cv[mae_cv['unique_id'] == uid]\n",
    "                if len(uid_cv) > 0:\n",
    "                    # Get most frequent best model across cutoffs\n",
    "                    cv_best_model = uid_cv['best_model'].mode().iloc[0] if len(uid_cv) > 0 else None\n",
    "                    cv_best_count = (uid_cv['best_model'] == cv_best_model).sum()\n",
    "                    cv_total = len(uid_cv)\n",
    "                    cv_consistency = cv_best_count / cv_total if cv_total > 0 else 0\n",
    "                else:\n",
    "                    cv_best_model = None\n",
    "                    cv_consistency = 0\n",
    "                \n",
    "                # Train-test results\n",
    "                uid_traintest = mae_traintest[mae_traintest['unique_id'] == uid]\n",
    "                traintest_best_model = uid_traintest['best_model'].iloc[0] if len(uid_traintest) > 0 else None\n",
    "                \n",
    "                result_row = {\n",
    "                    'unique_id': uid,\n",
    "                    'config_id': i + 1,\n",
    "                    **params,\n",
    "                    'cv_best_model': cv_best_model,\n",
    "                    'cv_consistency': cv_consistency,  # How often this model won across cutoffs\n",
    "                    'traintest_best_model': traintest_best_model,\n",
    "                    'cv_traintest_agree': cv_best_model == traintest_best_model,\n",
    "                }\n",
    "                \n",
    "                # Add MAE values for each model (from CV)\n",
    "                for model in model_cols:\n",
    "                    model_mae = uid_cv[model].mean() if len(uid_cv) > 0 else None\n",
    "                    result_row[f'{model}_mae'] = model_mae\n",
    "                \n",
    "                all_results.append(result_row)\n",
    "            \n",
    "            # Store detailed results\n",
    "            detailed_results.append({\n",
    "                'config_id': i + 1,\n",
    "                'params': params,\n",
    "                'eval_cv': eval_cv,\n",
    "                'eval_traintest': eval_traintest,\n",
    "                'cv_df': cv_df\n",
    "            })\n",
    "            \n",
    "            print(f\"   âœ“ Completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 70); print(\"ðŸ“ˆ SENSITIVITY ANALYSIS SUMMARY\"); print(\"=\" * 70)\n",
    "    \n",
    "    if len(results_df) > 0:\n",
    "        # Overall best model frequency\n",
    "        print(\"\\nðŸ† Best Model Frequency (Cross-Validation):\")\n",
    "        print(results_df['cv_best_model'].value_counts())\n",
    "        \n",
    "        print(\"\\nðŸ† Best Model Frequency (Train-Test):\")\n",
    "        print(results_df['traintest_best_model'].value_counts())\n",
    "        \n",
    "        # Agreement between CV and train-test\n",
    "        agreement_rate = results_df['cv_traintest_agree'].mean() * 100\n",
    "        print(f\"\\nðŸ¤ CV vs Train-Test Agreement: {agreement_rate:.1f}%\")\n",
    "        \n",
    "        # Consistency per unique_id\n",
    "        print(\"\\nðŸ“Š Model Selection Consistency by Series:\")\n",
    "        consistency_summary = results_df.groupby('unique_id').agg({\n",
    "            'cv_best_model': lambda x: x.mode().iloc[0] if len(x) > 0 else None,\n",
    "            'cv_consistency': 'mean',\n",
    "            'cv_traintest_agree': 'mean'\n",
    "        }).round(3)\n",
    "        print(consistency_summary)\n",
    "    \n",
    "    # Save results if path provided\n",
    "    if save_path:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        filename = os.path.join(save_path, f'{target_name.lower().replace(\" \", \"_\")}_sensitivity_analysis.xlsx')\n",
    "        \n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            results_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "            \n",
    "            # Pivot table: best model by config and unique_id\n",
    "            if len(results_df) > 0:\n",
    "                pivot_cv = results_df.pivot_table(\n",
    "                    index='unique_id', \n",
    "                    columns='config_id', \n",
    "                    values='cv_best_model', \n",
    "                    aggfunc='first'\n",
    "                )\n",
    "                pivot_cv.to_excel(writer, sheet_name='CV_BestModel_Pivot')\n",
    "                \n",
    "                pivot_traintest = results_df.pivot_table(\n",
    "                    index='unique_id', \n",
    "                    columns='config_id', \n",
    "                    values='traintest_best_model', \n",
    "                    aggfunc='first'\n",
    "                )\n",
    "                pivot_traintest.to_excel(writer, sheet_name='TrainTest_BestModel_Pivot')\n",
    "        \n",
    "        print(f\"\\nâœ“ Results saved to: {filename}\")\n",
    "    \n",
    "    return results_df, detailed_results\n",
    "\n",
    "def analyze_models(results_df, target_name,save_path=None):\n",
    "    \n",
    "    unique_ids = results_df['unique_id'].unique()\n",
    "    params_to_analyze = ['h', 'test_size', 'train_size', 'n_windows', 'step_size']\n",
    "    params_available = [p for p in params_to_analyze if p in results_df.columns]\n",
    "    \n",
    "    mae_cols = [col for col in results_df.columns if col.endswith('_mae')]\n",
    "    model_names = [col.replace('_mae', '') for col in mae_cols]\n",
    "    \n",
    "    #This is for both recommendations and parameter impacts \n",
    "    all_recommendations = []; all_parameter_impacts = []\n",
    "    \n",
    "    for uid in unique_ids:\n",
    "        uid_data = results_df[results_df['unique_id'] == uid].copy()\n",
    "        print(f\"\\nðŸ” Analyzing Unique ID: {uid}\")\n",
    "        #  Most frequent CV best model\n",
    "        cv_mode = uid_data['cv_best_model'].mode()\n",
    "        cv_best = cv_mode.iloc[0] if len(cv_mode) > 0 else None\n",
    "        cv_freq = (uid_data['cv_best_model'] == cv_best).sum() / len(uid_data)\n",
    "        \n",
    "        # Most frequent train-test best model\n",
    "        tt_mode = uid_data['traintest_best_model'].mode()\n",
    "        tt_best = tt_mode.iloc[0] if len(tt_mode) > 0 else None\n",
    "        tt_freq = (uid_data['traintest_best_model'] == tt_best).sum() / len(uid_data)\n",
    "        \n",
    "        # Overall agreement rate\n",
    "        agreement_rate = uid_data['cv_traintest_agree'].mean()\n",
    "        \n",
    "        # Average consistency across CV windows\n",
    "        avg_consistency = uid_data['cv_consistency'].mean()\n",
    "        \n",
    "        # Determine recommendation and confidence\n",
    "        if cv_best == tt_best and cv_freq >= 0.7 and avg_consistency >= 0.7:\n",
    "            confidence = 'High'\n",
    "            recommended_model = cv_best\n",
    "            reason = \"CV and Train-Test agree, high frequency and consistency\"\n",
    "        elif cv_best == tt_best and cv_freq >= 0.5:\n",
    "            confidence = 'Medium-High'\n",
    "            recommended_model = cv_best\n",
    "            reason = \"CV and Train-Test agree with moderate frequency\"\n",
    "        elif cv_freq >= 0.6:\n",
    "            confidence = 'Medium'\n",
    "            recommended_model = cv_best\n",
    "            reason = f\"CV favors {cv_best} ({cv_freq:.0%}), but Train-Test prefers {tt_best}\"\n",
    "        elif tt_freq >= 0.6:\n",
    "            confidence = 'Medium'\n",
    "            recommended_model = tt_best\n",
    "            reason = f\"Train-Test favors {tt_best} ({tt_freq:.0%}), but CV prefers {cv_best}\"\n",
    "        else:\n",
    "            confidence = 'Low'\n",
    "            # Use model with lowest average MAE\n",
    "            if mae_cols:\n",
    "                avg_maes = uid_data[mae_cols].mean()\n",
    "                recommended_model = avg_maes.idxmin().replace('_mae', '')\n",
    "            else:\n",
    "                recommended_model = cv_best\n",
    "            reason = \"No clear winner - using lowest average MAE\"\n",
    "        \n",
    "        # Get average MAE for recommended model\n",
    "        rec_mae_col = f'{recommended_model}_mae'\n",
    "        avg_mae = uid_data[rec_mae_col].mean() if rec_mae_col in uid_data.columns else None\n",
    "        \n",
    "        # Store recommendation\n",
    "        recommendation = {\n",
    "            'unique_id': uid,\n",
    "            'recommended_model': recommended_model,\n",
    "            'confidence': confidence,\n",
    "            'reason': reason,\n",
    "            'cv_best_model': cv_best,\n",
    "            'cv_frequency': cv_freq,\n",
    "            'cv_consistency': avg_consistency,\n",
    "            'traintest_best_model': tt_best,\n",
    "            'traintest_frequency': tt_freq,\n",
    "            'cv_traintest_agreement': agreement_rate,\n",
    "            'recommended_model_avg_mae': avg_mae\n",
    "        }\n",
    "        all_recommendations.append(recommendation)\n",
    "        \n",
    "        print(f\"\\nRECOMMENDATION: {recommended_model}\"); print(f\"   Confidence: {confidence}\")\n",
    "        print(f\"   Reason: {reason}\"); print(f\"   CV Best: {cv_best} ({cv_freq:.0%} of configs)\")\n",
    "        print(f\"   Train-Test Best: {tt_best} ({tt_freq:.0%} of configs)\"); print(f\"   CV-TT Agreement: {agreement_rate:.0%}\")\n",
    "        if avg_mae:\n",
    "            print(f\"   Avg MAE: {avg_mae:,.0f}\")\n",
    "\n",
    "        print(f\"\\nPARAMETER IMPACT:\")\n",
    "        \n",
    "        for param in params_available:\n",
    "            param_values = uid_data[param].dropna().unique()\n",
    "            \n",
    "            if len(param_values) <= 1:\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n   {param}:\")\n",
    "            \n",
    "            for val in sorted(param_values):\n",
    "                val_data = uid_data[uid_data[param] == val]\n",
    "                if len(val_data) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Best model for this parameter value\n",
    "                val_cv_best = val_data['cv_best_model'].mode()\n",
    "                val_cv_best = val_cv_best.iloc[0] if len(val_cv_best) > 0 else None\n",
    "                val_cv_freq = (val_data['cv_best_model'] == val_cv_best).sum() / len(val_data)\n",
    "                \n",
    "                # Average MAE for best model at this parameter value\n",
    "                if val_cv_best and f'{val_cv_best}_mae' in val_data.columns:\n",
    "                    val_avg_mae = val_data[f'{val_cv_best}_mae'].mean()\n",
    "                    mae_str = f\", Avg MAE: {val_avg_mae:,.0f}\"\n",
    "                else:\n",
    "                    val_avg_mae = None\n",
    "                    mae_str = \"\"\n",
    "                \n",
    "                print(f\"      {param}={val}: {val_cv_best} ({val_cv_freq:.0%}){mae_str}\")\n",
    "                \n",
    "                # Store parameter impact\n",
    "                all_parameter_impacts.append({\n",
    "                    'unique_id': uid,\n",
    "                    'parameter': param,\n",
    "                    'value': val,\n",
    "                    'best_model': val_cv_best,\n",
    "                    'frequency': val_cv_freq,\n",
    "                    'avg_mae': val_avg_mae,\n",
    "                    'n_configs': len(val_data)\n",
    "                })\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        \n",
    "        # Plot 1: CV Best Model Frequency\n",
    "        ax1 = axes[0, 0]\n",
    "        cv_counts = uid_data['cv_best_model'].value_counts()\n",
    "        colors = ['green' if m == recommended_model else 'steelblue' for m in cv_counts.index]\n",
    "        cv_counts.plot(kind='bar', ax=ax1, color=colors, edgecolor='black')\n",
    "        ax1.set_title('CV Best Model Frequency', fontweight='bold')\n",
    "        ax1.set_xlabel('Model')\n",
    "        ax1.set_ylabel('Count')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        for i, (idx, val) in enumerate(cv_counts.items()):\n",
    "            ax1.annotate(f'{val/len(uid_data):.0%}', xy=(i, val), ha='center', va='bottom')\n",
    "        \n",
    "        # Plot 2: Train-Test Best Model Frequency\n",
    "        ax2 = axes[0, 1]\n",
    "        tt_counts = uid_data['traintest_best_model'].value_counts()\n",
    "        colors = ['green' if m == recommended_model else 'darkorange' for m in tt_counts.index]\n",
    "        tt_counts.plot(kind='bar', ax=ax2, color=colors, edgecolor='black')\n",
    "        ax2.set_title('Train-Test Best Model Frequency', fontweight='bold')\n",
    "        ax2.set_xlabel('Model')\n",
    "        ax2.set_ylabel('Count')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        for i, (idx, val) in enumerate(tt_counts.items()):\n",
    "            ax2.annotate(f'{val/len(uid_data):.0%}', xy=(i, val), ha='center', va='bottom')\n",
    "        \n",
    "        # Plot 3: Average MAE by Model\n",
    "        ax3 = axes[0, 2]\n",
    "        if mae_cols:\n",
    "            avg_maes = uid_data[mae_cols].mean().sort_values()\n",
    "            colors = ['green' if m.replace('_mae', '') == recommended_model else 'gray' \n",
    "                     for m in avg_maes.index]\n",
    "            avg_maes.plot(kind='barh', ax=ax3, color=colors, edgecolor='black')\n",
    "            ax3.set_title('Average MAE by Model', fontweight='bold')\n",
    "            ax3.set_xlabel('MAE')\n",
    "            ax3.set_yticklabels([l.get_text().replace('_mae', '') for l in ax3.get_yticklabels()])\n",
    "            for i, val in enumerate(avg_maes):\n",
    "                ax3.annotate(f'{val:,.0f}', xy=(val, i), ha='left', va='center', fontsize=9)\n",
    "        \n",
    "        # Plot 4-6: Parameter Impact (up to 3 parameters)\n",
    "        for idx, param in enumerate(params_available[:3]):\n",
    "            ax = axes[1, idx]\n",
    "            param_pivot = uid_data.groupby([param, 'cv_best_model']).size().unstack(fill_value=0)\n",
    "            param_pivot.plot(kind='bar', ax=ax, edgecolor='black', width=0.8)\n",
    "            ax.set_title(f'Best Model by {param}', fontweight='bold')\n",
    "            ax.set_xlabel(param)\n",
    "            ax.set_ylabel('Count')\n",
    "            ax.legend(title='Model', fontsize=8)\n",
    "            ax.tick_params(axis='x', rotation=0)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for idx in range(len(params_available[:3]), 3):\n",
    "            axes[1, idx].set_visible(False)\n",
    "        \n",
    "        plt.suptitle(f'Model Analysis: {target_name} - {uid}\\n'\n",
    "                    f'Recommended: {recommended_model} (Confidence: {confidence})', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            filename = os.path.join(save_path, f'{target_name.lower().replace(\" \", \"_\")}_analysis_{uid}.png')\n",
    "            plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "            print(f\"\\n   âœ“ Saved: {filename}\")\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    recommendations_df = pd.DataFrame(all_recommendations)\n",
    "    parameter_impact_df = pd.DataFrame(all_parameter_impacts)\n",
    "    \n",
    "    print(\"\\nFINAL RECOMMENDATIONS SUMMARY\")\n",
    "    \n",
    "    summary_cols = ['unique_id', 'recommended_model', 'confidence', 'cv_frequency', 'traintest_frequency', 'cv_traintest_agreement']\n",
    "    print(recommendations_df[summary_cols].to_string(index=False))\n",
    "    \n",
    "    # Confidence distribution\n",
    "    print(f\"\\nConfidence Distribution:\")\n",
    "    print(recommendations_df['confidence'].value_counts().to_string())\n",
    "    \n",
    "    # Model recommendation distribution\n",
    "    print(f\"\\nRecommended Models Distribution:\")\n",
    "    print(recommendations_df['recommended_model'].value_counts().to_string())\n",
    "    \n",
    "    # Parameter sensitivity summary\n",
    "    print(f\"\\nParameter Sensitivity Summary:\")\n",
    "    param_sensitivity = parameter_impact_df.groupby(['unique_id', 'parameter']).apply(\n",
    "        lambda x: x['best_model'].nunique()\n",
    "    ).unstack(fill_value=0)\n",
    "    print(\"   (Number of different best models across parameter values)\")\n",
    "    print(param_sensitivity.to_string())\n",
    "    \n",
    "    # Save results to Excel\n",
    "    if save_path:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        filename = os.path.join(save_path, f'{target_name.lower().replace(\" \", \"_\")}_recommendations.xlsx')\n",
    "        \n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            recommendations_df.to_excel(writer, sheet_name='Recommendations', index=False)\n",
    "            parameter_impact_df.to_excel(writer, sheet_name='Parameter_Impact', index=False)\n",
    "            param_sensitivity.to_excel(writer, sheet_name='Param_Sensitivity')\n",
    "        \n",
    "        print(f\"\\nâœ“ Results saved to: {filename}\")\n",
    "    \n",
    "    return recommendations_df, parameter_impact_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef96e8",
   "metadata": {},
   "source": [
    "### Population Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a1f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath, states=None):\n",
    "    \n",
    "    df=pd.read_csv(filepath)\n",
    "    #Filtering for certain years\n",
    "    df = df[(df['Period'] >= '2017Q1') & (df['Period'] <= '2024Q4')].copy()\n",
    "    if states is not None:\n",
    "        if isinstance(states, str):\n",
    "            states = [states]\n",
    "        print(f\"Filtering data for states: {states}\")\n",
    "        df = df[df['State'].isin(states)].copy()\n",
    "        print(f\"Filtered to {len(df)} rows across {df['State'].unique()} state(s)\")\n",
    "        if len(df) == 0:\n",
    "            raise ValueError(\"No data available after filtering by states.\")    \n",
    "    \n",
    "    df['unique_id']=df['State']\n",
    "    df['ds']=pd.to_datetime(df['Period'])\n",
    "    df=df.sort_values(['unique_id','ds']).reset_index(drop=True)\n",
    "    df_pop=df[['unique_id','ds','Population']].copy()\n",
    "    df_pop.columns = ['unique_id', 'ds', 'y']\n",
    "    \n",
    "\n",
    "    return df_pop,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d336b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pop(filepath, states, config, save_path=None, run_cv=True):\n",
    "\n",
    "    print(\"\\n1. Loading and preparing pop data\")\n",
    "    df_pop, df_original = load_data(filepath, states=states)\n",
    "\n",
    "    print(f\"\\nTotal states: {df_pop['unique_id'].nunique()}\")\n",
    "    min_date = df_pop['ds'].min(); max_date = df_pop['ds'].max()\n",
    "    print(f\"Date range: {min_date.year}-Q{min_date.quarter} to {max_date.year}-Q{max_date.quarter}\")\n",
    "    print(f\"States included: {states}\")\n",
    "\n",
    "    print(\"\\n2. Training models and generating forecasts for Population\")\n",
    "    forecasts_pop, sf_pop = train_and_forecast(df_pop, \"Population\", config)\n",
    "\n",
    "    print(\"\\n3. Evaluating models on training/test split for Population\")\n",
    "    eval_traintest_pop, preds_traintest_pop, train_pop, test_pop = evaluate_train_test(\n",
    "        df_pop, \"Population\", config\n",
    "    )\n",
    "\n",
    "    if run_cv:\n",
    "        \n",
    "        print(\"\\n4. Running cross-validation for Population...\")\n",
    "        eval_cv_pop, cv_pop = evaluate_model_cross(\n",
    "            df_pop, \"Population\", config)\n",
    "        mae_summary_pop = eval_cv_pop[eval_cv_pop['metric'] == 'mae']\n",
    "        best_forecasts_pop = get_best_model_forecast(forecasts_pop, mae_summary_pop)\n",
    "        \n",
    "        print(\"\\n5. Selecting best model forecasts for Population...\")\n",
    "        best_forecasts_pop = get_best_model_forecast(forecasts_pop, mae_summary_pop)\n",
    "\n",
    "        print(\"\\n6. Saving results to Excel for Population...\")\n",
    "        save_results_to_excel(\n",
    "            forecasts_pop, \n",
    "            best_forecasts_pop, \n",
    "            eval_cv_pop, \n",
    "            eval_traintest_pop, \n",
    "            \"Population\", \n",
    "            save_path=save_path\n",
    "        )\n",
    "    else:\n",
    "        eval_cv_pop = None\n",
    "        best_forecasts_pop = forecasts_pop.reset_index()\n",
    "        print(\"Skipping cross-validation\")\n",
    "    \n",
    "    # Plot forecasts\n",
    "    plot_sample_forecasts(df_pop, forecasts_pop, \"Population\", config)\n",
    "    \n",
    "    # Plot train/test comparison\n",
    "    print(\"\\nðŸ“ˆ Generating train/test visualization...\")\n",
    "    plot_train_test_forecasts(\n",
    "        df=df_pop,\n",
    "        train_df=train_pop,\n",
    "        test_df=test_pop,\n",
    "        preds_df=preds_traintest_pop,\n",
    "        target_name=\"Population\",\n",
    "        config=config,\n",
    "        ids_to_plot=states\n",
    "    )\n",
    "    \n",
    "    print(\"\\n7. Compiling all results...\"); print(\"âœ“ Done!\")\n",
    "    return {\n",
    "        'population': {\n",
    "            'forecasts': forecasts_pop,\n",
    "            'best_forecasts': best_forecasts_pop if run_cv else forecasts_pop.reset_index(),\n",
    "            'evaluation_cv': eval_cv_pop if run_cv else None,\n",
    "            'evaluation_traintest': eval_traintest_pop,\n",
    "            'predictions_traintest': preds_traintest_pop\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_csv_path=rf\"C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\merged_data\\Prebuilt_panels\\medi_pop.csv\"\n",
    "pop_save_path=rf\"C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\Forecast\\Pop\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae66b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See first the class because some parameters are already fixed\n",
    "\n",
    "config = ForecastConfig(\n",
    "    h=8,                    \n",
    "    season_length=4,        \n",
    "    n_windows=2, \n",
    "    train_size=None,           \n",
    "    test_size=8,            \n",
    "    n_samples=4,            \n",
    "    confidence_level=95,    \n",
    "    models_to_plot=['Naive', 'ARIMA_manual', 'SARIMA']\n",
    ")\n",
    "\n",
    "results_pop = main_pop(\n",
    "    filepath=pop_csv_path,\n",
    "    states=['IN', 'MI', 'IL', 'OH'],\n",
    "    config=config,              \n",
    "    save_path=pop_save_path,\n",
    "    run_cv=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435320b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop, df_original = load_data(filepath=pop_csv_path, states=['IL', 'IN', 'MI', 'OH'])\n",
    "\n",
    "param_grid = {\n",
    "    'h': [4, 8],\n",
    "    'test_size': [4, 8],\n",
    "    'train_size': [20, 28, None],\n",
    "    'n_windows': [2, 3],\n",
    "    'step_size': [None, 4],\n",
    "}\n",
    "\n",
    "# Run sensitivity analysis\n",
    "results_df, detailed_results = sensitivity_analysis(\n",
    "    df=df_pop,\n",
    "    target_name=\"Population\",\n",
    "    param_grid=param_grid,\n",
    "    save_path=pop_save_path\n",
    ")\n",
    "\n",
    "# Run combined analysis and get recommendations\n",
    "recommendations_df, parameter_impact_df = analyze_models(\n",
    "    results_df=results_df,\n",
    "    target_name=\"Population\",\n",
    "    save_path=pop_save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93281f08",
   "metadata": {},
   "source": [
    "### Statistical Models - Dataset with pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b5d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(filepath, states=None):\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    #Cutting the dataframe fo selected years\n",
    "    df=df[(df['Period'] >= '2017Q1') & (df['Period'] <= '2024Q4')].copy()\n",
    "    \n",
    "    if states is not None:\n",
    "        if isinstance(states, str):\n",
    "            states = [states]\n",
    "        print(f\"Filtering data for states: {states}\")\n",
    "        df=df[df['State'].isin(states)].copy()\n",
    "        print(f\"Filtered to {len(df)} rows across {df['State'].nunique()} state(s).\")\n",
    "\n",
    "        if len(df) == 0:\n",
    "            raise ValueError(\"No data available for the specified states.\")\n",
    "    \n",
    "    df['unique_id'] = df['State'] + '_' + df['ATC2 Class']\n",
    "    df['ds'] = pd.to_datetime(df['Period'])\n",
    "    df = df.sort_values(['unique_id','ds']).reset_index(drop=True)\n",
    "\n",
    "    #Preparing dataframes for statsforecast\n",
    "    df_units=df[['unique_id','ds','Units Reimbursed']].copy()\n",
    "    df_units.columns=['unique_id','ds','y']\n",
    "\n",
    "    df_prescriptions=df[['unique_id','ds','Number of Prescriptions']].copy()\n",
    "    df_prescriptions.columns=['unique_id','ds','y']\n",
    "\n",
    "    return df_units, df_prescriptions, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(filepath, states=None, config=None, run_cv=True, save_path=None):\n",
    "    \n",
    "    # Create default config if not provided\n",
    "    if config is None:\n",
    "        config = ForecastConfig()\n",
    "    \n",
    "    print(\"\\n1. Loading and preparing data...\")\n",
    "    df_units, df_prescriptions, df_original = load_and_prepare_data(filepath, states=states)\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"  â€¢ Total unique series: {df_units['unique_id'].nunique()}\")\n",
    "    min_date = df_units['ds'].min()\n",
    "    max_date = df_units['ds'].max()\n",
    "    print(f\"  â€¢ Date range: {min_date.year}-Q{min_date.quarter} to {max_date.year}-Q{max_date.quarter}\")\n",
    "    if states is not None:\n",
    "        print(f\"  â€¢ Filtered states: {', '.join(states)}\")\n",
    "    \n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"UNITS REIMBURSED\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n2. Training models and generating forecasts...\")\n",
    "    forecasts_units, sf_units = train_and_forecast(df_units, \"Units Reimbursed\", config)\n",
    "\n",
    "    print(\"\\n3. Evaluating models on training/test split...\")\n",
    "    eval_traintest_units, preds_traintest_units, train_units, test_units = evaluate_train_test(\n",
    "        df_units, \"Units Reimbursed\", config\n",
    "    )\n",
    "    \n",
    "    if run_cv:\n",
    "        print(\"\\n4. Running cross-validation...\")\n",
    "        eval_cv_units, cv_units, eval_summary_units, per_cutoff_units = evaluate_model_cross(\n",
    "            df_units, \"Units Reimbursed\", config\n",
    "        )\n",
    "        \n",
    "        print(\"\\n5. Selecting best forecasts...\")\n",
    "        best_forecasts_units = get_best_model_forecast(forecasts_units, eval_summary_units)\n",
    "        \n",
    "        print(\"\\n6. Saving results...\")\n",
    "        save_results_to_excel(\n",
    "            forecasts_units, best_forecasts_units, \n",
    "            eval_cv_units, eval_traintest_units, \n",
    "            \"Units Reimbursed\", save_path=save_path\n",
    "        )\n",
    "    else:\n",
    "        eval_cv_units = None\n",
    "        per_cutoff_units = None\n",
    "        best_forecasts_units = forecasts_units.reset_index()\n",
    "        print(\"\\n Skipping cross-validation (run_cv=False)\")\n",
    "    \n",
    "    # Plot sample forecasts\n",
    "    print(\"\\nGenerating forecast plots...\")\n",
    "    plot_sample_forecasts(df_units, forecasts_units, \"Units Reimbursed\", config)\n",
    "    plot_train_test_forecasts(df_units, train_units, test_units, preds_traintest_units, \"Units Reimbursed\", config)\n",
    "    \n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"NUMBER OF PRESCRIPTIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n2. Training models and generating forecasts...\")\n",
    "    forecasts_prescriptions, sf_prescriptions = train_and_forecast(\n",
    "        df_prescriptions, \"Number of Prescriptions\", config\n",
    "    )\n",
    "\n",
    "    print(\"\\n3. Evaluating models on training/test split...\")\n",
    "    eval_traintest_prescriptions, preds_traintest_prescriptions, train_prescriptions, test_prescriptions = evaluate_train_test(\n",
    "        df_prescriptions, \"Number of Prescriptions\", config\n",
    "    )\n",
    "    \n",
    "    if run_cv:\n",
    "        print(\"\\n4. Running cross-validation...\")\n",
    "        eval_cv_prescriptions, cv_prescriptions, eval_summary_prescriptions, per_cutoff_prescriptions = evaluate_model_cross(\n",
    "            df_prescriptions, \"Number of Prescriptions\", config\n",
    "        )\n",
    "        \n",
    "        print(\"\\n5. Selecting best forecasts...\")\n",
    "        best_forecasts_prescriptions = get_best_model_forecast(\n",
    "            forecasts_prescriptions, eval_summary_prescriptions\n",
    "        )\n",
    "        \n",
    "        print(\"\\n6. Saving results...\")\n",
    "        save_results_to_excel(\n",
    "            forecasts_prescriptions, best_forecasts_prescriptions, \n",
    "            eval_cv_prescriptions, eval_traintest_prescriptions, \n",
    "            \"Number of Prescriptions\", save_path=save_path\n",
    "        )\n",
    "    else:\n",
    "        eval_cv_prescriptions = None\n",
    "        per_cutoff_prescriptions = None\n",
    "        best_forecasts_prescriptions = forecasts_prescriptions.reset_index()\n",
    "        print(\"\\n Skipping cross-validation (run_cv=False)\")\n",
    "    \n",
    "    # Plot sample forecasts\n",
    "    print(\"\\nGenerating forecast plots...\")\n",
    "    plot_sample_forecasts(df_prescriptions, forecasts_prescriptions, \"Number of Prescriptions\", config)\n",
    "    plot_train_test_forecasts(df_prescriptions, train_prescriptions, test_prescriptions, preds_traintest_prescriptions, \"Number of Prescriptions\", config)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âœ… PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    state_info = f\"{', '.join(states)}\" if states else \"ALL\"\n",
    "    print(f\"\\nForecasts generated for state(s): {state_info}\")\n",
    "    print(f\"Forecast horizon: {config.h} quarters ahead\")\n",
    "    \n",
    "    if run_cv:\n",
    "        print(f\"\\nAll files generated!\")\n",
    "    \n",
    "    return {\n",
    "        'units': {\n",
    "            'forecasts': forecasts_units,\n",
    "            'best_forecasts': best_forecasts_units if run_cv else None,\n",
    "            'evaluation_cv': eval_cv_units if run_cv else None,\n",
    "            'evaluation_traintest': eval_traintest_units,\n",
    "            'predictions_traintest': preds_traintest_units,\n",
    "            'per_cutoff_best': per_cutoff_units if run_cv else None,\n",
    "        },\n",
    "        'prescriptions': {\n",
    "            'forecasts': forecasts_prescriptions,\n",
    "            'best_forecasts': best_forecasts_prescriptions if run_cv else None,\n",
    "            'evaluation_cv': eval_cv_prescriptions if run_cv else None,\n",
    "            'evaluation_traintest': eval_traintest_prescriptions,\n",
    "            'predictions_traintest': preds_traintest_prescriptions,\n",
    "            'per_cutoff_best': per_cutoff_prescriptions if run_cv else None,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfdb507",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save=rf\"C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\Forecast\\TMF\\\\\"\n",
    "os.path.isdir(path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca3274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the pre-built dataset\n",
    "csv_path=rf\"C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\merged_data\\Prebuilt_panels\\P1_nopop.csv\"\n",
    "path_save=rf\"C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\Forecast\\TMF\\\\\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(filepath=csv_path, states=['IN'], h=8, run_cv=True, n_windows=5,save_path=path_save)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
