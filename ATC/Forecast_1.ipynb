{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de25d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "from utilsforecast.plotting import plot_series\n",
    "from utilsforecast.evaluation import evaluate\n",
    "from utilsforecast.losses import *\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import (\n",
    "    Naive,WindowAverage, ARIMA, \n",
    "    AutoARIMA,SeasonalNaive,HoltWinters,\n",
    "    CrostonClassic as Croston, HistoricAverage,DynamicOptimizedTheta as DOT,\n",
    "    SeasonalNaive\n",
    ")\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "from itertools import product\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # To ignore warnings from pandas/numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "user=\"lholguin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1baf5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New data class created to handle configuration parameters\n",
    "@dataclass\n",
    "class ForecastConfig:\n",
    "    \n",
    "    # Forecast parameters\n",
    "    h: int = 8                          \n",
    "    season_length: int = 4              \n",
    "    \n",
    "    # Cross-validation parameters\n",
    "    n_windows: int = 2                  \n",
    "    step_size: Optional[int] = None     \n",
    "    \n",
    "    # Train-test split parameters\n",
    "    train_size: Optional[int] = None    # Use all available data except test\n",
    "    test_size: Optional[int] = None     # Auto-set to h in __post_init__\n",
    "    \n",
    "    # Plotting parameters\n",
    "    n_samples: int = 4                  # Plot 4 random samples\n",
    "    models_to_plot: Optional[List[str]] = None  # \n",
    "    \n",
    "    # Other settings\n",
    "    confidence_level: int = 95          # 95% confidence intervals\n",
    "    n_jobs: int = -1                    \n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \n",
    "        if self.step_size is None:\n",
    "            self.step_size = self.h\n",
    "        if self.test_size is None:\n",
    "            self.test_size = self.h\n",
    "        if self.models_to_plot is None:\n",
    "            self.models_to_plot = ['Naive', 'ARIMA_manual', 'SARIMA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9feee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(config):\n",
    "    \n",
    "    models = [Naive(), HistoricAverage(), WindowAverage(window_size=4),\n",
    "        SeasonalNaive(season_length=4), ARIMA(order=(1, 1, 1), alias=\"ARIMA_manual\"),\n",
    "        AutoARIMA(seasonal=True, season_length=4, alias=\"SARIMA\"),\n",
    "    ]\n",
    "    return models\n",
    "\n",
    "def evaluate_train_test(df, target_name, config):\n",
    "\n",
    "    if config.train_size is None:\n",
    "        train = df.groupby('unique_id').apply(\n",
    "            lambda x: x.iloc[:-config.test_size]\n",
    "        ).reset_index(drop=True)\n",
    "    else:\n",
    "        train = df.groupby('unique_id').apply(\n",
    "            lambda x: x.iloc[-(config.train_size + config.test_size):-config.test_size]\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    test = df.groupby('unique_id').apply(\n",
    "        lambda x: x.iloc[-config.test_size:]\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    sf = StatsForecast(\n",
    "        models=get_models(config),\n",
    "        freq='QS',\n",
    "        n_jobs=config.n_jobs,\n",
    "        fallback_model=SeasonalNaive(season_length=config.season_length)\n",
    "    )\n",
    "    sf.fit(df=train)\n",
    "    preds = sf.predict(h=config.h)\n",
    "\n",
    "    preds_df = pd.merge(test, preds.reset_index(), on=['ds', 'unique_id'], how='left')\n",
    "    models = [col for col in preds.columns if col not in ['unique_id', 'ds']]\n",
    "    \n",
    "    eval_df = evaluate(preds_df, metrics=[mae, mse, rmse], models=models)\n",
    "    \n",
    "    mae_df = eval_df[eval_df['metric'] == 'mae'].copy()\n",
    "    mae_df['best_model'] = mae_df[models].idxmin(axis=1)\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Best Models (Train-Test Split - based on MAE):\")\n",
    "    print(mae_df['best_model'].value_counts())\n",
    "    \n",
    "    return eval_df, preds_df, train, test \n",
    "\n",
    "def evaluate_model_cross(df, target_name, config):\n",
    "    \n",
    "    sf = StatsForecast(\n",
    "        models=get_models(config),\n",
    "        freq='QS',\n",
    "        n_jobs=config.n_jobs,\n",
    "        fallback_model=SeasonalNaive(season_length=config.season_length)\n",
    "    )\n",
    "\n",
    "    print(f\"   Running cross-validation...\")\n",
    "    cv_df = sf.cross_validation(\n",
    "        df=df,\n",
    "        h=config.h,\n",
    "        n_windows=config.n_windows,\n",
    "        step_size=config.step_size\n",
    "    )\n",
    "    \n",
    "    # Define model columns\n",
    "    exclude_cols = ['unique_id', 'ds', 'y', 'cutoff', 'metric']\n",
    "    model_cols = [col for col in cv_df.columns if col not in exclude_cols]\n",
    "\n",
    "    # Evaluate per cutoff window\n",
    "    all_results = []\n",
    "    \n",
    "    for cutoff in cv_df['cutoff'].unique():\n",
    "        cutoff_data = cv_df[cv_df['cutoff'] == cutoff]\n",
    "        \n",
    "        # Evaluate all metrics for this cutoff\n",
    "        cutoff_eval = evaluate(cutoff_data, metrics=[mae, mse, rmse], models=model_cols)\n",
    "        cutoff_eval['cutoff'] = cutoff\n",
    "        \n",
    "        # Add best_model column (lowest value per row, regardless of metric)\n",
    "        cutoff_eval['best_model'] = cutoff_eval[model_cols].idxmin(axis=1)\n",
    "        cutoff_eval['best_value'] = cutoff_eval[model_cols].min(axis=1)\n",
    "        \n",
    "        all_results.append(cutoff_eval)\n",
    "        \n",
    "        # Print MAE summary for this cutoff\n",
    "        cutoff_mae = cutoff_eval[cutoff_eval['metric'] == 'mae']\n",
    "        print(f\"\\n   Cutoff {cutoff.strftime('%Y-%m-%d')} (MAE best models):\")\n",
    "        print(f\"   {cutoff_mae['best_model'].value_counts().to_dict()}\")\n",
    "\n",
    "    # Combine all cutoff results into single dataframe\n",
    "    eval_df = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # Print overall summary\n",
    "    mae_overall = eval_df[eval_df['metric'] == 'mae']\n",
    "    print(f\"\\nðŸ“ˆ Overall Best Models (all cutoffs - based on MAE):\")\n",
    "    print(mae_overall['best_model'].value_counts())\n",
    "    \n",
    "    return eval_df, cv_df\n",
    "\n",
    "def sensitivity_analysis_with_production_forecasts(df, target_name, param_grid=None, production_horizons=[4, 8], save_path=None):\n",
    "    \n",
    "    # Default parameter grid if not provided\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'h': [4, 8],\n",
    "            'test_size': [4, 8],\n",
    "            'train_size': [None, 20, 28],\n",
    "            'n_windows': [2, 3, 4],\n",
    "            'step_size': [None, 2, 4],\n",
    "        }\n",
    "   \n",
    "    print(\"\\nPHASE 1: SENSITIVITY ANALYSIS\")\n",
    "    \n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    combinations = list(product(*param_values))\n",
    "    \n",
    "    print(f\"Testing {len(combinations)} configurations...\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for i, combo in enumerate(combinations):\n",
    "        params = dict(zip(param_names, combo))\n",
    "        \n",
    "        # Skip invalid combinations\n",
    "        if params.get('test_size') and params.get('h'):\n",
    "            if params['test_size'] < params['h']:\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nConfiguration {i+1}/{len(combinations)}: {params}\")\n",
    "        \n",
    "        try:\n",
    "            config = ForecastConfig(\n",
    "                h=params.get('h', 8),\n",
    "                train_size=params.get('train_size'),\n",
    "                test_size=params.get('test_size'),\n",
    "                n_windows=params.get('n_windows', 2),\n",
    "                step_size=params.get('step_size'),\n",
    "                season_length=4,\n",
    "                n_samples=4,\n",
    "                confidence_level=95,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Run train-test evaluation\n",
    "            eval_traintest, preds_traintest, train_df, test_df = evaluate_train_test(\n",
    "                df, target_name, config\n",
    "            )\n",
    "            \n",
    "            # Run cross-validation\n",
    "            eval_cv, cv_df = evaluate_model_cross(df, target_name, config)\n",
    "            \n",
    "            # Extract MAE results\n",
    "            mae_cv = eval_cv[eval_cv['metric'] == 'mae'].copy()\n",
    "            mae_traintest = eval_traintest[eval_traintest['metric'] == 'mae'].copy()\n",
    "            \n",
    "            # Get model columns\n",
    "            exclude_cols = ['unique_id', 'ds', 'y', 'cutoff', 'metric', 'best_model', 'best_value']\n",
    "            model_cols = [col for col in mae_cv.columns if col not in exclude_cols]\n",
    "            \n",
    "            # Add best_model to train-test if not present\n",
    "            if 'best_model' not in mae_traintest.columns:\n",
    "                mae_traintest['best_model'] = mae_traintest[model_cols].idxmin(axis=1)\n",
    "            \n",
    "            # Store results\n",
    "            for uid in df['unique_id'].unique():\n",
    "                uid_cv = mae_cv[mae_cv['unique_id'] == uid]\n",
    "                uid_traintest = mae_traintest[mae_traintest['unique_id'] == uid]\n",
    "                \n",
    "                cv_best_model = uid_cv['best_model'].mode().iloc[0] if len(uid_cv) > 0 else None\n",
    "                cv_best_count = (uid_cv['best_model'] == cv_best_model).sum()\n",
    "                cv_total = len(uid_cv)\n",
    "                cv_consistency = cv_best_count / cv_total if cv_total > 0 else 0\n",
    "                \n",
    "                traintest_best_model = uid_traintest['best_model'].iloc[0] if len(uid_traintest) > 0 else None\n",
    "                \n",
    "                result_row = {\n",
    "                    'unique_id': uid,\n",
    "                    'config_id': i + 1,\n",
    "                    **params,\n",
    "                    'cv_best_model': cv_best_model,\n",
    "                    'cv_consistency': cv_consistency,\n",
    "                    'traintest_best_model': traintest_best_model,\n",
    "                    'cv_traintest_agree': cv_best_model == traintest_best_model,\n",
    "                }\n",
    "                \n",
    "                # Add MAE values for each model\n",
    "                for model in model_cols:\n",
    "                    model_mae = uid_cv[model].mean() if len(uid_cv) > 0 else None\n",
    "                    result_row[f'{model}_mae'] = model_mae\n",
    "                \n",
    "                all_results.append(result_row)\n",
    "            \n",
    "            print(f\"   âœ“ Completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âœ— Error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(\"\\nPHASE 2: IDENTIFYING BEST MODELS PER HORIZON\")\n",
    "    recommendations_per_horizon = {}\n",
    "    \n",
    "    for horizon in production_horizons:\n",
    "        print(f\"\\nðŸŽ¯ Analyzing h={horizon}...\")\n",
    "        \n",
    "        # Filter results for this horizon\n",
    "        horizon_results = results_df[results_df['h'] == horizon].copy()\n",
    "        \n",
    "        if len(horizon_results) == 0:\n",
    "            print(f\"   âš ï¸ No results found for h={horizon}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Get recommendations for each unique_id at this horizon\n",
    "        horizon_recommendations = []\n",
    "        \n",
    "        for uid in horizon_results['unique_id'].unique():\n",
    "            uid_data = horizon_results[horizon_results['unique_id'] == uid]\n",
    "            \n",
    "            # Most frequent CV best model\n",
    "            cv_mode = uid_data['cv_best_model'].mode()\n",
    "            cv_best = cv_mode.iloc[0] if len(cv_mode) > 0 else None\n",
    "            cv_freq = (uid_data['cv_best_model'] == cv_best).sum() / len(uid_data)\n",
    "            \n",
    "            # Most frequent train-test best model\n",
    "            tt_mode = uid_data['traintest_best_model'].mode()\n",
    "            tt_best = tt_mode.iloc[0] if len(tt_mode) > 0 else None\n",
    "            tt_freq = (uid_data['traintest_best_model'] == tt_best).sum() / len(uid_data)\n",
    "            \n",
    "            # Average consistency\n",
    "            avg_consistency = uid_data['cv_consistency'].mean()\n",
    "            \n",
    "            # Determine recommendation\n",
    "            if cv_best == tt_best and cv_freq >= 0.7 and avg_consistency >= 0.7:\n",
    "                confidence = 'High'\n",
    "                recommended_model = cv_best\n",
    "                reason = \"CV and Train-Test agree, high frequency and consistency\"\n",
    "            elif cv_best == tt_best and cv_freq >= 0.5:\n",
    "                confidence = 'Medium-High'\n",
    "                recommended_model = cv_best\n",
    "                reason = \"CV and Train-Test agree with moderate frequency\"\n",
    "            elif cv_freq >= 0.6:\n",
    "                confidence = 'Medium'\n",
    "                recommended_model = cv_best\n",
    "                reason = f\"CV favors {cv_best} ({cv_freq:.0%})\"\n",
    "            elif tt_freq >= 0.6:\n",
    "                confidence = 'Medium'\n",
    "                recommended_model = tt_best\n",
    "                reason = f\"Train-Test favors {tt_best} ({tt_freq:.0%})\"\n",
    "            else:\n",
    "                confidence = 'Low'\n",
    "                mae_cols = [col for col in uid_data.columns if col.endswith('_mae')]\n",
    "                if mae_cols:\n",
    "                    avg_maes = uid_data[mae_cols].mean()\n",
    "                    recommended_model = avg_maes.idxmin().replace('_mae', '')\n",
    "                else:\n",
    "                    recommended_model = cv_best\n",
    "                reason = \"No clear winner - using lowest average MAE\"\n",
    "            \n",
    "            # Get average MAE\n",
    "            rec_mae_col = f'{recommended_model}_mae'\n",
    "            avg_mae = uid_data[rec_mae_col].mean() if rec_mae_col in uid_data.columns else None\n",
    "            \n",
    "            recommendation = {\n",
    "                'unique_id': uid,\n",
    "                'horizon': horizon,\n",
    "                'recommended_model': recommended_model,\n",
    "                'confidence': confidence,\n",
    "                'reason': reason,\n",
    "                'cv_best_model': cv_best,\n",
    "                'cv_frequency': cv_freq,\n",
    "                'cv_consistency': avg_consistency,\n",
    "                'traintest_best_model': tt_best,\n",
    "                'avg_mae': avg_mae\n",
    "            }\n",
    "            \n",
    "            horizon_recommendations.append(recommendation)\n",
    "            \n",
    "            print(f\"   {uid}: {recommended_model} (confidence: {confidence}, MAE: {avg_mae:,.0f})\")\n",
    "        \n",
    "        recommendations_per_horizon[horizon] = pd.DataFrame(horizon_recommendations)\n",
    "    \n",
    "\n",
    "    print(\"\\nPHASE 3: GENERATING PRODUCTION FORECASTS\")\n",
    "    \n",
    "    production_forecasts = {}\n",
    "    \n",
    "    for horizon in production_horizons:\n",
    "        if horizon not in recommendations_per_horizon:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nðŸ”® Generating forecasts for h={horizon}...\")\n",
    "        \n",
    "        recommendations = recommendations_per_horizon[horizon]\n",
    "        \n",
    "        # Train on FULL dataset\n",
    "        config_prod = ForecastConfig(\n",
    "            h=horizon,\n",
    "            season_length=4,\n",
    "            confidence_level=95,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Train all models on full data\n",
    "        sf = StatsForecast(\n",
    "            models=get_models(config_prod),\n",
    "            freq='QS',\n",
    "            n_jobs=-1,\n",
    "            fallback_model=SeasonalNaive(season_length=4)\n",
    "        )\n",
    "        \n",
    "        forecasts_df = sf.forecast(df=df, h=horizon, level=[95])\n",
    "        \n",
    "        # Create a \"best model\" forecast by selecting the recommended model for each unique_id\n",
    "        best_forecasts = []\n",
    "        \n",
    "        for uid in df['unique_id'].unique():\n",
    "            uid_forecasts = forecasts_df.reset_index()\n",
    "            uid_forecasts = uid_forecasts[uid_forecasts['unique_id'] == uid]\n",
    "            \n",
    "            uid_rec = recommendations[recommendations['unique_id'] == uid]\n",
    "            if len(uid_rec) == 0:\n",
    "                print(f\"   âš ï¸ No recommendation found for {uid}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            best_model = uid_rec['recommended_model'].iloc[0]\n",
    "            \n",
    "            for _, row in uid_forecasts.iterrows():\n",
    "                best_row = {\n",
    "                    'unique_id': uid,\n",
    "                    'ds': row['ds'],\n",
    "                    'recommended_model': best_model,\n",
    "                    'forecast': row.get(best_model, np.nan),\n",
    "                    'forecast_lo_95': row.get(f'{best_model}-lo-95', np.nan),\n",
    "                    'forecast_hi_95': row.get(f'{best_model}-hi-95', np.nan)\n",
    "                }\n",
    "                best_forecasts.append(best_row)\n",
    "        \n",
    "        best_forecasts_df = pd.DataFrame(best_forecasts)\n",
    "        \n",
    "        production_forecasts[horizon] = {\n",
    "            'all_models': forecasts_df,\n",
    "            'best_model': best_forecasts_df,\n",
    "            'recommendations': recommendations\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ“ Generated {len(best_forecasts_df)} forecast periods\")\n",
    "        print(f\"   âœ“ Models used: {recommendations['recommended_model'].value_counts().to_dict()}\")\n",
    "    \n",
    "        print(\"\\nPHASE 4: SAVING RESULTS\")\n",
    "        \n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        # Save sensitivity analysis results\n",
    "        sensitivity_file = os.path.join(save_path, f'{target_name.lower().replace(\" \", \"_\")}_sensitivity_analysis.xlsx')\n",
    "        with pd.ExcelWriter(sensitivity_file, engine='openpyxl') as writer:\n",
    "            results_df.to_excel(writer, sheet_name='All_Configurations', index=False)\n",
    "            \n",
    "            # Combined recommendations\n",
    "            all_recommendations = pd.concat(recommendations_per_horizon.values(), ignore_index=True)\n",
    "            all_recommendations.to_excel(writer, sheet_name='Recommendations', index=False)\n",
    "        \n",
    "        print(f\"âœ“ Sensitivity analysis saved: {sensitivity_file}\")\n",
    "        \n",
    "        # Save production forecasts for each horizon\n",
    "        for horizon, forecasts in production_forecasts.items():\n",
    "            horizon_file = os.path.join(save_path, f'{target_name.lower().replace(\" \", \"_\")}_production_h{horizon}.xlsx')\n",
    "            \n",
    "            with pd.ExcelWriter(horizon_file, engine='openpyxl') as writer:\n",
    "                # All models\n",
    "                forecasts['all_models'].reset_index().to_excel(\n",
    "                    writer, sheet_name='All_Models', index=False\n",
    "                )\n",
    "                \n",
    "                # Best model forecasts\n",
    "                forecasts['best_model'].to_excel(\n",
    "                    writer, sheet_name='Best_Model_Forecast', index=False\n",
    "                )\n",
    "                \n",
    "                # Recommendations\n",
    "                forecasts['recommendations'].to_excel(\n",
    "                    writer, sheet_name='Model_Selection', index=False\n",
    "                )\n",
    "            \n",
    "            print(f\"âœ“ Production forecasts (h={horizon}) saved: {horizon_file}\")\n",
    "    \n",
    "    print(\"\\nâœ… COMPLETE!\")\n",
    "    \n",
    "    return {\n",
    "        'sensitivity_results': results_df,\n",
    "        'production_forecasts': production_forecasts,\n",
    "        'recommendations_per_horizon': recommendations_per_horizon\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef96e8",
   "metadata": {},
   "source": [
    "### Population Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a1f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath, states=None):\n",
    "    \n",
    "    df=pd.read_csv(filepath)\n",
    "    #Filtering for certain years\n",
    "    df = df[(df['Period'] >= '2017Q1') & (df['Period'] <= '2024Q4')].copy()\n",
    "    if states is not None:\n",
    "        if isinstance(states, str):\n",
    "            states = [states]\n",
    "        print(f\"Filtering data for states: {states}\")\n",
    "        df = df[df['State'].isin(states)].copy()\n",
    "        print(f\"Filtered to {len(df)} rows across {df['State'].unique()} state(s)\")\n",
    "        if len(df) == 0:\n",
    "            raise ValueError(\"No data available after filtering by states.\")    \n",
    "    \n",
    "    df['unique_id']=df['State']\n",
    "    df['ds']=pd.to_datetime(df['Period'])\n",
    "    df=df.sort_values(['unique_id','ds']).reset_index(drop=True)\n",
    "    df_pop=df[['unique_id','ds','Population']].copy()\n",
    "    df_pop.columns = ['unique_id', 'ds', 'y']\n",
    "    \n",
    "\n",
    "    return df_pop,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_csv_path=rf\"C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\merged_data\\Prebuilt_panels\\medi_pop.csv\"\n",
    "pop_save_path=rf\"C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\Forecast\\Pop\\\\\"\n",
    "\n",
    "df_pop, df_original = load_data(filepath=pop_csv_path, states=['IN'])\n",
    "param_grid = {\n",
    "    'h': [4, 8],\n",
    "    'test_size': [4, 8],\n",
    "    'train_size': [None, 26],\n",
    "    'n_windows': [2, 3],\n",
    "    'step_size': [4],\n",
    "}\n",
    "results = sensitivity_analysis_with_production_forecasts(\n",
    "    df=df_pop,\n",
    "    target_name=\"Population\",\n",
    "    param_grid=param_grid,\n",
    "    production_horizons=[4, 8],\n",
    "    save_path=pop_save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93281f08",
   "metadata": {},
   "source": [
    "### Statistical Models - Dataset with pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b5d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(filepath, states=None):\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    #Cutting the dataframe fo selected years\n",
    "    df=df[(df['Period'] >= '2017Q1') & (df['Period'] <= '2024Q4')].copy()\n",
    "    \n",
    "    if states is not None:\n",
    "        if isinstance(states, str):\n",
    "            states = [states]\n",
    "        print(f\"Filtering data for states: {states}\")\n",
    "        df=df[df['State'].isin(states)].copy()\n",
    "        print(f\"Filtered to {len(df)} rows across {df['State'].nunique()} state(s).\")\n",
    "\n",
    "        if len(df) == 0:\n",
    "            raise ValueError(\"No data available for the specified states.\")\n",
    "    \n",
    "    df['unique_id'] = df['State'] + '_' + df['ATC2 Class']\n",
    "    df['ds'] = pd.to_datetime(df['Period'])\n",
    "    df = df.sort_values(['unique_id','ds']).reset_index(drop=True)\n",
    "\n",
    "    #Preparing dataframes for statsforecast\n",
    "    df_units=df[['unique_id','ds','Units Reimbursed']].copy()\n",
    "    df_units.columns=['unique_id','ds','y']\n",
    "\n",
    "    df_prescriptions=df[['unique_id','ds','Number of Prescriptions']].copy()\n",
    "    df_prescriptions.columns=['unique_id','ds','y']\n",
    "\n",
    "    return df_units, df_prescriptions, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(filepath, states=None, config=None, run_cv=True, save_path=None):\n",
    "    \n",
    "    # Create default config if not provided\n",
    "    if config is None:\n",
    "        config = ForecastConfig()\n",
    "    \n",
    "    print(\"\\n1. Loading and preparing data...\")\n",
    "    df_units, df_prescriptions, df_original = load_and_prepare_data(filepath, states=states)\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"  â€¢ Total unique series: {df_units['unique_id'].nunique()}\")\n",
    "    min_date = df_units['ds'].min()\n",
    "    max_date = df_units['ds'].max()\n",
    "    print(f\"  â€¢ Date range: {min_date.year}-Q{min_date.quarter} to {max_date.year}-Q{max_date.quarter}\")\n",
    "    if states is not None:\n",
    "        print(f\"  â€¢ Filtered states: {', '.join(states)}\")\n",
    "    \n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"UNITS REIMBURSED\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n2. Training models and generating forecasts...\")\n",
    "    forecasts_units, sf_units = train_and_forecast(df_units, \"Units Reimbursed\", config)\n",
    "\n",
    "    print(\"\\n3. Evaluating models on training/test split...\")\n",
    "    eval_traintest_units, preds_traintest_units, train_units, test_units = evaluate_train_test(\n",
    "        df_units, \"Units Reimbursed\", config\n",
    "    )\n",
    "    \n",
    "    if run_cv:\n",
    "        print(\"\\n4. Running cross-validation...\")\n",
    "        eval_cv_units, cv_units, eval_summary_units, per_cutoff_units = evaluate_model_cross(\n",
    "            df_units, \"Units Reimbursed\", config\n",
    "        )\n",
    "        \n",
    "        print(\"\\n5. Selecting best forecasts...\")\n",
    "        best_forecasts_units = get_best_model_forecast(forecasts_units, eval_summary_units)\n",
    "        \n",
    "        print(\"\\n6. Saving results...\")\n",
    "        save_results_to_excel(\n",
    "            forecasts_units, best_forecasts_units, \n",
    "            eval_cv_units, eval_traintest_units, \n",
    "            \"Units Reimbursed\", save_path=save_path\n",
    "        )\n",
    "    else:\n",
    "        eval_cv_units = None\n",
    "        per_cutoff_units = None\n",
    "        best_forecasts_units = forecasts_units.reset_index()\n",
    "        print(\"\\n Skipping cross-validation (run_cv=False)\")\n",
    "    \n",
    "    # Plot sample forecasts\n",
    "    print(\"\\nGenerating forecast plots...\")\n",
    "    plot_sample_forecasts(df_units, forecasts_units, \"Units Reimbursed\", config)\n",
    "    plot_train_test_forecasts(df_units, train_units, test_units, preds_traintest_units, \"Units Reimbursed\", config)\n",
    "    \n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"NUMBER OF PRESCRIPTIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n2. Training models and generating forecasts...\")\n",
    "    forecasts_prescriptions, sf_prescriptions = train_and_forecast(\n",
    "        df_prescriptions, \"Number of Prescriptions\", config\n",
    "    )\n",
    "\n",
    "    print(\"\\n3. Evaluating models on training/test split...\")\n",
    "    eval_traintest_prescriptions, preds_traintest_prescriptions, train_prescriptions, test_prescriptions = evaluate_train_test(\n",
    "        df_prescriptions, \"Number of Prescriptions\", config\n",
    "    )\n",
    "    \n",
    "    if run_cv:\n",
    "        print(\"\\n4. Running cross-validation...\")\n",
    "        eval_cv_prescriptions, cv_prescriptions, eval_summary_prescriptions, per_cutoff_prescriptions = evaluate_model_cross(\n",
    "            df_prescriptions, \"Number of Prescriptions\", config\n",
    "        )\n",
    "        \n",
    "        print(\"\\n5. Selecting best forecasts...\")\n",
    "        best_forecasts_prescriptions = get_best_model_forecast(\n",
    "            forecasts_prescriptions, eval_summary_prescriptions\n",
    "        )\n",
    "        \n",
    "        print(\"\\n6. Saving results...\")\n",
    "        save_results_to_excel(\n",
    "            forecasts_prescriptions, best_forecasts_prescriptions, \n",
    "            eval_cv_prescriptions, eval_traintest_prescriptions, \n",
    "            \"Number of Prescriptions\", save_path=save_path\n",
    "        )\n",
    "    else:\n",
    "        eval_cv_prescriptions = None\n",
    "        per_cutoff_prescriptions = None\n",
    "        best_forecasts_prescriptions = forecasts_prescriptions.reset_index()\n",
    "        print(\"\\n Skipping cross-validation (run_cv=False)\")\n",
    "    \n",
    "    # Plot sample forecasts\n",
    "    print(\"\\nGenerating forecast plots...\")\n",
    "    plot_sample_forecasts(df_prescriptions, forecasts_prescriptions, \"Number of Prescriptions\", config)\n",
    "    plot_train_test_forecasts(df_prescriptions, train_prescriptions, test_prescriptions, preds_traintest_prescriptions, \"Number of Prescriptions\", config)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âœ… PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    state_info = f\"{', '.join(states)}\" if states else \"ALL\"\n",
    "    print(f\"\\nForecasts generated for state(s): {state_info}\")\n",
    "    print(f\"Forecast horizon: {config.h} quarters ahead\")\n",
    "    \n",
    "    if run_cv:\n",
    "        print(f\"\\nAll files generated!\")\n",
    "    \n",
    "    return {\n",
    "        'units': {\n",
    "            'forecasts': forecasts_units,\n",
    "            'best_forecasts': best_forecasts_units if run_cv else None,\n",
    "            'evaluation_cv': eval_cv_units if run_cv else None,\n",
    "            'evaluation_traintest': eval_traintest_units,\n",
    "            'predictions_traintest': preds_traintest_units,\n",
    "            'per_cutoff_best': per_cutoff_units if run_cv else None,\n",
    "        },\n",
    "        'prescriptions': {\n",
    "            'forecasts': forecasts_prescriptions,\n",
    "            'best_forecasts': best_forecasts_prescriptions if run_cv else None,\n",
    "            'evaluation_cv': eval_cv_prescriptions if run_cv else None,\n",
    "            'evaluation_traintest': eval_traintest_prescriptions,\n",
    "            'predictions_traintest': preds_traintest_prescriptions,\n",
    "            'per_cutoff_best': per_cutoff_prescriptions if run_cv else None,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfdb507",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save=rf\"C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\Forecast\\TMF\\\\\"\n",
    "os.path.isdir(path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca3274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the pre-built dataset\n",
    "csv_path=rf\"C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\merged_data\\Prebuilt_panels\\P1_nopop.csv\"\n",
    "path_save=rf\"C:\\Users\\{user}\\OneDrive - purdue.edu\\VS code\\Data\\ATC\\Forecast\\TMF\\\\\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(filepath=csv_path, states=['IN'], h=8, run_cv=True, n_windows=5,save_path=path_save)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
